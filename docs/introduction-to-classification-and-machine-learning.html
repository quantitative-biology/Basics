<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Introduction to classification and machine learning | Quantitative biology: a quick introduction</title>
  <meta name="description" content="A set of modules on quantitaitve topics for biology graduates" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Introduction to classification and machine learning | Quantitative biology: a quick introduction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A set of modules on quantitaitve topics for biology graduates" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Introduction to classification and machine learning | Quantitative biology: a quick introduction" />
  
  <meta name="twitter:description" content="A set of modules on quantitaitve topics for biology graduates" />
  

<meta name="author" content="Kim Cuddington, Andrew Edwards, Brian Ingalls" />


<meta name="date" content="2021-11-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-multivariate-analysis-clustering-and-ordination.html"/>
<link rel="next" href="optimization.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome!</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#open-science"><i class="fa fa-check"></i><b>1.1</b> Open Science</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i><b>1.2</b> Feedback</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html"><i class="fa fa-check"></i><b>2</b> Introduction to Git and GitHub</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#what-are-git-and-github-and-how-might-they-be-useful"><i class="fa fa-check"></i><b>2.1</b> What are Git and GitHub and how might they be useful?</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#you-just-want-do-download-someone-elses-code-from-their-github-repository"><i class="fa fa-check"></i><b>2.2</b> You just want do download someone else’s code from their GitHub repository</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#motivation-for-learning-more"><i class="fa fa-check"></i><b>2.3</b> Motivation for learning more</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#example-application-pacific-hake-stock-assessment"><i class="fa fa-check"></i><b>2.3.1</b> Example application – Pacific Hake stock assessment</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#examples-of-what-we-can-avoid"><i class="fa fa-check"></i><b>2.3.2</b> Examples of what we can avoid</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#example-of-advantages-that-arise-from-using-github"><i class="fa fa-check"></i><b>2.3.3</b> Example of advantages that arise from using GitHub</a></li>
<li class="chapter" data-level="2.3.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#why-this-course"><i class="fa fa-check"></i><b>2.3.4</b> Why this course?</a></li>
<li class="chapter" data-level="2.3.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#does-it-matter-which-computer-language-my-code-is-in"><i class="fa fa-check"></i><b>2.3.5</b> Does it matter which computer language my code is in?</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#getting-set-up-for-the-first-time"><i class="fa fa-check"></i><b>2.4</b> Getting set up for the first time</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#what-you-will-end-up-having-installed"><i class="fa fa-check"></i><b>2.4.1</b> What you will end up having installed</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#text-editor"><i class="fa fa-check"></i><b>2.4.2</b> Text Editor</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#install-the-git-application-on-your-machine"><i class="fa fa-check"></i><b>2.4.3</b> Install the Git application on your machine</a></li>
<li class="chapter" data-level="2.4.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#git-shell-guis-and-rstudio"><i class="fa fa-check"></i><b>2.4.4</b> Git shell, GUIs and RStudio</a></li>
<li class="chapter" data-level="2.4.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#powershell-and-posh-git"><i class="fa fa-check"></i><b>2.4.5</b> Powershell and posh-git</a></li>
<li class="chapter" data-level="2.4.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#create-a-directory-to-keep-all-your-git-tracked-work"><i class="fa fa-check"></i><b>2.4.6</b> Create a directory to keep all your Git-tracked work</a></li>
<li class="chapter" data-level="2.4.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#install-diffmerge-optional"><i class="fa fa-check"></i><b>2.4.7</b> Install Diffmerge (optional)</a></li>
<li class="chapter" data-level="2.4.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#save-our-template-.gitconfig-file"><i class="fa fa-check"></i><b>2.4.8</b> Save our template <em>.gitconfig</em> file</a></li>
<li class="chapter" data-level="2.4.9" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#edit-the-.gitconfig-file"><i class="fa fa-check"></i><b>2.4.9</b> Edit the <em>.gitconfig</em> file</a></li>
<li class="chapter" data-level="2.4.10" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#github-authorisation"><i class="fa fa-check"></i><b>2.4.10</b> GitHub authorisation</a></li>
<li class="chapter" data-level="2.4.11" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#navigating-in-a-shell"><i class="fa fa-check"></i><b>2.4.11</b> Navigating in a shell</a></li>
<li class="chapter" data-level="2.4.12" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#one-time-authentication"><i class="fa fa-check"></i><b>2.4.12</b> One-time authentication</a></li>
<li class="chapter" data-level="2.4.13" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#mac-only-make-your-output-pretty"><i class="fa fa-check"></i><b>2.4.13</b> MAC only: make your output pretty</a></li>
<li class="chapter" data-level="2.4.14" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#something-to-view-markdown-files-optional"><i class="fa fa-check"></i><b>2.4.14</b> Something to view Markdown files (optional)</a></li>
<li class="chapter" data-level="2.4.15" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#bonus-keyboard-shortcut-optional"><i class="fa fa-check"></i><b>2.4.15</b> Bonus keyboard shortcut (optional)</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#using-git-and-github-to-share-your-own-code"><i class="fa fa-check"></i><b>2.5</b> Using Git and GitHub to share your own code</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#definitions"><i class="fa fa-check"></i><b>2.5.1</b> Definitions</a></li>
<li class="chapter" data-level="2.5.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#creating-a-new-repository"><i class="fa fa-check"></i><b>2.5.2</b> Creating a new repository</a></li>
<li class="chapter" data-level="2.5.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#cloning-your-new-repository"><i class="fa fa-check"></i><b>2.5.3</b> Cloning your new repository</a></li>
<li class="chapter" data-level="2.5.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#committing"><i class="fa fa-check"></i><b>2.5.4</b> Committing</a></li>
<li class="chapter" data-level="2.5.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-1-create-edit-and-commit-simpletext.txt"><i class="fa fa-check"></i><b>2.5.5</b> Exercise 1: create, edit and commit <em>simpleText.txt</em></a></li>
<li class="chapter" data-level="2.5.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-2-multiple-files"><i class="fa fa-check"></i><b>2.5.6</b> Exercise 2: multiple files</a></li>
<li class="chapter" data-level="2.5.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#what-to-write-in-commit-messages"><i class="fa fa-check"></i><b>2.5.7</b> What to write in commit messages</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#using-git-and-github-to-collaborate-with-colleagues"><i class="fa fa-check"></i><b>2.6</b> Using Git and GitHub to collaborate with colleagues</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#demonstration-of-collaborating"><i class="fa fa-check"></i><b>2.6.1</b> Demonstration of collaborating</a></li>
<li class="chapter" data-level="2.6.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#a-bit-more-about-git-rebase"><i class="fa fa-check"></i><b>2.6.2</b> A bit more about git rebase</a></li>
<li class="chapter" data-level="2.6.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#fixing-a-conflict"><i class="fa fa-check"></i><b>2.6.3</b> Fixing a conflict</a></li>
<li class="chapter" data-level="2.6.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-3-collaborating-on-a-single-repository"><i class="fa fa-check"></i><b>2.6.4</b> Exercise 3: collaborating on a single repository</a></li>
<li class="chapter" data-level="2.6.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#collaborating-summary"><i class="fa fa-check"></i><b>2.6.5</b> Collaborating summary</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#workflow-tips-when-collaborating"><i class="fa fa-check"></i><b>2.7</b> Workflow tips when collaborating</a></li>
<li class="chapter" data-level="2.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#beyond-the-basics-of-git-and-github-getting-more-advanced"><i class="fa fa-check"></i><b>2.8</b> Beyond the basics of Git and GitHub – getting more advanced</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#so-ive-made-some-changes-but-dont-really-want-to-keep-them-git-stash"><i class="fa fa-check"></i><b>2.8.1</b> So I’ve made some changes but don’t really want to keep them – git stash</a></li>
<li class="chapter" data-level="2.8.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#the-power-to-go-back"><i class="fa fa-check"></i><b>2.8.2</b> The power to go back</a></li>
<li class="chapter" data-level="2.8.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#so-how-does-git-do-all-this"><i class="fa fa-check"></i><b>2.8.3</b> So how does Git do all this?</a></li>
<li class="chapter" data-level="2.8.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#git-terminology"><i class="fa fa-check"></i><b>2.8.4</b> Git terminology</a></li>
<li class="chapter" data-level="2.8.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#branching"><i class="fa fa-check"></i><b>2.8.5</b> Branching</a></li>
<li class="chapter" data-level="2.8.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#pull-requests"><i class="fa fa-check"></i><b>2.8.6</b> Pull requests</a></li>
<li class="chapter" data-level="2.8.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#undoing-stuff"><i class="fa fa-check"></i><b>2.8.7</b> Undoing stuff</a></li>
<li class="chapter" data-level="2.8.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#using-r-and-github-within-rstudio"><i class="fa fa-check"></i><b>2.8.8</b> Using R and GitHub within RStudio</a></li>
<li class="chapter" data-level="2.8.9" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#feedback-1"><i class="fa fa-check"></i><b>2.8.9</b> Feedback</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html"><i class="fa fa-check"></i><b>3</b> Introduction to R Markdown</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#basic-idea"><i class="fa fa-check"></i><b>3.2</b> Basic idea</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#simple-example"><i class="fa fa-check"></i><b>3.3</b> Simple example</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#exercise"><i class="fa fa-check"></i><b>3.3.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#output-format"><i class="fa fa-check"></i><b>3.4</b> Output format</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#further-reading"><i class="fa fa-check"></i><b>3.5</b> Further reading</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#feedback-2"><i class="fa fa-check"></i><b>3.5.1</b> Feedback</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html"><i class="fa fa-check"></i><b>4</b> Introduction to multivariate analysis: Clustering and Ordination</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#multivariate-resemblance"><i class="fa fa-check"></i><b>4.1</b> Multivariate resemblance</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#binary-similarity-metrics"><i class="fa fa-check"></i><b>4.1.1</b> Binary Similarity metrics</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#quantitative-similarity-dissimilarity-metrics"><i class="fa fa-check"></i><b>4.1.2</b> Quantitative similarity &amp; dissimilarity metrics</a></li>
<li class="chapter" data-level="4.1.3" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#comparing-more-than-two-communitiessamplessitesgenesspecies"><i class="fa fa-check"></i><b>4.1.3</b> Comparing more than two communities/samples/sites/genes/species</a></li>
<li class="chapter" data-level="4.1.4" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#r-functions"><i class="fa fa-check"></i><b>4.1.4</b> R functions</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#cluster-analysis"><i class="fa fa-check"></i><b>4.2</b> Cluster Analysis</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#hierarchical-clustering-groups-are-nested-within-other-groups."><i class="fa fa-check"></i><b>4.2.1</b> Hierarchical clustering: groups are nested within other groups.</a></li>
<li class="chapter" data-level="4.2.2" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#partitional-clustering-and-fuzzy-clustering"><i class="fa fa-check"></i><b>4.2.2</b> Partitional clustering and Fuzzy clustering</a></li>
<li class="chapter" data-level="4.2.3" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#r-functions-for-clustering"><i class="fa fa-check"></i><b>4.2.3</b> R functions for clustering</a></li>
<li class="chapter" data-level="4.2.4" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#example-cluster-analysis-of-isotope-data"><i class="fa fa-check"></i><b>4.2.4</b> Example: Cluster analysis of isotope data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#Ord"><i class="fa fa-check"></i><b>4.3</b> Ordination</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>4.3.1</b> Principal Components Analysis (PCA)</a></li>
<li class="chapter" data-level="4.3.2" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#principle-coordinates-analysis-pcoa"><i class="fa fa-check"></i><b>4.3.2</b> Principle Coordinates Analysis (PCoA)</a></li>
<li class="chapter" data-level="4.3.3" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#nonmetric-multidimensional-scaling-nmds"><i class="fa fa-check"></i><b>4.3.3</b> Nonmetric Multidimensional Scaling (nMDS)</a></li>
<li class="chapter" data-level="4.3.4" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#example-nmds-and-pcoa"><i class="fa fa-check"></i><b>4.3.4</b> Example: nMDS and PCoA</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#constrained-ordination"><i class="fa fa-check"></i><b>4.4</b> Constrained Ordination</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#redundancy-analysis"><i class="fa fa-check"></i><b>4.4.1</b> Redundancy Analysis</a></li>
<li class="chapter" data-level="4.4.2" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#example-constrained-ordination"><i class="fa fa-check"></i><b>4.4.2</b> Example: Constrained ordination</a></li>
<li class="chapter" data-level="4.4.3" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#signficance-tests-for-constrained-ordination"><i class="fa fa-check"></i><b>4.4.3</b> Signficance tests for constrained ordination</a></li>
<li class="chapter" data-level="4.4.4" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#forward-selection-of-explanatory-variables"><i class="fa fa-check"></i><b>4.4.4</b> Forward Selection of explanatory variables</a></li>
<li class="chapter" data-level="4.4.5" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#tri"><i class="fa fa-check"></i><b>4.4.5</b> Triplots: Graphing a constrained ordination</a></li>
<li class="chapter" data-level="4.4.6" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#feedback-3"><i class="fa fa-check"></i><b>4.4.6</b> Feedback</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#references"><i class="fa fa-check"></i><b>4.5</b> References</a></li>
<li class="chapter" data-level="4.6" data-path="introduction-to-multivariate-analysis-clustering-and-ordination.html"><a href="introduction-to-multivariate-analysis-clustering-and-ordination.html#answer-key"><i class="fa fa-check"></i><b>4.6</b> Answer Key</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html"><i class="fa fa-check"></i><b>5</b> Introduction to classification and machine learning</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#logistic-regression-as-a-binary-classifier"><i class="fa fa-check"></i><b>5.1</b> Logistic regression as a binary classifier</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#interpreting-the-logistic-regression"><i class="fa fa-check"></i><b>5.1.1</b> Interpreting the logistic regression</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#measuring-the-performance-of-a-binary-classifier"><i class="fa fa-check"></i><b>5.1.2</b> Measuring the performance of a binary classifier</a></li>
<li class="chapter" data-level="5.1.3" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>5.1.3</b> Multiple logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#cross-validation"><i class="fa fa-check"></i><b>5.2</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.2.1</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.2.2" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#examples-of-logistic-regression-used-for-classification"><i class="fa fa-check"></i><b>5.2.2</b> Examples of logistic regression used for classification</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>5.3</b> <strong>L</strong>inear <strong>D</strong>iscriminant <strong>A</strong>nalysis (<strong>LDA</strong>)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#pca-and-lda-whats-the-difference"><i class="fa fa-check"></i><b>5.3.1</b> PCA and LDA what’s the difference?</a></li>
<li class="chapter" data-level="5.3.2" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#examples-of-discriminant-analysis-used-for-classification"><i class="fa fa-check"></i><b>5.3.2</b> Examples of discriminant analysis used for classification</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#tree-based-methods-for-classification"><i class="fa fa-check"></i><b>5.4</b> Tree-based methods for classification</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#classification-and-regression-trees-carts"><i class="fa fa-check"></i><b>5.4.1</b> Classification and regression trees (CARTs)</a></li>
<li class="chapter" data-level="5.4.2" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#random-forests"><i class="fa fa-check"></i><b>5.4.2</b> Random Forests</a></li>
<li class="chapter" data-level="5.4.3" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#examples-of-tree-based-methods-used-for-classification"><i class="fa fa-check"></i><b>5.4.3</b> Examples of tree-based methods used for classification</a></li>
<li class="chapter" data-level="5.4.4" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#r-functions-1"><i class="fa fa-check"></i><b>5.4.4</b> R functions</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#artificial-neural-networks-ann"><i class="fa fa-check"></i><b>5.5</b> <strong>A</strong>rtificial <strong>N</strong>eural <strong>N</strong>etworks (ANN)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#ann-learning"><i class="fa fa-check"></i><b>5.5.1</b> ANN learning</a></li>
<li class="chapter" data-level="5.5.2" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#our-first-ann"><i class="fa fa-check"></i><b>5.5.2</b> Our first ANN</a></li>
<li class="chapter" data-level="5.5.3" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#examples-of-anns-used-for-classification"><i class="fa fa-check"></i><b>5.5.3</b> Examples of ANNs used for classification</a></li>
<li class="chapter" data-level="5.5.4" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#r-functions-2"><i class="fa fa-check"></i><b>5.5.4</b> R functions</a></li>
<li class="chapter" data-level="5.5.5" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#what-else"><i class="fa fa-check"></i><b>5.5.5</b> What else?</a></li>
<li class="chapter" data-level="5.5.6" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#feedback-4"><i class="fa fa-check"></i><b>5.5.6</b> Feedback</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#references-1"><i class="fa fa-check"></i><b>5.6</b> References</a></li>
<li class="chapter" data-level="5.7" data-path="introduction-to-classification-and-machine-learning.html"><a href="introduction-to-classification-and-machine-learning.html#answer-key-1"><i class="fa fa-check"></i><b>5.7</b> Answer Key</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>6</b> Optimization</a>
<ul>
<li class="chapter" data-level="6.1" data-path="optimization.html"><a href="optimization.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="optimization.html"><a href="optimization.html#fundamentals-of-optimization"><i class="fa fa-check"></i><b>6.2</b> Fundamentals of Optimization</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="optimization.html"><a href="optimization.html#fermats-theorem"><i class="fa fa-check"></i><b>6.2.1</b> Fermat’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="optimization.html"><a href="optimization.html#regression"><i class="fa fa-check"></i><b>6.3</b> Regression</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="optimization.html"><a href="optimization.html#linear-regression"><i class="fa fa-check"></i><b>6.3.1</b> Linear Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="optimization.html"><a href="optimization.html#nonlinear-regression"><i class="fa fa-check"></i><b>6.3.2</b> Nonlinear Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="optimization.html"><a href="optimization.html#iterative-optimization-algorithms"><i class="fa fa-check"></i><b>6.4</b> Iterative Optimization Algorithms</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="optimization.html"><a href="optimization.html#gradient-descent"><i class="fa fa-check"></i><b>6.4.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="6.4.2" data-path="optimization.html"><a href="optimization.html#global-optimization"><i class="fa fa-check"></i><b>6.4.2</b> Global Optimization</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="optimization.html"><a href="optimization.html#calibration-of-dynamic-models"><i class="fa fa-check"></i><b>6.5</b> Calibration of Dynamic Models</a></li>
<li class="chapter" data-level="6.6" data-path="optimization.html"><a href="optimization.html#uncertainty-analysis-and-bayesian-calibration"><i class="fa fa-check"></i><b>6.6</b> Uncertainty Analysis and Bayesian Calibration</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="optimization.html"><a href="optimization.html#feedback-5"><i class="fa fa-check"></i><b>6.6.1</b> Feedback</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="optimization.html"><a href="optimization.html#references-2"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
<li class="chapter" data-level="6.8" data-path="optimization.html"><a href="optimization.html#answer-key-2"><i class="fa fa-check"></i><b>6.8</b> Answer Key</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="optimization.html"><a href="optimization.html#exercise-1-1"><i class="fa fa-check"></i><b>6.8.1</b> Exercise 1</a></li>
<li class="chapter" data-level="6.8.2" data-path="optimization.html"><a href="optimization.html#solution"><i class="fa fa-check"></i><b>6.8.2</b> Solution</a></li>
<li class="chapter" data-level="6.8.3" data-path="optimization.html"><a href="optimization.html#exercise-2-1"><i class="fa fa-check"></i><b>6.8.3</b> Exercise 2</a></li>
<li class="chapter" data-level="6.8.4" data-path="optimization.html"><a href="optimization.html#solution-1"><i class="fa fa-check"></i><b>6.8.4</b> Solution</a></li>
<li class="chapter" data-level="6.8.5" data-path="optimization.html"><a href="optimization.html#exercise-3-1"><i class="fa fa-check"></i><b>6.8.5</b> Exercise 3</a></li>
<li class="chapter" data-level="6.8.6" data-path="optimization.html"><a href="optimization.html#solution-2"><i class="fa fa-check"></i><b>6.8.6</b> Solution</a></li>
<li class="chapter" data-level="6.8.7" data-path="optimization.html"><a href="optimization.html#exercise-4-1"><i class="fa fa-check"></i><b>6.8.7</b> Exercise 4</a></li>
<li class="chapter" data-level="6.8.8" data-path="optimization.html"><a href="optimization.html#solution-3"><i class="fa fa-check"></i><b>6.8.8</b> Solution</a></li>
<li class="chapter" data-level="6.8.9" data-path="optimization.html"><a href="optimization.html#exercise-5-1"><i class="fa fa-check"></i><b>6.8.9</b> Exercise 5</a></li>
<li class="chapter" data-level="6.8.10" data-path="optimization.html"><a href="optimization.html#solution-4"><i class="fa fa-check"></i><b>6.8.10</b> Solution</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative biology: a quick introduction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-classification-and-machine-learning" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Introduction to classification and machine learning</h1>
<p>In this module we will continue our exploration of techniques for multivariate data (see Module 4 on Clustering and ordination), but will pay more attention to machine learning approaches. Classification is the task of assigning data objects, such as sites, species or images to predetermined classes. Determining what class of data object you have is a question that usually turns on multiple predictors. For example, to classify leaf images to different species, predictors such as size, shape and colour may be used. If you have satellite data, you may need to classify the different pixels of the image as agricultural, forest, or urban. So for classification tasks our response variable, y, is qualitative or categorical (e.g., gender, species, land classification).</p>
<p>There are many methods that can be employed for classification tasks ranging from logistic regression to random forest techniques. While some of these methods are classic multivariate methods, others, like random forest classifiers, are machine learning tasks. Machine learning is an application of artificial intelligence, which allows algorithms to become more accurate at predicting outcomes without being explicitly programmed to do so. When we fit a regression in the ordinary way, we specify the model ahead of time and determine if the model has good or bad fit. We might then modify our model to reduce prediction error. In machine learning, the algorithm itself completes the tasks of model specification, evaluation and improvement. Spooky!</p>
<p>The main thing to remember about machine learning is that these methods develop their own models for prediction. For example, in image recognition, they might learn to identify images that contain different species of zooplankton by analyzing example images that have been previously labelled by the researcher. However, generally there is no information provided about what rules the researcher used to identify the images (e.g., size, shape, etc). Instead the machine learning tool develops its own framework for identification, which is one reason why the solution from the machine learning routine can seem like a black box. Once trained, the algorithm can identify the species of unlabelled images, and may or may not be using the same kind of identifiers that a researcher would use to manually process the images.</p>
<div id="logistic-regression-as-a-binary-classifier" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Logistic regression as a binary classifier</h2>
<p>One of the simplest classification methods, and one that does not involve machine learning, is logistic regression. Let’s take a common example. A non-native species has been introduced to a region, and we would like to know what percentage of the region would be suitable habitat, in order to get an idea of risks of impact on the native ecosystem. We think that average temperature controls habitat suitability, and we have presence/absence data for the species across a range of different sites. Could we use simple regression to answer the question of whether a given area is suitable habitat? If we indicate absence as 0, and presence as 1, we can regress species occurrence again average annual temperatures at each location.</p>
<div class="figure"><span id="fig:log"></span>
<img src="_main_files/figure-html/log-1.png" alt="This figure shows a plot with y-axis labelled 'Species occurance' and x-axis labelled 'Mean annual temperature (°C)'. The plot shows a set of points where 'Species presence/absence' is at 0 and 1. There is a red fitted line, based on the plotted points, that trends upwards." width="672" />
<p class="caption">
Figure 5.1: Species presence/absence and mean annual temperature with linear regression
</p>
</div>
<p>As we can see in Fig. <a href="introduction-to-classification-and-machine-learning.html#fig:log">5.1</a>, the linear regression does not make a lot of sense for a response variable that is restricted to the values of 0 and 1. The regression line <span class="math inline">\(\beta_0+\beta_1x\)</span> can take on any value between negative and positive infinity, but we don’t know how to interpret values greater than 1 or less than zero. The regression line almost always predicts wrong value for y in classification problems.</p>
<p>Instead of trying to predict y, we can try to predict p(y = 1), i.e., the probability that the species will be found in the area. For invasive species, this probability is often interpreted as habitat suitability for the species. We need a function that gives outputs between 0 and 1: logistic regression is one solution. In this model, probability of y for a given value of x, p(x) is given as: <span class="math display">\[\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}.\]</span> Rearranging, we have: <span class="math display">\[\frac{p(x)}{1-p(x)}=e^{\beta_0+\beta_1x}.\]</span> Taking the natural logarithm, we can see that the logistic regression is linear in x: <span class="math display">\[\log{\frac{p(x)}{1-p(x)}}=\beta_0+\beta_1x,\]</span> where the lefthand side is called the log-odds or logit. The logistic function will always produce an S-shaped curve, so regardless of the value of x, we will obtain a sensible prediction.</p>
<p>Let’s apply this model to our non-native species data using the <strong>g</strong>eneralized <strong>l</strong>inear <strong>m</strong>odel or <strong>glm()</strong> function. We use the glm() function to perform logistic regression by passing in the family=“binomial” argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the <strong>l</strong>inear <strong>m</strong>odel or <strong>lm()</strong> function. We will fit the model and then get the predictions.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="introduction-to-classification-and-machine-learning.html#cb89-1" aria-hidden="true" tabindex="-1"></a>glm.fit.sp <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">family =</span> binomial)</span>
<span id="cb89-2"><a href="introduction-to-classification-and-machine-learning.html#cb89-2" aria-hidden="true" tabindex="-1"></a>glm.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fit.sp, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:logfit"></span>
<img src="_main_files/figure-html/logfit-1.png" alt="This figure shows a plot with y-axis labelled 'Species occurance' and x-axis labelled 'Mean annual temperature (°C). There are a set of points where 'Species occurance' is at 0 and 1. There is a red logistic regression line based on the plotted points." width="672" />
<p class="caption">
Figure 5.2: Species presence/absence and mean annual temperature with logistic regression
</p>
</div>
<div id="interpreting-the-logistic-regression" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Interpreting the logistic regression</h3>
<p>Let’s take a closer look at the output for our logistic regression (Fig. <a href="introduction-to-classification-and-machine-learning.html#fig:logfit">5.2</a>). First off, we need to know if the intercept, <span class="math inline">\(\beta_0\)</span>, and slope, <span class="math inline">\(\beta_1\)</span>, are significantly different from zero. A z distribution is used for this test, and we find that both the intercept and slope are significantly different from zero (Table <a href="introduction-to-classification-and-machine-learning.html#tab:logcoef">5.1</a>). Our p-values are very, very small, so our model is doing better than random chance.</p>
<table>
<caption>
<span id="tab:logcoef">Table 5.1: </span>Logistic regression coefficient estimates and hypothesis tests from species occurrence data
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Std. Error
</th>
<th style="text-align:right;">
z value
</th>
<th style="text-align:right;">
Pr(&gt;|z|)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
-8.87
</td>
<td style="text-align:right;">
1.38
</td>
<td style="text-align:right;">
-6.41
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
x
</td>
<td style="text-align:right;">
0.62
</td>
<td style="text-align:right;">
0.10
</td>
<td style="text-align:right;">
5.97
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p>The estimated intercept is typically not of interest. Its main purpose is to adjust the average fitted probability to the proportion of ones in the data. You may be confused by the slope value. Interpreting what <span class="math inline">\(\beta_1\)</span> means is not very easy with logistic regression, simply because we are predicting p(y) and not y. If <span class="math inline">\(\beta_1\)</span> = 0, this means there is no relationship between p(y) and x. If <span class="math inline">\(\beta_1\)</span> &gt; 0, this means that when y gets larger so does the probability that y = 1. If <span class="math inline">\(\beta_1\)</span> &lt; 0, this means that when x gets larger, the probability that y = 1 gets smaller. Our <span class="math inline">\(\beta_1\)</span> is positive, so we are sure that as temperature increases, the probability of habitat suitability will increase as well. For example, suppose a region has an average annual temperature of 12°C. We know that the probability that the habitat is suitable p(y) for our invasive species is: <span class="math display">\[p(y)=\frac{e^{paste \beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}.\]</span>
Substituting in our fitted values from the logistic regression, we have:
<span class="math display">\[p(y)=\frac{e^{-8.87 + 0.62 (12)}}{1+e^{-8.87 + 0.62(12)}}=0.19.\]</span>
The value changes with temperature, so for an average annual temperature of 16°C that probability is 0.74, and so on.</p>
<p><strong>Exercise 1</strong> Try this for yourself, see if you can use similar code in order to estimate the probability that a site with an average annual temperature of 18°C will be occupied by the invasive species.</p>
</div>
<div id="measuring-the-performance-of-a-binary-classifier" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Measuring the performance of a binary classifier</h3>
<p>Of course, the data used to fit this relationship is zeros (absence) and ones (present). How well does our model do at predicting species occurrence? In order to answer this question we need to decide on a <strong>threshold value</strong> for the probability prediction. For example, an easy one is 50%. If the model predicts a probability of greater than 0.5, then we will score that as presence, while if the predicted value is equal to or less than 0.5, we will score as an absence. Now we have model predictions in terms of predicted absences and presences (i.e., zeros and ones), and can compare directly to our data.</p>
<p>When binary classifications are made, by converting the probabilities using a threshold, there can be four cases for a certain observation:</p>
<ol style="list-style-type: decimal">
<li><p>The response actually negative, the model predicts it to be negative. This is known as true negative (TN). In our case, the invasive species is not present at the location, and the model predicts that it should not be present.</p></li>
<li><p>The response actually negative, but the model predicts it to be positive (i.e., false positive, FP).</p></li>
<li><p>The response actually positive, and the model predicts it to be positive (i.e., true positive TP).</p></li>
<li><p>The response actually positive, but the model predicts it to be negative (i.e., false negative FN).</p></li>
</ol>
<p>We can summarize this information in a <strong>confusion matrix</strong> which records the number of times the model correctly predicted the data, and the number of times the model makes incorrect predictions.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="introduction-to-classification-and-machine-learning.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># determine model occurrence predictions based on threshold</span></span>
<span id="cb90-2"><a href="introduction-to-classification-and-machine-learning.html#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="co"># value</span></span>
<span id="cb90-3"><a href="introduction-to-classification-and-machine-learning.html#cb90-3" aria-hidden="true" tabindex="-1"></a>logocc <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(glm.probs <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb90-4"><a href="introduction-to-classification-and-machine-learning.html#cb90-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-5"><a href="introduction-to-classification-and-machine-learning.html#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a confusion matrix</span></span>
<span id="cb90-6"><a href="introduction-to-classification-and-machine-learning.html#cb90-6" aria-hidden="true" tabindex="-1"></a>ctab <span class="ot">=</span> <span class="fu">table</span>(logocc, y)</span>
<span id="cb90-7"><a href="introduction-to-classification-and-machine-learning.html#cb90-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dimnames</span>(ctab) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">Actual =</span> <span class="fu">c</span>(<span class="st">&quot;absence(0)&quot;</span>, <span class="st">&quot;presence(1)&quot;</span>),</span>
<span id="cb90-8"><a href="introduction-to-classification-and-machine-learning.html#cb90-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">Predicted =</span> <span class="fu">c</span>(<span class="st">&quot;absence(0)&quot;</span>, <span class="st">&quot;presence(1)&quot;</span>))</span>
<span id="cb90-9"><a href="introduction-to-classification-and-machine-learning.html#cb90-9" aria-hidden="true" tabindex="-1"></a>ctab</span></code></pre></div>
<pre><code>             Predicted
Actual        absence(0) presence(1)
  absence(0)         128          33
  presence(1)         17          22</code></pre>
<p>Elements on the diagonal from left to right are correct classifications, while off-diagonal elements are miss-classifications (i.e., predictions where the predict code of 0 or 1 does not match the actual code of 0 or 1). So in this case, we have 128 true negatives (TN), 33 false positives (FP), 17 false negatives (FN), and 22 true positives (TP).</p>
<p>We can quantify these errors in a number of ways. The classifications rate, or <strong>error rate</strong> is the most common metric used to quantify the performance of a binary classifier. This is the probability that the classifier makes a wrong prediction (given as <span class="math inline">\(\frac{FN+FP}{TN+FN+TP+FP}\)</span>). Overall 50 observations have been misclassified, so we have a total error rate of 25%. However, 44% of the presence data has been misclassified, compared to 20% of the absence data. So with respect to the predicting the potential presence of an invasive species, we’re not doing much better than random chance.</p>
<p>The terms sensitivity and specificity characterize the performance of classifier for these specific types of errors. In this case, the
<strong>sensitivity</strong> is the percentage of occupancy locations that are correctly identified (true positives), which is 56% in this case or one minus the misclassification rate of positives (or 1-0.44), also calculated as TP/(TP+FN). <strong>Specificity</strong> is the percentage of non-occupancy sites that are correctly identified (true negatives). We can calculate this as one minus the misclassification of true negatives from the values above (1 − 0.2) = 0.8, or from the formula TN/(TN+FP).</p>
<p>We can of course, change the decision threshold to see if we can get a better outcome. Let’s try 0.65 instead of 0.5.</p>
<pre><code>      Predicted
Actual   0   1
     0 136  44
     1   9  11</code></pre>
<p>This higher threshold gives us an error rate of 26%, sensitivity of 55% and specificity of 76%, so not much improvement.</p>
<p><strong>Exercise 2</strong> Can you calculate the sensitivity and specificity for a threshold classification of 0.45?</p>
<div id="roc-and-auc" class="section level4" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> ROC and AUC</h4>
<p>We can also examine the performance of the model across a range of thresholds. We often see this approach in species distribution modelling, where specificity (% of true positives) is plotted against 1-sensitivity (% of false positives) for threshold values from 0 to 1. This graph is called the <strong>R</strong>eceiver <strong>O</strong>perator <strong>C</strong>urve (<strong>ROC</strong>), and the <strong>A</strong>rea <strong>U</strong>nder that <strong>C</strong>urve (<strong>AUC</strong>) is calculated. This is a fairly standard evaluation for binary classifiers, and there are a number of R packages that will complete this analysis for you. If the model is not performing better than random chance, the expected ROC curve is simply the y=x line. Where the model can perfectly separate the two classes, the ROC curve consists of a vertical line (x=0) and a horizontal line (y=1). For real and simulated data, usually the ROC stays in between these two extreme scenarios. Let’s try with our simulated data.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="introduction-to-classification-and-machine-learning.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ROCit)</span>
<span id="cb93-2"><a href="introduction-to-classification-and-machine-learning.html#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="introduction-to-classification-and-machine-learning.html#cb93-3" aria-hidden="true" tabindex="-1"></a>ROCit_obj <span class="ot">&lt;-</span> <span class="fu">rocit</span>(<span class="at">score =</span> glm.fit.sp<span class="sc">$</span>fitted.values, <span class="at">class =</span> y)</span>
<span id="cb93-4"><a href="introduction-to-classification-and-machine-learning.html#cb93-4" aria-hidden="true" tabindex="-1"></a>pauc <span class="ot">=</span> <span class="fu">plot</span>(ROCit_obj)</span></code></pre></div>
<div class="figure"><span id="fig:roc2"></span>
<img src="_main_files/figure-html/roc2-1.png" alt="Sensitivity (TPR) ploted versus 1-Specificity (FPR) and the y=x line, where the optimal threshold is also indicated" width="672" />
<p class="caption">
Figure 5.3: Receiver Operator Curve (ROC) for the logistic regression binary classifier of species occurence data
</p>
</div>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="introduction-to-classification-and-machine-learning.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ROCit_obj)</span></code></pre></div>
<pre><code>                           
 Method used: empirical    
 Number of positive(s): 55 
 Number of negative(s): 145
 Area under curve: 0.8361  </code></pre>
<p>Overall, we have an area under the ROC curve (Fig.<a href="introduction-to-classification-and-machine-learning.html#fig:roc2">5.3</a>) of 0.84, which is not bad, given the maximum value is one. The optimal threshold value is given by the Youden index as 0.66. The Youden index maximizes the difference between sensitivity and 1-specificity and is defined as sensitivity+specificity-1. Let’s try this threshold directly:</p>
<pre><code>      Predicted
Actual   0   1
     0 139  46
     1   6   9</code></pre>
<p>We can see that this error threshold, as summarized above, gives an error rate of 26%, sensitivity of 60% and specificity of 75%. Overall, an okay, but not fantastic model, at the best performing threshold.</p>
</div>
</div>
<div id="multiple-logistic-regression" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Multiple logistic regression</h3>
<p>If we have more than one predictor, we can fit multiple logistic just like regular regression, as:
<span class="math display">\[p(y)=\frac{e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}{1+e^{\beta_0+\beta_1x_1+...+\beta_nx_n}}\]</span>
and the <span class="math inline">\(x_n\)</span> predictors can be both qualitative or quantitative. For example, we could add a land classification to our invasive species habitat suitability model so that we have both temperature (<span class="math inline">\(x_1\)</span>) and urban and rural (<span class="math inline">\(x_2\)</span>) land types.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="introduction-to-classification-and-machine-learning.html#cb97-1" aria-hidden="true" tabindex="-1"></a>glm.fit.sp2 <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> x <span class="sc">+</span> <span class="fu">as.factor</span>(x2), <span class="at">family =</span> binomial)</span></code></pre></div>
<p>In this case, our model now has two responses, one for land categorized as urban, and one for land categorized as rural (Fig. <a href="introduction-to-classification-and-machine-learning.html#fig:multilog">5.4</a>).</p>
<table>
<caption>
<span id="tab:logcoefs2">Table 5.2: </span>Logistic regression coefficient estimates and hypothesis tests from species occurrence data with temperature and land category as predictors
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Std. Error
</th>
<th style="text-align:right;">
z value
</th>
<th style="text-align:right;">
Pr(&gt;|z|)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
-8.79
</td>
<td style="text-align:right;">
1.37
</td>
<td style="text-align:right;">
-6.40
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
x
</td>
<td style="text-align:right;">
0.60
</td>
<td style="text-align:right;">
0.10
</td>
<td style="text-align:right;">
5.79
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
as.factor(x2)urban
</td>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.42
</td>
<td style="text-align:right;">
1.87
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
</tbody>
</table>
Both predictors are significantly different from zero (Table <a href="introduction-to-classification-and-machine-learning.html#tab:logcoefs2">5.2</a>), and the values tell is that if the land is categorized as urban, we must increase our probability estimate upwards from that of a rural area as:<br />
<span class="math display">\[p(y)=\frac{e^{-8.79 + 0.6 x_1+0.79 (1)}}{1+e^{-8.79 + 0.6x_1+0.79 (1)}}\]</span>
<div class="figure"><span id="fig:multilog"></span>
<img src="_main_files/figure-html/multilog-1.png" alt="This figure shows a plot with y-axis labelled 'Species occurrence' and x-axis labelled 'Mean annual temperature (°C)'. The top left of the plot contains a legend with label 'urban' and 'rural' which are denoted by a red circle and a black circle respectively. The plot shows a set of 'urban' red points and 'rural' black points where 'Species occurance' is at 0 and 1. There are two logistic regression lines that represent the land categories. The 'urban' red logistic regression line is above the 'rural' black logistic regression line." width="672" />
<p class="caption">
Figure 5.4: Species occurrence vs temperature and land classification as urban or rural
</p>
</div>
<p>Logistic regression can be extended to multiple classification problems in different ways, but in practice these methods tend not to be used all that often. Instead other techniques such as linear discriminant analysis and random forest tend to be used for multiple-class classification problems (see below).</p>
</div>
</div>
<div id="cross-validation" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Cross-validation</h2>
<p>So far, we’ve evaluated model performance with the data that we used to train the model, but the point of classification tools is to be able to use them on data where we don’t already know the answer. In that sense, we are uninterested in in model performance on <strong>training data</strong>, what we really want to do is to test the model on data that was not used in model fitting. For example, we don’t really care how well our method predicts habitat suitability where the invasive species is already located! What we need to know is how well it predicts the habitat suitability of locations where the species has not yet invaded. The model performance on this <strong>testing data</strong> will give us a better idea of the errors we might expect when we apply our classifier to novel data. While we might naively expect that model performance on the training data will be the same on the testing data, in practice the errors are usually larger, sometimes much larger. In more complex models, this error rate is often the result of overfitting the training data, so that pattern which is just noise is included in the model fit. Consequently, the model is not well fit to data with different sources of noise.</p>
<p>Of course, in biological data is almost always limited! While you might have an extra independent dataset kicking around waiting to be used for model testing, if you don’t, you can divide your single dataset into training and testing sets. One easy way to do this is just using random selection. Let’s try on our data. We’ll divide a dataframe with our data into two parts using the <em>sample()</em> function, fit our logistic model on the training data, and evaluate its performance on the testing data.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="introduction-to-classification-and-machine-learning.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate some temperature and occupancy data using a</span></span>
<span id="cb98-2"><a href="introduction-to-classification-and-machine-learning.html#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="co"># random number generator</span></span>
<span id="cb98-3"><a href="introduction-to-classification-and-machine-learning.html#cb98-3" aria-hidden="true" tabindex="-1"></a>xs <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">12</span>, <span class="fl">2.5</span>)</span>
<span id="cb98-4"><a href="introduction-to-classification-and-machine-learning.html#cb98-4" aria-hidden="true" tabindex="-1"></a>ys <span class="ot">=</span> <span class="fu">ifelse</span>(xs <span class="sc">&gt;</span> <span class="dv">12</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb98-5"><a href="introduction-to-classification-and-machine-learning.html#cb98-5" aria-hidden="true" tabindex="-1"></a>es <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb98-6"><a href="introduction-to-classification-and-machine-learning.html#cb98-6" aria-hidden="true" tabindex="-1"></a><span class="co"># adding some randomness to simulated occupancy data</span></span>
<span id="cb98-7"><a href="introduction-to-classification-and-machine-learning.html#cb98-7" aria-hidden="true" tabindex="-1"></a>ys <span class="ot">=</span> <span class="fu">ifelse</span>(ys <span class="sc">+</span> es <span class="sc">&gt;=</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb98-8"><a href="introduction-to-classification-and-machine-learning.html#cb98-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-9"><a href="introduction-to-classification-and-machine-learning.html#cb98-9" aria-hidden="true" tabindex="-1"></a><span class="co"># create a dataframe with our temperature and occupancy</span></span>
<span id="cb98-10"><a href="introduction-to-classification-and-machine-learning.html#cb98-10" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb98-11"><a href="introduction-to-classification-and-machine-learning.html#cb98-11" aria-hidden="true" tabindex="-1"></a>ivsp <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">temp =</span> xs, <span class="at">occ =</span> ys)</span>
<span id="cb98-12"><a href="introduction-to-classification-and-machine-learning.html#cb98-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-13"><a href="introduction-to-classification-and-machine-learning.html#cb98-13" aria-hidden="true" tabindex="-1"></a><span class="co"># randomly sample 75% of the data (by generating random</span></span>
<span id="cb98-14"><a href="introduction-to-classification-and-machine-learning.html#cb98-14" aria-hidden="true" tabindex="-1"></a><span class="co"># numbers based on the number of rows)</span></span>
<span id="cb98-15"><a href="introduction-to-classification-and-machine-learning.html#cb98-15" aria-hidden="true" tabindex="-1"></a>samp <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(ivsp), <span class="fu">nrow</span>(ivsp) <span class="sc">*</span> <span class="fl">0.75</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb98-16"><a href="introduction-to-classification-and-machine-learning.html#cb98-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-17"><a href="introduction-to-classification-and-machine-learning.html#cb98-17" aria-hidden="true" tabindex="-1"></a><span class="co"># divide into training and testing sets</span></span>
<span id="cb98-18"><a href="introduction-to-classification-and-machine-learning.html#cb98-18" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> ivsp[samp, ]</span>
<span id="cb98-19"><a href="introduction-to-classification-and-machine-learning.html#cb98-19" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> ivsp[<span class="sc">-</span>samp, ]</span>
<span id="cb98-20"><a href="introduction-to-classification-and-machine-learning.html#cb98-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-21"><a href="introduction-to-classification-and-machine-learning.html#cb98-21" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the logistic model on the training data</span></span>
<span id="cb98-22"><a href="introduction-to-classification-and-machine-learning.html#cb98-22" aria-hidden="true" tabindex="-1"></a>log.fit.inv <span class="ot">=</span> <span class="fu">glm</span>(occ <span class="sc">~</span> temp, <span class="at">family =</span> binomial, <span class="at">data =</span> train)</span>
<span id="cb98-23"><a href="introduction-to-classification-and-machine-learning.html#cb98-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-24"><a href="introduction-to-classification-and-machine-learning.html#cb98-24" aria-hidden="true" tabindex="-1"></a><span class="co"># test the logistic model on the testing data</span></span>
<span id="cb98-25"><a href="introduction-to-classification-and-machine-learning.html#cb98-25" aria-hidden="true" tabindex="-1"></a>log.predict <span class="ot">&lt;-</span> <span class="fu">predict</span>(log.fit.inv, <span class="at">newdata =</span> test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb98-26"><a href="introduction-to-classification-and-machine-learning.html#cb98-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-27"><a href="introduction-to-classification-and-machine-learning.html#cb98-27" aria-hidden="true" tabindex="-1"></a><span class="co"># determine predicted occupancy based on threshold value of</span></span>
<span id="cb98-28"><a href="introduction-to-classification-and-machine-learning.html#cb98-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.5</span></span>
<span id="cb98-29"><a href="introduction-to-classification-and-machine-learning.html#cb98-29" aria-hidden="true" tabindex="-1"></a>pred.occ <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(log.predict <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb98-30"><a href="introduction-to-classification-and-machine-learning.html#cb98-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-31"><a href="introduction-to-classification-and-machine-learning.html#cb98-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a confusion matrix</span></span>
<span id="cb98-32"><a href="introduction-to-classification-and-machine-learning.html#cb98-32" aria-hidden="true" tabindex="-1"></a>ctab <span class="ot">=</span> <span class="fu">table</span>(pred.occ, test<span class="sc">$</span>occ)</span>
<span id="cb98-33"><a href="introduction-to-classification-and-machine-learning.html#cb98-33" aria-hidden="true" tabindex="-1"></a><span class="fu">dimnames</span>(ctab) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">Actual =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">Predicted =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb98-34"><a href="introduction-to-classification-and-machine-learning.html#cb98-34" aria-hidden="true" tabindex="-1"></a>ctab</span></code></pre></div>
<pre><code>      Predicted
Actual  0  1
     0 35 11
     1  1  3</code></pre>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="introduction-to-classification-and-machine-learning.html#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate error rate, sensitivity and specificity</span></span>
<span id="cb100-2"><a href="introduction-to-classification-and-machine-learning.html#cb100-2" aria-hidden="true" tabindex="-1"></a>err <span class="ot">=</span> <span class="fu">round</span>((ctab[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">+</span> ctab[<span class="dv">2</span>, <span class="dv">1</span>])<span class="sc">/</span><span class="fu">sum</span>(ctab), <span class="dv">2</span>)</span>
<span id="cb100-3"><a href="introduction-to-classification-and-machine-learning.html#cb100-3" aria-hidden="true" tabindex="-1"></a>sens <span class="ot">=</span> <span class="fu">round</span>(ctab[<span class="dv">2</span>, <span class="dv">2</span>]<span class="sc">/</span>(ctab[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">+</span> ctab[<span class="dv">2</span>, <span class="dv">2</span>]), <span class="dv">2</span>)</span>
<span id="cb100-4"><a href="introduction-to-classification-and-machine-learning.html#cb100-4" aria-hidden="true" tabindex="-1"></a>spec <span class="ot">=</span> <span class="fu">round</span>(<span class="dv">1</span> <span class="sc">-</span> ctab[<span class="dv">1</span>, <span class="dv">2</span>]<span class="sc">/</span>(ctab[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> ctab[<span class="dv">1</span>, <span class="dv">2</span>]), <span class="dv">2</span>)</span>
<span id="cb100-5"><a href="introduction-to-classification-and-machine-learning.html#cb100-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-6"><a href="introduction-to-classification-and-machine-learning.html#cb100-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;error rate=&quot;</span>, err, <span class="st">&quot;; sensitivity=&quot;</span>, sens, <span class="st">&quot;; specificity=&quot;</span>,</span>
<span id="cb100-7"><a href="introduction-to-classification-and-machine-learning.html#cb100-7" aria-hidden="true" tabindex="-1"></a>    spec))</span></code></pre></div>
<pre><code>[1] &quot;error rate=0.24; sensitivity=0.75; specificity=0.76&quot;</code></pre>
<p>You may want to verify for yourself that the sample function randomly selects rows out of a dataframe</p>
<p>You’ll notice that by dividing the data up this way we have only 50 observations in our testing data. We can of course use different percentages to divide up our one dataset into training and testing sets, but models tend to have poorer performance when trained on fewer observations. On the other hand, the small testing dataset may tend to overestimate the test error rate for the model fit, as compared to error rates obtained on a larger amount of data.</p>
<p>And what about the effects of that random sampling? If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the error rate on testing data each time, because different observations will be randomly included in the dataset each time. Sometimes the differences between error rates on different testing datasets can be rather large. For example, by random selection an observation which is a huge outlier could be included in one small testing dataset, but not in another, resulting is very different error rates. To guard against undue influence of single observations in our small test dataset we could do the routine of randomly sampling to obtain testing and training sets several times, and look at the average of our testing data performance.</p>
<div id="k-fold-cross-validation" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> k-Fold Cross-Validation</h3>
<p>One way of implementing this type of resampling scheme is k-fold cross-validation. With this method, we randomly divide the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a testing set, and the model is fit on the remaining k − 1 folds. This procedure is repeated k times, and each time, a different group of observations is treated as the testing set. We then average the error rates from each test fold. We can even repeat the entire procedure several times in <strong>repeated k-fold cross-validation</strong>.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="introduction-to-classification-and-machine-learning.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example code for iterated cross-validation set up</span></span>
<span id="cb102-2"><a href="introduction-to-classification-and-machine-learning.html#cb102-2" aria-hidden="true" tabindex="-1"></a>reps <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb102-3"><a href="introduction-to-classification-and-machine-learning.html#cb102-3" aria-hidden="true" tabindex="-1"></a>nfolds <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb102-4"><a href="introduction-to-classification-and-machine-learning.html#cb102-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-5"><a href="introduction-to-classification-and-machine-learning.html#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>reps) {</span>
<span id="cb102-6"><a href="introduction-to-classification-and-machine-learning.html#cb102-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-7"><a href="introduction-to-classification-and-machine-learning.html#cb102-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate array containing fold-number for each sample</span></span>
<span id="cb102-8"><a href="introduction-to-classification-and-machine-learning.html#cb102-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (row)</span></span>
<span id="cb102-9"><a href="introduction-to-classification-and-machine-learning.html#cb102-9" aria-hidden="true" tabindex="-1"></a>    foldsset <span class="ot">&lt;-</span> <span class="fu">rep_len</span>(<span class="dv">1</span><span class="sc">:</span>nfolds, <span class="fu">nrow</span>(data))</span>
<span id="cb102-10"><a href="introduction-to-classification-and-machine-learning.html#cb102-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-11"><a href="introduction-to-classification-and-machine-learning.html#cb102-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create subsetes of data based on the random fold</span></span>
<span id="cb102-12"><a href="introduction-to-classification-and-machine-learning.html#cb102-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># assignment</span></span>
<span id="cb102-13"><a href="introduction-to-classification-and-machine-learning.html#cb102-13" aria-hidden="true" tabindex="-1"></a>    folds <span class="ot">&lt;-</span> <span class="fu">sample</span>(foldsset, <span class="fu">nrow</span>(data))</span>
<span id="cb102-14"><a href="introduction-to-classification-and-machine-learning.html#cb102-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-15"><a href="introduction-to-classification-and-machine-learning.html#cb102-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># actual cross validation: we allow each fold to act as</span></span>
<span id="cb102-16"><a href="introduction-to-classification-and-machine-learning.html#cb102-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the test data in turn</span></span>
<span id="cb102-17"><a href="introduction-to-classification-and-machine-learning.html#cb102-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nfolds) {</span>
<span id="cb102-18"><a href="introduction-to-classification-and-machine-learning.html#cb102-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-19"><a href="introduction-to-classification-and-machine-learning.html#cb102-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># split of the data</span></span>
<span id="cb102-20"><a href="introduction-to-classification-and-machine-learning.html#cb102-20" aria-hidden="true" tabindex="-1"></a>        fold <span class="ot">&lt;-</span> <span class="fu">which</span>(folds <span class="sc">==</span> k)</span>
<span id="cb102-21"><a href="introduction-to-classification-and-machine-learning.html#cb102-21" aria-hidden="true" tabindex="-1"></a>        data.train <span class="ot">&lt;-</span> data[<span class="sc">-</span>fold, ]</span>
<span id="cb102-22"><a href="introduction-to-classification-and-machine-learning.html#cb102-22" aria-hidden="true" tabindex="-1"></a>        data.test <span class="ot">&lt;-</span> data[fold, ]</span>
<span id="cb102-23"><a href="introduction-to-classification-and-machine-learning.html#cb102-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-24"><a href="introduction-to-classification-and-machine-learning.html#cb102-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># train and test your model with data.train and</span></span>
<span id="cb102-25"><a href="introduction-to-classification-and-machine-learning.html#cb102-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># data.test</span></span>
<span id="cb102-26"><a href="introduction-to-classification-and-machine-learning.html#cb102-26" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb102-27"><a href="introduction-to-classification-and-machine-learning.html#cb102-27" aria-hidden="true" tabindex="-1"></a>}  <span class="co"># repeat for the desired number of interactions </span></span></code></pre></div>
<p><strong>Exercise 3: Logistic regression as a binary classifier</strong></p>
<p>Try to use logistic regression on your own, with a pre-existing dataset in the MASS package. Start by installing the MASS package, load the library and load the data. The dataset we will use has been previously divided into training (Pima.tr) and testing sets (Pima.te). The dataset describes risk factors for diabetes. Type help(Pima.tr) or ?Pima.tr to get a description. You’ll notice that the “type” variable is our classifier and determines whether the patient has diabetes or not.</p>
<p>Next construct a logistic regression to use as a classifier. Examine your output to determine if the regression is significant. We can use “~.” in the formula argument to mean that we use all the remaining variables in the dataset as predictors.</p>
<p>Next, use some testing data to test your classifier. The Prima.te dataset to be used as testing data has already been created for you in the MASS package. Use the predict function to get the predicted probabilities, and a threshold value to get classifications. Then construct a confusion matrix to determine how well your predictor did.</p>
<p>There is a nice package in R, <em>caret</em>, that is a good wrapper for these kinds of tasks, and which is really great for machine learning stuff (we’ll use in the random forest and artificial neural network sections). We can use the caret package to generate both our confusion matrix and other statistical info about our classifier (where we have saved the predictions of our logistic regression to the dataframe testPima.</p>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  200  23
       Yes  43  66
                                          
               Accuracy : 0.8012          
                 95% CI : (0.7542, 0.8428)
    No Information Rate : 0.7319          
    P-Value [Acc &gt; NIR] : 0.002095        
                                          
                  Kappa : 0.5271          
                                          
 Mcnemar&#39;s Test P-Value : 0.019349        
                                          
            Sensitivity : 0.8230          
            Specificity : 0.7416          
         Pos Pred Value : 0.8969          
         Neg Pred Value : 0.6055          
             Prevalence : 0.7319          
         Detection Rate : 0.6024          
   Detection Prevalence : 0.6717          
      Balanced Accuracy : 0.7823          
                                          
       &#39;Positive&#39; Class : No              
                                          </code></pre>
<p>Perhaps the most informative of these stats is the <em>No Information Rate</em> which tests whether our classifier does better than random assignment. We see that our accuracy rate is significantly greater than this no information rate, and so, this should be an okay classifier. Also, the <em>Balanced Accuracy Statistic</em> gives an accuracy value that weights both majority and minority classes evenly, and is useful if you have unbalanced class membership in your data. In our case, the accuracy and balance accuracy rates are similar, so we’re doing alright.</p>
<p>The caret package can be used to do lots of the programming required for cross-validation approaches automatically. Going back to the species occurrence data, let’s use the caret package to run a k-fold cross-validation, with 5 folds, rather than simply dividing the data into training and testing sets.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="introduction-to-classification-and-machine-learning.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the resampling method you want to use, in this</span></span>
<span id="cb104-2"><a href="introduction-to-classification-and-machine-learning.html#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="co"># case cross-validation with 5 folds</span></span>
<span id="cb104-3"><a href="introduction-to-classification-and-machine-learning.html#cb104-3" aria-hidden="true" tabindex="-1"></a>train_control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>)</span>
<span id="cb104-4"><a href="introduction-to-classification-and-machine-learning.html#cb104-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-5"><a href="introduction-to-classification-and-machine-learning.html#cb104-5" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model on training set, remember to tell caret</span></span>
<span id="cb104-6"><a href="introduction-to-classification-and-machine-learning.html#cb104-6" aria-hidden="true" tabindex="-1"></a><span class="co"># that this is a classifier by indicating that occ should</span></span>
<span id="cb104-7"><a href="introduction-to-classification-and-machine-learning.html#cb104-7" aria-hidden="true" tabindex="-1"></a><span class="co"># be considered a factor variable</span></span>
<span id="cb104-8"><a href="introduction-to-classification-and-machine-learning.html#cb104-8" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="fu">as.factor</span>(occ) <span class="sc">~</span> temp, <span class="at">data =</span> ivsp, <span class="at">trControl =</span> train_control,</span>
<span id="cb104-9"><a href="introduction-to-classification-and-machine-learning.html#cb104-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="at">family =</span> <span class="fu">binomial</span>())</span>
<span id="cb104-10"><a href="introduction-to-classification-and-machine-learning.html#cb104-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-11"><a href="introduction-to-classification-and-machine-learning.html#cb104-11" aria-hidden="true" tabindex="-1"></a><span class="co"># print cv scores</span></span>
<span id="cb104-12"><a href="introduction-to-classification-and-machine-learning.html#cb104-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>
Call:
NULL

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9180  -0.6162  -0.4012  -0.1155   1.8648  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -8.8704     1.4078  -6.301 2.96e-10 ***
temp          0.6093     0.1064   5.729 1.01e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 220.43  on 199  degrees of freedom
Residual deviance: 170.39  on 198  degrees of freedom
AIC: 174.39

Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="introduction-to-classification-and-machine-learning.html#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(model)</span></code></pre></div>
<pre><code>Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction    0    1
         0 68.5 17.5
         1  7.5  6.5
                          
 Accuracy (average) : 0.75</code></pre>
<p>The caret package neatly does our data resampling, fits the model, gets an average model performance across the 5 testing sets, AND calculates the confusion matrix for us. Not bad!</p>
</div>
<div id="examples-of-logistic-regression-used-for-classification" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Examples of logistic regression used for classification</h3>
<p>While there is an emphasis in the literature on more complex classification methods, we should point out that we have spent a significant amount of time on this relatively simple method not just for pedagogical reasons. Logistic regression is sometimes just as effective, or even more effective, at classification problems, and further, has the merit of being easy to understand.</p>
<p>Cuddington, K., Sobek-Swant, S., Drake, J., Lee, W., &amp; Brook, M. (2021). Risks of giant hogweed (Heracleum mantegazzianum) range increase in North America. Biological Invasions. <a href="https://doi.org/10.1007/s10530-021-02645-x" class="uri">https://doi.org/10.1007/s10530-021-02645-x</a></p>
<p>Tuda, M., &amp; Luna-Maldonado, A. I. (2020). Image-based insect species and gender classification by trained supervised machine learning algorithms. Ecological Informatics, 60, 101135. <a href="https://doi.org/10.1016/j.ecoinf.2020.101135" class="uri">https://doi.org/10.1016/j.ecoinf.2020.101135</a></p>
</div>
</div>
<div id="linear-discriminant-analysis-lda" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> <strong>L</strong>inear <strong>D</strong>iscriminant <strong>A</strong>nalysis (<strong>LDA</strong>)</h2>
<p>While there are ways to modify logistic regression to classify more that two types of objects, in practice there are better methods such as classification linear discriminant analysis or various machine learning techniques (described below). Linear Discriminant Analysis (LDA) can also be used to classify data based on categorical response variables. One implementation of LDA tries to find a linear combination of the predictors that gives maximum separation between the centers of the data while at the same time minimizing the variation within each group of data. This approach is implemented in many R packages, as in the <strong>lda()</strong> function of the MASS package, for example.</p>
<p>You might wonder how finding a linear combination of predictors differs from linear regression, logistic regression, or principal components analysis (PCA). LDA is popular when we have more than two response classes (polytomous responses), which logistic regression can only handle with some modification. In addition, when classes are well-separated, parameter estimates for logistic regression are surprisingly unstable, but LDA does not suffer from this problem. Finally, unlike linear regression, LDA chooses parameters to maximize distance between means of different categories.</p>
<div id="pca-and-lda-whats-the-difference" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> PCA and LDA what’s the difference?</h3>
<p>This approach of using a linear combination of predictors to predict similarity also seems similar to PCA, although of course PCA is an unsupervised learning technique (that is we don’t don’t have classification which we provide), while LDA is a supervised learning technique (it uses <em>a priori</em> class information to train the model). Both PCA and LDA provide the possibility of dimensionality reduction, which is very useful for visualization, and is often used to prepare data for machine learning techniques. However, we would expect (by definition) LDA to provide better data separation when compared to PCA.</p>
<p>Let’s try out this method by attempting to classify observations in the iris dataset to species using the 4 metrics of sepal and petal length and width.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="introduction-to-classification-and-machine-learning.html#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb108-2"><a href="introduction-to-classification-and-machine-learning.html#cb108-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">150</span>, <span class="dv">75</span>)</span>
<span id="cb108-3"><a href="introduction-to-classification-and-machine-learning.html#cb108-3" aria-hidden="true" tabindex="-1"></a>irislda <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species <span class="sc">~</span> ., iris, <span class="at">subset =</span> train)</span>
<span id="cb108-4"><a href="introduction-to-classification-and-machine-learning.html#cb108-4" aria-hidden="true" tabindex="-1"></a>irislda</span></code></pre></div>
<pre><code>Call:
lda(Species ~ ., data = iris, subset = train)

Prior probabilities of groups:
    setosa versicolor  virginica 
      0.36       0.36       0.28 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa         4.974074    3.381481     1.470370   0.2518519
versicolor     6.040741    2.833333     4.344444   1.3518519
virginica      6.666667    2.985714     5.604762   2.0190476

Coefficients of linear discriminants:
                    LD1        LD2
Sepal.Length  0.9707378  0.2536343
Sepal.Width   1.3142953  1.6569362
Petal.Length -2.8758066 -1.4064678
Petal.Width  -2.7477591  3.6067341

Proportion of trace:
   LD1    LD2 
0.9956 0.0044 </code></pre>
<p>Look at the lda object you have saved. A call to <strong>lda()</strong> returns the prior probability of each class, which is just based on the number of classes (the 3 difference species in this case), and the number of observations in each class. We start out very close to an even probability of an observation falling in any class (about 1/3). We also have the class-specific means for each covariate. So observations of species <em>I.setosa</em> have a mean Sepal.Length of 4.97.</p>
<p>Finally we have the fitted model, which consists of the the linear combination of predictor variables that are used to form the LDA decision rule. With 3 classes we have at most two linear discriminants, and similar to PCA we can see parameters that relate the various characteristics such as Sepal.Length to these axes.(e.g., LD1 = 0.97 Sepal.Length + 1.31 Sepal.Width - -2.88 Petal.Length - -2.75 Petal.Width. The trace shows us how much variance is captured by each axis. In this case, almost all of our class division can be done on the first axis.</p>
<p>There is also a prediction method implemented for lda objects. It returns the classification and the posterior probabilities of the new data. If you examine the prediction object, you will see it contains a matrix called posterior whose columns are the groups, rows are the individuals and values are the posterior probability that the corresponding observation belongs to each of the groups. Below, Let’s see how we do on the testing portion of the data (i.e., everything that is not the training data).</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="introduction-to-classification-and-machine-learning.html#cb110-1" aria-hidden="true" tabindex="-1"></a>plda <span class="ot">=</span> <span class="fu">predict</span>(irislda, <span class="at">newdata =</span> iris[<span class="sc">-</span>train, ])</span>
<span id="cb110-2"><a href="introduction-to-classification-and-machine-learning.html#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(iris<span class="sc">$</span>Species[<span class="sc">-</span>train], plda<span class="sc">$</span>class)</span></code></pre></div>
<pre><code>Confusion Matrix and Statistics

            Reference
Prediction   setosa versicolor virginica
  setosa         23          0         0
  versicolor      0         21         2
  virginica       0          1        28

Overall Statistics
                                          
               Accuracy : 0.96            
                 95% CI : (0.8875, 0.9917)
    No Information Rate : 0.4             
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.9395          
                                          
 Mcnemar&#39;s Test P-Value : NA              

Statistics by Class:

                     Class: setosa Class: versicolor Class: virginica
Sensitivity                 1.0000            0.9545           0.9333
Specificity                 1.0000            0.9623           0.9778
Pos Pred Value              1.0000            0.9130           0.9655
Neg Pred Value              1.0000            0.9808           0.9565
Prevalence                  0.3067            0.2933           0.4000
Detection Rate              0.3067            0.2800           0.3733
Detection Prevalence        0.3067            0.3067           0.3867
Balanced Accuracy           1.0000            0.9584           0.9556</code></pre>
<p>We can also visualize our prediction accuracy, using a regular plot() function.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="introduction-to-classification-and-machine-learning.html#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(plda<span class="sc">$</span>x, <span class="at">col =</span> plda<span class="sc">$</span>class, <span class="at">pch =</span> <span class="fu">as.numeric</span>(iris[<span class="sc">-</span>train,</span>
<span id="cb112-2"><a href="introduction-to-classification-and-machine-learning.html#cb112-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Species&quot;</span>]))</span>
<span id="cb112-3"><a href="introduction-to-classification-and-machine-learning.html#cb112-3" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="fu">levels</span>(iris[<span class="sc">-</span>train, <span class="st">&quot;Species&quot;</span>]), <span class="at">pch =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb112-4"><a href="introduction-to-classification-and-machine-learning.html#cb112-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:classlda"></span>
<img src="_main_files/figure-html/classlda-1.png" alt="Figure shows symbols for each species where the colour of the symbol indicates the classification produced by the LDA model" width="672" />
<p class="caption">
Figure 5.5: Classification of the iris dataset using LDA
</p>
</div>
<p>Again, we see some misclassifications (<a href="introduction-to-classification-and-machine-learning.html#fig:classlda">5.5</a>), where the colour of the symbol indicates whether it has been correctly identified, but this classifier works pretty well. The <strong>ldahist()</strong> function shows us why this is so, the centers of our synthetic variables are pretty well-separated between predicted groups on the first axis (<a href="introduction-to-classification-and-machine-learning.html#fig:ldahist">5.6</a>).</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="introduction-to-classification-and-machine-learning.html#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ldahist</span>(plda<span class="sc">$</span>x[, <span class="dv">1</span>], <span class="at">g =</span> plda<span class="sc">$</span>class, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:ldahist"></span>
<img src="_main_files/figure-html/ldahist-1.png" alt="Three histograms" width="672" />
<p class="caption">
Figure 5.6: Histogram of the synthetic axis cooredinates for each species
</p>
</div>
<p>The basic <strong>lda()</strong> function assumes that the predictors have linear relationships relationships with the classification. However, it is also possible to assume nonlinear relationships. We can see the difference this makes using another way to visualize the action of the classifier. We will plot out the decision boundary lines in the variable space. The <strong>drawparti()</strong> function in the klaR package can display the results of a linear or quadratic classifications 2 variables at a time.</p>
<p>#```{r ldabound, fig.cap=“Pairwise plots showing the classification regions based on Sepal.length and th other variables in the #iris dataset, for either linear or quadratic relationships. The large symbol indicates the mean values for a given species, #while the small symbols are individual observations,” fig.alt=“Plots have shaded regions that indicate the combination of #parameter values which correspond to classification as a particular species,” echo=TRUE, comment=““}
#library(klaR)
#pout=partimat(x=iris[,1:4], groupi=iris[,”Species”], Species ~ ., data = iris, method =”lda”, #main=““)
# # plot.control = list(gs=c(”x”,“y,”“z”)))</p>
<p>#par(mfrow = c(2,3))
#for(i in 2:4) drawparti(iris[,5], iris[,1], iris[,i], main=““,print.err=FALSE,method =”lda”, gs=c(15,17,1), col.mean=“black,” #col.correct=“blue,”col.wrong=“red,” xlab=colnames(iris)[1], ylab=colnames(iris)[i],
# pch.mean=c(15,17,16), cex.mean=2)
#legend(“bottomright,”pch=c(15,17,1), c(“setosa,” “versicolor,” “virginia”) )</p>
<p>#for(i in 2:4) drawparti(iris[,5], iris[,1], iris[,i], main=““,print.err=FALSE,method =”qda”, gs=c(15,17,1), col.mean=“black,” #col.correct=“blue,” col.wrong=“red,” xlab=colnames(iris)[1], ylab=colnames(iris)[i],
# pch.mean=c(15,17,16), cex.mean=2, cex=1.4)</p>
<p>#```
#We can see the curving boundaries in parameter space for the quadratic model in the bottom row (<a href="#fig:ldabound"><strong>??</strong></a>).</p>
<div id="unbalanced-classes" class="section level4" number="5.3.1.1">
<h4><span class="header-section-number">5.3.1.1</span> Unbalanced classes</h4>
<p>We should note that most if not all classification methods will perform poorly when the training data has a large number of observations in some classes and very few in others. There are several methods to try and cope with this problem, which we don’t have space to describe here. The simplest approach is, of course, to just subsample your data so that there are close to equal numbers of observations in each class (called <strong>undersampling</strong>). However, this method will waste some data, and you may not even have enough data for this method to be feasible. With <strong>oversampling</strong>, we randomly duplicate samples from the class with fewer instances, or we generate additional data, based on the data that we do have, so as to match the number of samples in each class. We avoid losing information with this approach, but we run the risk of overfitting our model, as some methods of oversampling will lead to having the same samples in both the training and test data. There are also hybrid methods that combine undersampling with the generation of additional data. Two of the most popular are ROSE (Lunardon et al. 2014) and SMOTE (Chawla et al. 2002). Both of which can be implemented direct in the caret package. You can read more about unbalanced data here (Fernández et al 2018: <a href="https://link.springer.com/content/pdf/10.1007/978-3-319-98074-4.pdf" class="uri">https://link.springer.com/content/pdf/10.1007/978-3-319-98074-4.pdf</a>)</p>
</div>
</div>
<div id="examples-of-discriminant-analysis-used-for-classification" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Examples of discriminant analysis used for classification</h3>
<p>Examples of using discriminant analysis (linear or nonlinear) in the literature include classification of images, sounds and stable isotope data.</p>
<p>Mahdianpari, M., Salehi, B., Mohammadimanesh, F., Brisco, B., Mahdavi, S., Amani, M., &amp; Granger, J. E. (2018). Fisher Linear Discriminant Analysis of coherency matrix for wetland classification using PolSAR imagery. Remote Sensing of Environment, 206, 300–317. <a href="https://doi.org/10.1016/j.rse.2017.11.005" class="uri">https://doi.org/10.1016/j.rse.2017.11.005</a></p>
<p>Bellisario, K. M., VanSchaik, J., Zhao, Z., Gasc, A., Omrani, H., &amp; Pijanowski, B. C. (2019). Contributions of MIR to Soundscape Ecology. Part 2: Spectral timbral analysis for discriminating soundscape components. Ecological Informatics, 51, 1–14. <a href="https://doi.org/10.1016/j.ecoinf.2019.01.008" class="uri">https://doi.org/10.1016/j.ecoinf.2019.01.008</a></p>
<p>Polito, M. J., Hinke, J. T., Hart, T., Santos, M., Houghton, L. A., &amp; Thorrold, S. R. (2017). Stable isotope analyses of feather amino acids identify penguin migration strategies at ocean basin scales. Biology Letters, 13(8), 20170241. <a href="https://doi.org/10.1098/rsbl.2017.0241" class="uri">https://doi.org/10.1098/rsbl.2017.0241</a></p>
</div>
</div>
<div id="tree-based-methods-for-classification" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Tree-based methods for classification</h2>
<div id="classification-and-regression-trees-carts" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Classification and regression trees (CARTs)</h3>
<p>Tree-based methods for classification involve dividing up regions defined by the predictor variables. For example, simple species identification trees use this approach. You are looking at a tree: does it have needle- or scale-shaped leaves? Or are the leaves wide? The leaf characteristic predictor is used to divide up the classification possibilities into conifers and deciduous trees (not without errors!). So we repeatedly split the response data into two groups that are as homogeneous as possible. The split is determined by the single predictor that best discriminates among the data. The binary splits continue to partition the data into smaller and smaller groups, or nodes, until the groups are no longer homogeneous. This effort produces a single tree where the binary splits form the branches and the final groups compose the terminal nodes, or leaves.</p>
<p>If this type of method is applied with a continuous response variable instead it is called a <em>regression tree</em>, if the response is categorical, it is called a <em>classification tree</em>. Often we refer to both techniques at the same time as <strong>C</strong>lassification <strong>A</strong>nd <strong>R</strong>egression <strong>T</strong>rees (CARTs). When used as a regression response predictor, this technique differs from standard regression approaches which are global models where the predictive formula is supposed to hold in the entire data space. Instead trees try to partition the data space into small enough parts where we can apply a simple different model on each part.</p>
<p>Let’s try a simple example on the iris data that is built-in to R. Take a look at the data set before your start the example. Then, we’ll first split the data into a training and testing set, and run our classification tree algorithm using the <strong>rpart</strong> package.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="introduction-to-classification-and-machine-learning.html#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb114-2"><a href="introduction-to-classification-and-machine-learning.html#cb114-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.7</span>  <span class="co"># percentage of training set</span></span>
<span id="cb114-3"><a href="introduction-to-classification-and-machine-learning.html#cb114-3" aria-hidden="true" tabindex="-1"></a>inTrain <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(iris), alpha <span class="sc">*</span> <span class="fu">nrow</span>(iris))</span>
<span id="cb114-4"><a href="introduction-to-classification-and-machine-learning.html#cb114-4" aria-hidden="true" tabindex="-1"></a>train.set <span class="ot">&lt;-</span> iris[inTrain, ]</span>
<span id="cb114-5"><a href="introduction-to-classification-and-machine-learning.html#cb114-5" aria-hidden="true" tabindex="-1"></a>test.set <span class="ot">&lt;-</span> iris[<span class="sc">-</span>inTrain, ]</span>
<span id="cb114-6"><a href="introduction-to-classification-and-machine-learning.html#cb114-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-7"><a href="introduction-to-classification-and-machine-learning.html#cb114-7" aria-hidden="true" tabindex="-1"></a>mytree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> train.set, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb114-8"><a href="introduction-to-classification-and-machine-learning.html#cb114-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb114-9"><a href="introduction-to-classification-and-machine-learning.html#cb114-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mytree)</span>
<span id="cb114-10"><a href="introduction-to-classification-and-machine-learning.html#cb114-10" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(mytree)</span></code></pre></div>
<div class="figure"><span id="fig:firstree"></span>
<img src="_main_files/figure-html/firstree-1.png" alt="This figure shows a classification tree using the iris dataset. The top of the plot has label 'Petal.Length&lt;2.45' and splits into two groups: 'setosa' and 'Petal.Width&lt;1.75'. The 'Petal.Width&lt;1.75' group splits into two smaller groups: 'versicolor' and 'virginica'." width="960" />
<p class="caption">
Figure 5.7: A simple plot of a tree classifier for the iris data
</p>
</div>
<p>We can see that petal length is used to distinguish species <em>I.setosa</em> from the other two species, and then petal width classifies into <em>I. versicolor</em> or <em>I. virginia</em>. The model of course includes more information than this regarding the number of observations aggregating to each branch of the tree etc. More detailed information can be obtained from <strong>summary(mytree)</strong>, or just typing mytree. A nicer plot, with more details can also be obtained with rpart.plot library.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="introduction-to-classification-and-machine-learning.html#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb115-2"><a href="introduction-to-classification-and-machine-learning.html#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(mytree)</span></code></pre></div>
<div class="figure"><span id="fig:fancytree"></span>
<img src="_main_files/figure-html/fancytree-1.png" alt="The figure shows a classification tree using the iris dataset. The top right of the figure shows a legend with labels 'setosa', 'versicolor' and 'virginica' which are denoted by an orange square, a gray square and a green square respectively. The top of the plot has a textbox with label 'setosa .36 .30 .33 100%'. Beneath the textbox is the label 'Petal.Length &lt; 2.5' which branches off into options 'yes' to the left of the label and 'no' to the right. Following the 'yes' option is an orange textbox with label 'setosa 1.00 .00 .00 36%'. Following the 'no' option is a light green textbox with label 'virginica .00 .48 .52 64%'. Beneath the textbox is the label 'Petal.Width &lt; 1.8' which splits off into a gray textbox with label 'versicolor .00 .89 .11 33%' and a green textbox with label 'virginica .00 .03 .97 30%'." width="672" />
<p class="caption">
Figure 5.8: A nicer plot of a tree classifier for the iris data made with the rpart.plot package
</p>
</div>
<p>This plot (Fig. <a href="introduction-to-classification-and-machine-learning.html#fig:fancytree">5.8</a>), in addition to the factor that splits each branch, also tells us the percentage of the data in each class, and the percentage that travels down each branch in each class. Starting at the top, each species makes up roughly a third of the data, after the petal length branch, travelling down the petal length greater than or equal to 2.5, all I.setosa observations all on the other side of the split, and we are left with data divided evenly between the <em>I.versicolor</em> and <em>I. viriginica</em>. The petal length &lt; 4.8 branch separates out these two species, with some error in classification.</p>
<p>The algorithm determines which variable to split based on <strong>impurity</strong>, or how similar points are within a group. If all data points are identical, then impurity is zero. Impurity increases as points become more dissimilar. Impurity is calculated differently for different kinds of trees. For classification trees: the Gini index, which reflects the proportion of responses in each level of a categorical variable is often used. The Gini index is calculated as: <span class="math inline">\(Gini=1-\sum p_i\)</span>, where <span class="math inline">\(p_i\)</span> is the proportion of observations in each class. The Gini index is small when many observations fall into a single category, so the split is made at the single variable which minimizes the Gini index. Some classifiers use the Shannon-Weiner index instead, which has similar properties.</p>
<p>Using this tree classifier, we can make predictions for our testing data, and get a confusion matrix</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="introduction-to-classification-and-machine-learning.html#cb116-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(mytree, test.set, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb116-2"><a href="introduction-to-classification-and-machine-learning.html#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(pred, test.set<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>Confusion Matrix and Statistics

            Reference
Prediction   setosa versicolor virginica
  setosa         17          0         0
  versicolor      0         12         1
  virginica       0          0        15

Overall Statistics
                                          
               Accuracy : 0.9778          
                 95% CI : (0.8823, 0.9994)
    No Information Rate : 0.3778          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.9664          
                                          
 Mcnemar&#39;s Test P-Value : NA              

Statistics by Class:

                     Class: setosa Class: versicolor Class: virginica
Sensitivity                 1.0000            1.0000           0.9375
Specificity                 1.0000            0.9697           1.0000
Pos Pred Value              1.0000            0.9231           1.0000
Neg Pred Value              1.0000            1.0000           0.9667
Prevalence                  0.3778            0.2667           0.3556
Detection Rate              0.3778            0.2667           0.3333
Detection Prevalence        0.3778            0.2889           0.3333
Balanced Accuracy           1.0000            0.9848           0.9688</code></pre>
<p>Our accuracy is pretty good for all species, and significantly greater than the no information rate.</p>
<div id="tree-pruning" class="section level4" number="5.4.1.1">
<h4><span class="header-section-number">5.4.1.1</span> Tree pruning</h4>
<p>So how does the model decide when to stop? Presumably you could continue to build out the tree until every single observation is a node. Another way to phrase this question is: how do you prevent the model from overfitting the data? The answer is: pruning (the best part about this classification method is the metaphorical gardening language!). Pruning is the act of overgrowing the tree and then cutting it back. Ultimately pruning should yield a tree that optimizes the trade-off between complexity and predictive ability.</p>
<p>Pruning begins by creating a nested series of trees of increasing number of branches, from 0 (no splits) to however many can be reasonably obtained from the data. For each number of branches, an optimal tree can be recovered, i.e., one that minimizes the overall misclassification rate.
To select the tree of optimal size we use cross-validation. For a given tree size, cross-validation divides the data into equal portions, removes one portion from the data, builds a tree using the remaining portion, and then calculates the error between the observed data and the predictions. This procedure is repeated for each of the remaining portions and then the overall error is summed across all subsets of the data. This is done for each of the nested trees. The tree of optimal size is then determined based on the smallest tree that is with in 1 standard error of the minimum error observed across all trees.</p>
<p>Even with pruning, a single CART is likely to overfit the data, particularly when there are many, many predictors, and thus is not very good for prediction. One way to get around this is to build a bunch of different, non-nested, trees on subsets of the data, and then average across them. Because any given tree is constructed with only a portion of the data, the likelihood of overfitting is drastically reduced. Moreover, averaging across many trees reduces the impact of anomalous results from a single tree. This is the idea of <strong>ensemble learning</strong>, or combining many ‘weak learners’ (individual trees) to produce one ‘strong learner’ (the ensemble).</p>
</div>
</div>
<div id="random-forests" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Random Forests</h3>
<p>One type of ensemble decision tree is a random forest. Random forests are a frequently used machine learning tool that can be used for both classification and regression. Random forests use a bootstrapped sample of the data, and only a portion of the predictors to construct each tree. This procedure ensures that each individual tree is independent from the others, making it a much more accurate method than some other ensemble learning techniques (e.g., bagging). As well, since both the data and the predictors are subsampled, these models can be fit to more predictors than there are observations. This seems a little counterintuitive, but can be a real benefit for ecological data which typically suffers from low replication.</p>
<p>Let’s try an ensemble decision tree on the iris data. We will use the <em>randomForest</em> package. Note that we do not have to split our data into training and testing sets now, the randomForest package is already doing this sort of thing for us.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="introduction-to-classification-and-machine-learning.html#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb118-2"><a href="introduction-to-classification-and-machine-learning.html#cb118-2" aria-hidden="true" tabindex="-1"></a>RF.model <span class="ot">=</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris)</span>
<span id="cb118-3"><a href="introduction-to-classification-and-machine-learning.html#cb118-3" aria-hidden="true" tabindex="-1"></a>RF.model</span></code></pre></div>
<pre><code>
Call:
 randomForest(formula = Species ~ ., data = iris) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06</code></pre>
<p>Our classification is pretty good with a misclassification rate on the <strong>O</strong>ut <strong>O</strong>f <strong>B</strong>ag data of only 4% (this is the equivalent to the test data from a cross-validation), and errors for individual species from 0 to 0.06%. We might like to look at what the model is doing, but unlike a single CART, random forests do not produce a single visual, since of course the predictions are averaged across many hundreds or thousands of trees.</p>
<p>When building random forests, there are three tuning parameters of interest: node size, number of trees, and number of predictors sampled at each split. Careful tuning of these parameters can prevent extended computations with little gain in error reduction. For example, the plot below (Fig. <a href="introduction-to-classification-and-machine-learning.html#fig:rfoob">5.9</a>) shows how the overall OOB error rate, and the error rate for each of the three species, changes with the size of the forest (the number of trees).</p>
<div class="figure"><span id="fig:rfoob"></span>
<img src="_main_files/figure-html/rfoob-1.png" alt="This figure shows a plot with y-axis labelled 'Error' and x-axis labelled 'trees'. The top right of the plot shows a legend with labels 'OOB', 'setosa', 'versicolor' and 'virginica' which are denoted by a black solid line, a red dashed line, a light green dotted line and a blue dotted and dashed line respectively. The 'OOB', 'versicolor' and 'virginica' variables show fluctuations in error at the beginning and middle of the plot but flatline towards the second half of the plot. The 'setosa' variable remains shows 0 error and is parallel with the x-axis." width="672" />
<p class="caption">
Figure 5.9: Out of bag error, and individual classification errors for the three species classes in the random forest model
</p>
</div>
<p>Obviously with fewer trees the error rate is higher, but as more trees are added you can see the error rate decrease and more or less flatten out. In the above plot (Fig <a href="introduction-to-classification-and-machine-learning.html#fig:rfoob">5.9</a>), we could easily reduce the number of trees down to 300 and experience relatively little loss in predictive ability. This is easy to do:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="introduction-to-classification-and-machine-learning.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">update</span>(RF.model, <span class="at">ntree =</span> <span class="dv">300</span>)</span></code></pre></div>
<pre><code>
Call:
 randomForest(formula = Species ~ ., data = iris, ntree = 300) 
               Type of random forest: classification
                     Number of trees: 300
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08</code></pre>
<p>So we see little change in our error rate.</p>
<p>Despite not yielding a single visualizable tree, we can get information about the random forest model. One metric is the relative importance of the predictors. By ranking predictors based on how much they influence the response, random forests may be a useful tool for selecting predictors before trying another framework, such as CART. Importance can be obtained using the importance function, and plotted using the <em>varImpPlotfunction()</em>:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="introduction-to-classification-and-machine-learning.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(RF.model)</span></code></pre></div>
<div class="figure"><span id="fig:vipp"></span>
<img src="_main_files/figure-html/vipp-1.png" alt="This figure shows a plot with title 'RF.model' that has y-axis with variables 'Petal.Width', 'Petal.Length', 'Sepal.Length' and 'Sepal.Width' from top to bottom and x-axis with label 'MeanDecreaseGini'. There are 4 points on the graph. Each point corresponds to one of the y-axis variables. The mean decrease in Gini Index is at the lowest for 'Sepal.Width' followed by 'Sepal.Length', then 'Petal.Length' and lastly is at the highest for 'Petal.Width'." width="672" />
<p class="caption">
Figure 5.10: Variable importance plot for our random forest model of the iris data
</p>
</div>
<p>Variable importance reports the mean decrease in the Gini Index for each predictor (Fig. <a href="introduction-to-classification-and-machine-learning.html#fig:vipp">5.10</a>). If you recall, the Gini index is a measure of impurity for categorical data. For each tree, each predictor in the OOB sample is randomly permuted (aka, shuffled around) and passed to the tree to obtain the error rate. The error rate from the unpermuted OOB is then subtracted from the error rate on the permuted OOB data, and averaged across all trees. When this value is large, it implies that a variable had a strong relationship with the response. That is, the model got much worse at predicting the data when that variable was permuted. As we already knew, Petal.Length and Petal.Width are the two most important variables.</p>
<p>One other useful aspect of random forests is getting a sense of the partial effect of each predictor given the other predictors in the model. This has analogues to partial correlation plots in linear models. We can construct a partial effects response by holding each value of the predictor of interest constant (while allowing all other predictors to vary at their original values), passing it through the random forest, and predicting the responses. The average of the predicted responses are plotted against each value of the predictor of interest (the ones that were held constant) to see how the effect of that predictor changes based on its value. This exercise can be repeated for all other predictors to gain a sense of their partial effects.</p>
<p>The function to calculate partial effects in the randomForest package is <em>partialPlot()</em>. Let’s look at the effect of Petal.Length:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="introduction-to-classification-and-machine-learning.html#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="fu">partialPlot</span>(RF.model, iris, <span class="st">&quot;Petal.Length&quot;</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;log odds&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:PEplot"></span>
<img src="_main_files/figure-html/PEplot-1.png" alt="This figure shows a plot with y-axis labelled 'log odds' and x-axis labelled 'Petal.Length'. The plot shows a line that begins where 'log odds' is highest and trends downwards as 'Petal.Length' increases." width="672" />
<p class="caption">
Figure 5.11: Partial effect of petal length in the random forest model of the iris data
</p>
</div>
<p>The y-axis is a bit tricky to interpret (Fig. <a href="introduction-to-classification-and-machine-learning.html#fig:PEplot">5.11</a>). Since we are dealing with classification trees, y on the logit scale, and is the probability of success. In this case, the partial plot has defaulted to the first class, which is <em>I. setosa</em>. The plot says that there is a high chance of successfully predicting this species from Petal.Length when Petal.Length is less than around 2.5 cm, after which point the chance of successful prediction drops off precipitously. This is actually quite reassuring as this is the first split identified way back in the very first CART (where the split was &lt; 2.45 cm).</p>
<p><strong>Missing data</strong></p>
<p>Its worth noting that the default behavior of randomForest is to refuse to fit trees with missing predictors. You can, however, specify a few alternative arguments: the first is na.action = na.omit, which removes the rows with missing values outright. Another option is to use na.action = na.roughfix, which replaces missing values with the median (for continuous variables) or the most frequent level (for categorical variables). Missing responses are harder: you can either remove that row, or use the function rfImpute to impute values. The imputed values are the average of the non-missing observations, weighted by their proximity to non-missing observations (based on how often they fall in terminal nodes with those observations). rfImpute tends to give optimistic estimates of the OOB error.</p>
<p><strong>Exercise 4</strong> See if you can construct a random forest that predicts diabetes in patients based on other indicators using the Pima.tr data as your training set and Pima.te as your test set. Report on the performance of your classifier.</p>
</div>
<div id="examples-of-tree-based-methods-used-for-classification" class="section level3" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> Examples of tree-based methods used for classification</h3>
<p>Bertsimas, D., Dunn, J., Steele, D. W., Trikalinos, T. A., &amp; Wang, Y. (2019). Comparison of Machine Learning Optimal Classification Trees With the Pediatric Emergency Care Applied Research Network Head Trauma Decision Rules. JAMA Pediatrics, 173(7), 648–656. <a href="https://doi.org/10.1001/jamapediatrics.2019.1068" class="uri">https://doi.org/10.1001/jamapediatrics.2019.1068</a></p>
<p>Ghiasi, M. M., &amp; Zendehboudi, S. (2021). Application of decision tree-based ensemble learning in the classification of breast cancer. Computers in Biology and Medicine, 128, 104089. <a href="https://doi.org/10.1016/j.compbiomed.2020.104089" class="uri">https://doi.org/10.1016/j.compbiomed.2020.104089</a></p>
<p>Kruk, C., Devercelli, M., Huszar, V. L. M., Hernández, E., Beamud, G., Diaz, M., Silva, L. H. S., &amp; Segura, A. M. (2017). Classification of Reynolds phytoplankton functional groups using individual traits and machine learning techniques. Freshwater Biology, 62(10), 1681–1692. <a href="https://doi.org/10.1111/fwb.12968" class="uri">https://doi.org/10.1111/fwb.12968</a></p>
</div>
<div id="r-functions-1" class="section level3" number="5.4.4">
<h3><span class="header-section-number">5.4.4</span> R functions</h3>
<p><strong>tree()</strong> (tree library) produces CARTs</p>
<p><strong>rpart()</strong> (rpart) CARTs</p>
<p><strong>randomforest()</strong> (randomforest) produces ensemble trees</p>
<p><strong>ranger()</strong> (ranger) is a faster implementation of the random forest algorithm</p>
</div>
</div>
<div id="artificial-neural-networks-ann" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> <strong>A</strong>rtificial <strong>N</strong>eural <strong>N</strong>etworks (ANN)</h2>
<p>Artificial neural networks (ANN) are another machine learning tool that can be used for both classification and regression. They can be viewed as analogous to human nervous system, in the sense that a neural network is made up of interconnected information processing units. Like tree-based methods, artificial neural networks learn by processing previously categorized examples (supervised learning).</p>
An artificial neural network is made up of artificial neurons. Each neuron in the network takes inputs, processes them, passes the processed information through a nonlinear function that converts the prediction to the desired type of output and finally gives the result (Fig. <a href="introduction-to-classification-and-machine-learning.html#fig:neuron">5.12</a>).
<div class="figure"><span id="fig:neuron"></span>
<img src="_main_files/figure-html/neuron-1.png" alt="Diagram showing the inputs, and internal components, including weighting, summation and activation function, that make up an aritifcial neuron" width="672" />
<p class="caption">
Figure 5.12: Diagram showing the inputs, and internal components, including weighting, summation and activation function, that make up an aritifcial neuron
</p>
</div>
<p>Some of the information will be more important for producing the correct output, the machine learning algorithm then weights this type of information more heavily. During the process of training, the ANN determines the error between the prediction and the target output. The network then adjusts its the weight it gives to each type of information. Successive adjustments are made using a learning rule and will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria (see below for more discussion about learning).</p>
<p>Once an ann model has been fitted, to predict classification of novel data it,
1. Weights the input
2. Multiplies all the inputs, x, by their weights, w
3. Adds all the multiplied values to get a weighted sum, and then
4. applies an <em>activation</em> function to the weighted input</p>
<p>The activation functions are used to map the input between the required values. For example, between 0 and 1 for probability predictions. There are many functions used such as sigmoid, tan hyperbolic, linear etc. I haven’t shown it here, but a bias can also be applied to the data which allows you to shift the activation function curve up or down.</p>
<p>Of course, like a human brain, there is more than on neuron in a network, so we have have a <em>layer</em> of neurons. In Fig <a href="introduction-to-classification-and-machine-learning.html#fig:ann">5.13</a> we have a <strong>feedforward neural network</strong> which consists of 2 inputs, 1 hidden layer with 3 hidden neurons, and 1 final output. We could have decided to add another intermediate “hidden” layer between the input and the final output exactly in the same way. Then we would have had a neural network with 2 hidden layers. Or we could have chosen to stay with 1 hidden layer but to have more neurons in it (5 instead of 3, for example).The parallel processing of information by many neurons allows ANNs to deal with non-linearity easily. Not all the neural networks fit the template described above. There are different kinds of architectures, but the feedforward neural network (also called Multi Layer Perceptron) is really the first basic architecture to understand.</p>
<div class="figure"><span id="fig:ann"></span>
<img src="_main_files/figure-html/ann-1.png" alt="Artificial neural network" width="672" />
<p class="caption">
Figure 5.13: Diagram of an artificial neural network
</p>
</div>
<div id="ann-learning" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> ANN learning</h3>
<p>Once defined, the model still need to be fitted (i.e., the weights should be adjusted, based on the data, to minimize some error function, just as in the case of the linear regression) and that is a really difficult optimization task to complete. There are many learning rules that are used with neural networks such as: least mean square, gradient descent, newton’s rule, conjugate gradient and others. So during the model fitting process the algorithm sets the initial weights of the network randomly (given some well chosen distribution), and then tries to adjust the weights (and other model parameters such as bias and intercepts) to minimize error.</p>
<p>A common optimization method is <strong>backward error propagation using gradient descent</strong>. In this method, the ANN applies a gradient descent over all these parameters to iteratively improve (reduce) the error metric. As its name suggests, gradient descent involves calculating the gradient of the target function, which is a vector of partial derivatives with respect to input variables. You may recall from calculus that the first-order derivative of a function gives the slope of a function at a given point. The gradient descent algorithm uses this calculation to select new weights for each input variable that result in a lower error. The step size, sometimes called the learning rate, is used to control how much to change each input variable with respect to the gradient. This process is repeated until the minimum of the target function is located, a maximum number of candidate solutions are evaluated, or some other stop condition.</p>
<p>The back propagation algorithm consists in using the layered structure of the ANN to make the computation of derivatives for gradient descent more efficient. The backpropagation algorithm was originally introduced in the 1970s, but its importance wasn’t fully appreciated until Rumelhart et al. (1986) described how backpropagation works far faster than earlier approaches to learning, making it possible to use neural nets to solve problems which had previously been insoluble. Backpropagation will give us an expression for the partial derivative of the error with respect to any weight w (or bias b) in the network. The expression tells us how quickly the error changes when we change the weights and biases.</p>
<p>I’m not going to give the mathematical details here, but briefly, to implement this method, the algorithm first completes a <strong>forward pass</strong> through the network, which is just a fancy way of saying the model makes a prediction. The error between the prediction and the expected value is calculated. Then, going backwards, through the network, the algorithm finds the partial derivative of the error with respect to the weights from each neuron in the hidden layer. It changes the weights a little bit in a way that makes the prediction slightly closer to the true value (through a gradient descent approach). It then repeat this process as long as it can make “little” changes to the weights that improve the result. To read more about neural net learning, and backwards propagation in particular, take a look at <a href="http://neuralnetworksanddeeplearning.com/" class="uri">http://neuralnetworksanddeeplearning.com/</a>.</p>
</div>
<div id="our-first-ann" class="section level3" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Our first ANN</h3>
<p>We’ll get started trying to use this machine learning technique using our old friend the iris data and the package neuralnets. We divide the data into training and test sets, and then run the neural net algorithm from the neuralnet package. Let’s use two inputs, three neurons and standard backward error propagation. We’ll also need to set the learning rate to some small value. Our output will be the classification of one of the three species.</p>
<p>When you are done, take a look at the output produced by the algorithm, and plot the ANN using the plot function. What is the meaning of the numbers on the diagram?</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="introduction-to-classification-and-machine-learning.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sample the data to divide into training and test</span></span>
<span id="cb124-2"><a href="introduction-to-classification-and-machine-learning.html#cb124-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">nrow</span>(iris), <span class="at">size =</span> <span class="fu">nrow</span>(iris) <span class="sc">*</span> <span class="fl">0.5</span>)</span>
<span id="cb124-3"><a href="introduction-to-classification-and-machine-learning.html#cb124-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-4"><a href="introduction-to-classification-and-machine-learning.html#cb124-4" aria-hidden="true" tabindex="-1"></a>iristrain <span class="ot">&lt;-</span> iris[train, ]</span>
<span id="cb124-5"><a href="introduction-to-classification-and-machine-learning.html#cb124-5" aria-hidden="true" tabindex="-1"></a>iristest <span class="ot">&lt;-</span> iris[<span class="sc">-</span>train, ]</span>
<span id="cb124-6"><a href="introduction-to-classification-and-machine-learning.html#cb124-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-7"><a href="introduction-to-classification-and-machine-learning.html#cb124-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the multiclass neural net to produce a classifier</span></span>
<span id="cb124-8"><a href="introduction-to-classification-and-machine-learning.html#cb124-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(neuralnet)</span>
<span id="cb124-9"><a href="introduction-to-classification-and-machine-learning.html#cb124-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-10"><a href="introduction-to-classification-and-machine-learning.html#cb124-10" aria-hidden="true" tabindex="-1"></a>nn <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(Species <span class="sc">~</span> Petal.Width <span class="sc">+</span> Petal.Length, iristrain,</span>
<span id="cb124-11"><a href="introduction-to-classification-and-machine-learning.html#cb124-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">hidden =</span> <span class="dv">3</span>, <span class="at">linear.output =</span> <span class="cn">FALSE</span>, <span class="at">algorith =</span> <span class="st">&quot;backprop&quot;</span>,</span>
<span id="cb124-12"><a href="introduction-to-classification-and-machine-learning.html#cb124-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">learningrate =</span> <span class="fl">0.01</span>)</span>
<span id="cb124-13"><a href="introduction-to-classification-and-machine-learning.html#cb124-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-14"><a href="introduction-to-classification-and-machine-learning.html#cb124-14" aria-hidden="true" tabindex="-1"></a><span class="co"># rounding the weight values so the plot is tidier</span></span>
<span id="cb124-15"><a href="introduction-to-classification-and-machine-learning.html#cb124-15" aria-hidden="true" tabindex="-1"></a>nn<span class="sc">$</span>weights[[<span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fu">lapply</span>(nn<span class="sc">$</span>weights[[<span class="dv">1</span>]], <span class="cf">function</span>(x) <span class="fu">round</span>(x,</span>
<span id="cb124-16"><a href="introduction-to-classification-and-machine-learning.html#cb124-16" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>))</span>
<span id="cb124-17"><a href="introduction-to-classification-and-machine-learning.html#cb124-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-18"><a href="introduction-to-classification-and-machine-learning.html#cb124-18" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting the neural net (but not the intercepts or error</span></span>
<span id="cb124-19"><a href="introduction-to-classification-and-machine-learning.html#cb124-19" aria-hidden="true" tabindex="-1"></a><span class="co"># rates)</span></span>
<span id="cb124-20"><a href="introduction-to-classification-and-machine-learning.html#cb124-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nn, <span class="at">rep =</span> <span class="st">&quot;best&quot;</span>, <span class="at">information =</span> <span class="cn">FALSE</span>, <span class="at">intercept =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
Finally, we’ll generate a confusion matrix for our neural net to see how well it did. The <strong>compute()</strong> function gives us the probability of class membership, and we’ll just select the species with the highest probability.
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
setosa
</th>
<th style="text-align:right;">
versicolor
</th>
<th style="text-align:right;">
virginica
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
setosa
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
versicolor
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
virginica
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
23
</td>
</tr>
</tbody>
</table>
<p>Overall, not too bad. Pretty comparable to the other classifiers we have tried out.</p>
</div>
<div id="examples-of-anns-used-for-classification" class="section level3" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> Examples of ANNs used for classification</h3>
<p>Bewes, J., Low, A., Morphett, A., Pate, F. D., &amp; Henneberg, M. (2019). Artificial intelligence for sex determination of skeletal remains: Application of a deep learning artificial neural network to human skulls. Journal of Forensic and Legal Medicine, 62, 40–43. <a href="https://doi.org/10.1016/j.jflm.2019.01.004" class="uri">https://doi.org/10.1016/j.jflm.2019.01.004</a></p>
<p>Krtolica, I., Cvijanović, D., Obradović, Đ., Novković, M., Milošević, D., Savić, D., Vojinović-Miloradov, M., &amp; Radulović, S. (2021). Water quality and macrophytes in the Danube River: Artificial neural network modelling. Ecological Indicators, 121, 107076. <a href="https://doi.org/10.1016/j.ecolind.2020.107076" class="uri">https://doi.org/10.1016/j.ecolind.2020.107076</a></p>
</div>
<div id="r-functions-2" class="section level3" number="5.5.4">
<h3><span class="header-section-number">5.5.4</span> R functions</h3>
<p><strong>nnet()</strong> (nnet package) is good for a simple neural network with just a single hidden layer</p>
<p><strong>neuralnet()</strong> (neuralnet) has a faster version of backwards propagation</p>
<p>Packages for deep learning applications that use complex neural networks include
MXNet, darch, deepnet, and h2o</p>
<p><strong>plotnet()</strong> (NeuralNetTools) implements tools for visualization and understanding as described by Olden &amp; Jackson (2002)</p>
</div>
<div id="what-else" class="section level3" number="5.5.5">
<h3><span class="header-section-number">5.5.5</span> What else?</h3>
<p>We’ve just provided a small sampler of classification methods here that will get you started. But other methods k-nearest neighbors, and <strong>S</strong>upport <strong>V</strong>ector <strong>M</strong>achines (SVM) are also useful for building classification models.</p>
</div>
<div id="feedback-4" class="section level3" number="5.5.6">
<h3><span class="header-section-number">5.5.6</span> Feedback</h3>
<p>We value your input! Please take the time to let us know how we might improve these materials. <a href="https://forms.gle/vLkcZq7NxPb2ajTQ6">Survey</a></p>
</div>
</div>
<div id="references-1" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> References</h2>
<p>Chawla, N. V., Bowyer, K. W., Hall, L. O., &amp; Kegelmeyer, W. P. (2002). SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research, 16, 321–357. <a href="https://doi.org/10.1613/jair.953" class="uri">https://doi.org/10.1613/jair.953</a></p>
<p>Fernández, A., García, S., Galar, M., Prati, R. C., Krawczyk, B., &amp; Herrera, F. (2018). Learning from imbalanced data sets (Vol. 10). Springer. <a href="https://link.springer.com/content/pdf/10.1007/978-3-319-98074-4.pdf" class="uri">https://link.springer.com/content/pdf/10.1007/978-3-319-98074-4.pdf</a></p>
<p>Lunardon, N., Menardi, G., &amp; Torelli, N. (2014). ROSE: A Package for Binary Imbalanced Learning. The R Journal, 6(1), 79. <a href="https://doi.org/10.32614/RJ-2014-008" class="uri">https://doi.org/10.32614/RJ-2014-008</a></p>
<p>Olden, J. D., &amp; Jackson, D. A. (2002). Illuminating the “black box”: A randomization approach for understanding variable contributions in artificial neural networks. Ecological Modelling, 154(1), 135–150. <a href="https://doi.org/10.1016/S0304-3800(02)00064-9" class="uri">https://doi.org/10.1016/S0304-3800(02)00064-9</a></p>
<p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533–536. <a href="https://doi.org/10.1038/323533a0" class="uri">https://doi.org/10.1038/323533a0</a></p>
</div>
<div id="answer-key-1" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Answer Key</h2>
<p><strong>Exercise 1.</strong> Once you have fitted your model, you can use the code</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="introduction-to-classification-and-machine-learning.html#cb125-1" aria-hidden="true" tabindex="-1"></a>p18 <span class="ot">=</span> <span class="fu">round</span>((<span class="fu">exp</span>(logcoefs[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> logcoefs[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">*</span> <span class="dv">18</span>))<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span></span>
<span id="cb125-2"><a href="introduction-to-classification-and-machine-learning.html#cb125-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">exp</span>(logcoefs[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> logcoefs[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">*</span> <span class="dv">18</span>)), <span class="dv">2</span>)</span></code></pre></div>
<p>to get the probability of occupancy at locations with a mean annual temperature of 18C of approximately 0.91. Your value will vary slightly depending on the stochasticity from the random routines used to generate our simulated observations.</p>
<p><strong>Exercise 2.</strong> We first need to change the threshold for our classification, and then recalculate sensitivity and specificity for our results</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="introduction-to-classification-and-machine-learning.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># determine model predictions based on a new threshold</span></span>
<span id="cb126-2"><a href="introduction-to-classification-and-machine-learning.html#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="co"># value</span></span>
<span id="cb126-3"><a href="introduction-to-classification-and-machine-learning.html#cb126-3" aria-hidden="true" tabindex="-1"></a>logoccnew <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(glm.probs <span class="sc">&gt;</span> <span class="fl">0.45</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb126-4"><a href="introduction-to-classification-and-machine-learning.html#cb126-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-5"><a href="introduction-to-classification-and-machine-learning.html#cb126-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-6"><a href="introduction-to-classification-and-machine-learning.html#cb126-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a new confusion matrix</span></span>
<span id="cb126-7"><a href="introduction-to-classification-and-machine-learning.html#cb126-7" aria-hidden="true" tabindex="-1"></a>ctabnew <span class="ot">=</span> <span class="fu">table</span>(logoccnew, y)</span>
<span id="cb126-8"><a href="introduction-to-classification-and-machine-learning.html#cb126-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dimnames</span>(ctabnew) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">Actual =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">Predicted =</span> <span class="fu">c</span>(<span class="dv">0</span>,</span>
<span id="cb126-9"><a href="introduction-to-classification-and-machine-learning.html#cb126-9" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>))</span>
<span id="cb126-10"><a href="introduction-to-classification-and-machine-learning.html#cb126-10" aria-hidden="true" tabindex="-1"></a>ctabnew</span></code></pre></div>
<pre><code>      Predicted
Actual   0   1
     0 124  28
     1  21  27</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="introduction-to-classification-and-machine-learning.html#cb128-1" aria-hidden="true" tabindex="-1"></a>errN <span class="ot">=</span> (ctabnew[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">+</span> ctabnew[<span class="dv">2</span>, <span class="dv">1</span>])<span class="sc">/</span><span class="fu">sum</span>(ctabnew)</span>
<span id="cb128-2"><a href="introduction-to-classification-and-machine-learning.html#cb128-2" aria-hidden="true" tabindex="-1"></a>sensN <span class="ot">=</span> ctabnew[<span class="dv">2</span>, <span class="dv">2</span>]<span class="sc">/</span>(ctabnew[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">+</span> ctabnew[<span class="dv">2</span>, <span class="dv">2</span>])</span>
<span id="cb128-3"><a href="introduction-to-classification-and-machine-learning.html#cb128-3" aria-hidden="true" tabindex="-1"></a>specN <span class="ot">=</span> <span class="dv">1</span> <span class="sc">-</span> ctabnew[<span class="dv">1</span>, <span class="dv">2</span>]<span class="sc">/</span>(ctabnew[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> ctabnew[<span class="dv">1</span>, <span class="dv">2</span>])</span></code></pre></div>
<p>Again, there will be a bit of variation in your exact result. We obtain a sensitivity of 0.56 and specificity of 0.82</p>
<p><strong>Exercise 3.</strong> To construct a logistic regression to determine if a patient has diabetes or not, first read in the the training dataset Pima.tr from the MASS package</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="introduction-to-classification-and-machine-learning.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb129-2"><a href="introduction-to-classification-and-machine-learning.html#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Pima.tr)</span>
<span id="cb129-3"><a href="introduction-to-classification-and-machine-learning.html#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(Pima.tr)</span></code></pre></div>
<pre><code>&#39;data.frame&#39;:   200 obs. of  8 variables:
 $ npreg: int  5 7 5 0 0 5 3 1 3 2 ...
 $ glu  : int  86 195 77 165 107 97 83 193 142 128 ...
 $ bp   : int  68 70 82 76 60 76 58 50 80 78 ...
 $ skin : int  28 33 41 43 25 27 31 16 15 37 ...
 $ bmi  : num  30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ...
 $ ped  : num  0.364 0.163 0.156 0.259 0.133 ...
 $ age  : int  24 55 35 26 23 52 25 24 63 31 ...
 $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ...</code></pre>
<p>Next construct a logistic regression on the training data</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="introduction-to-classification-and-machine-learning.html#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run logistic regression</span></span>
<span id="cb131-2"><a href="introduction-to-classification-and-machine-learning.html#cb131-2" aria-hidden="true" tabindex="-1"></a>Pima.log <span class="ot">&lt;-</span> <span class="fu">glm</span>(type <span class="sc">~</span> ., <span class="at">family =</span> binomial, <span class="at">data =</span> Pima.tr)</span></code></pre></div>
<p>Then get the predictions of the model for the testing data. We’ve just used a threshold of 50%.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="introduction-to-classification-and-machine-learning.html#cb132-1" aria-hidden="true" tabindex="-1"></a>testPima <span class="ot">&lt;-</span> <span class="fu">predict</span>(Pima.log, <span class="at">newdata =</span> Pima.te, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb132-2"><a href="introduction-to-classification-and-machine-learning.html#cb132-2" aria-hidden="true" tabindex="-1"></a>testPima <span class="ot">=</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(testPima <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))</span></code></pre></div>
<p>Finally create a confusion matrix</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="introduction-to-classification-and-machine-learning.html#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a confusion matrix</span></span>
<span id="cb133-2"><a href="introduction-to-classification-and-machine-learning.html#cb133-2" aria-hidden="true" tabindex="-1"></a>atab <span class="ot">=</span> <span class="fu">table</span>(Pima.te<span class="sc">$</span>type, testPima)</span>
<span id="cb133-3"><a href="introduction-to-classification-and-machine-learning.html#cb133-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dimnames</span>(atab) <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">Actual =</span> <span class="fu">levels</span>(Pima.te<span class="sc">$</span>type), <span class="at">Predicted =</span> <span class="fu">levels</span>(Pima.te<span class="sc">$</span>type))</span>
<span id="cb133-4"><a href="introduction-to-classification-and-machine-learning.html#cb133-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-5"><a href="introduction-to-classification-and-machine-learning.html#cb133-5" aria-hidden="true" tabindex="-1"></a>atab</span></code></pre></div>
<pre><code>      Predicted
Actual  No Yes
   No  200  23
   Yes  43  66</code></pre>
<p>You should notice there there have been some misclassifications at 50%, and that the accuracy is only about 0.8. See if another decision boundary (e.g., 75%) does any better, or use the <em>ROCit</em> library, or other R packages to calculate an optimal threshold or an overall performance across thresholds.</p>
<p><strong>Exercise 4.</strong></p>
<p>We’ve used the caret package to run our random forest to classify the Pima diabetes data using a kfold cross-validation procedure, given that the dataset is fairly small. We’ve even divided the data into training and testing data (i.e., this is a bit overkill, considering that random forest will calculate out of bag error all by itself).</p>
<p>We use the fitted model to predict the test data, and then generate a confusion matrix to take a look at how well it does. We also examine which predictors are most important.</p>
<pre><code>Random Forest 

200 samples
  7 predictor
  2 classes: &#39;No&#39;, &#39;Yes&#39; 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 160, 160, 159, 161, 160 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
  2     0.7249250  0.3588639
  4     0.7300469  0.3665523
  7     0.7299187  0.3620973

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 4.</code></pre>
<pre><code>
Call:
 randomForest(x = x, y = y, mtry = min(param$mtry, ncol(x))) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 4

        OOB estimate of  error rate: 28%
Confusion matrix:
     No Yes class.error
No  106  26   0.1969697
Yes  30  38   0.4411765</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  186  46
       Yes  37  63
                                          
               Accuracy : 0.75            
                 95% CI : (0.6998, 0.7957)
    No Information Rate : 0.6717          
    P-Value [Acc &gt; NIR] : 0.001172        
                                          
                  Kappa : 0.4209          
                                          
 Mcnemar&#39;s Test P-Value : 0.379882        
                                          
            Sensitivity : 0.8341          
            Specificity : 0.5780          
         Pos Pred Value : 0.8017          
         Neg Pred Value : 0.6300          
             Prevalence : 0.6717          
         Detection Rate : 0.5602          
   Detection Prevalence : 0.6988          
      Balanced Accuracy : 0.7060          
                                          
       &#39;Positive&#39; Class : No              
                                          </code></pre>
<div class="figure"><span id="fig:e4"></span>
<img src="_main_files/figure-html/e4-1.png" alt="Variable importance is indicated by the mean decrease in the Gini index" width="672" />
<p class="caption">
Figure 5.14: Variable importance plot from the final random forest model selected by caret for the Pima.tr dataset
</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-multivariate-analysis-clustering-and-ordination.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optimization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
