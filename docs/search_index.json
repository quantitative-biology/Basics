[["index.html", "Further Git and GitHub 1 Prerequisites", " Further Git and GitHub Andrew Edwards 05/07/2021 1 Prerequisites The book presupposes the user is familiar with R. All chapters are stored as .Rmd files on github. The whole book can be compiled to various formats using the bookdown package which can be installed from CRAN or Github. install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["introduction-to-git-and-github.html", "2 Introduction to Git and GitHub 2.1 Motivation 2.2 Getting set up for the first time 2.3 Using Git and GitHub 2.4 Beyond the basics", " 2 Introduction to Git and GitHub 2.1 Motivation As a biology graduate student or a professional biologist in a university or government setting, why might you want to use Git and GitHub? And what are Git and GitHub anyway? Scientists (including students) are working far more collaboratively than in the past This involves both sharing code and writing up results There is a push towards open science  including your code as part of a scientific paper We have called this a TTT approach: Transparent  a clear and open way to show data, code, and results, enabling reproducibility Traceable  a clear link from database queries and code to final results (numbers, tables, and graphs in a document) Transferable  it should be feasible for another person to reproduce work and build upon it with a minimal learning curve Using Git and GitHub in your workflow greatly enables this, both when working alone and in a team. Git keeps track of the latest versions of your files, such as computer code or write up for results as you work on them. It also allows you to go back to any previous version of your files (this is version control). (It also does much more). GitHub is a website that hosts a repository of your work code and enables users to easily collaborate. Your repositories can be either public or private. We use Git and GitHub extensively: to collaborate on writing code and producing documents (such as this entire document!). to easily share code publically for scientific papers, and update it if necessary. when working alone to retain a methodical workflow. Example application  Pacific Hake stock assessment Under a formal Agreement between the Canadian and US governments, a team of four of us (two from each country) conduct an annual stock assessment for Pacific Hake (Merluccius productus) off the west coast of Canada and the US. The assessment is used to manage the stock, which is of important ecological and economic value ($100 million export value in Canada). We fit complex population models to data to make projections about future health of the stock under different levels of catch. There is a very short turnaround (five weeks) between getting the final data, doing the analyses (model runs can take many hours of computer time) and submitting the assessment document, which is typically &gt;200 pages and contains numerous figures and tables available here. Prior to 2016, the document was assembled in Word, requiring lots of editing and amaglamating of files, often late at night. Now we share our code via GitHub, automate a lot of the document production using knitr (similar to Rmarkdown, covered in Module 2). So with four people constantly working on the same large document, we need to ensure we are keeping up-to-date with each other, can all produce the latest version, and have identical folder structures on each others computers. The alternative of emailing files back and forth is: very inefficient, prone to errors, just painful. What we can avoid Using GitHub it is easy to see what text/code collaborators have changed, avoiding things like the following, for which it hard to see where to get started: We may want to keep old versions of files (and email them back and forth), but without GitHub we can end up with a veritable gong show: We can avoid having to co-ordinate having only one person working on the latest version of a document, so we dont get things like: Can avoid multiple versions of a file that then have to be carefully merged: While GoogleDocs, for example, is fine for collaborating on a short document, it isnt suitable for sharing complex code, or complex documents that are somewhat automatically updated. GitHub advantages Say youve off on a two-week hike while your collaborators have been diligently working away and they have edited 15 new files of code in five folders, added four data sets, and created five new pages of text towards a manuscript. You can easily catch up with them (get all their changes onto your computer) with a few simple commands. You dont even have to pester them to ask what theyve done, as you can check it yourself. So rather than this conversation: You: Hey, Im back from my awesome trip and saw some bears. What have you been doing with the project? Likely reply: Glad you had fun. Im busy on something else right now. Er, where were we at when you left? You can have this one: You: Hey, Im back from my awesome trip and saw some bears. I went through your commits on GitHub and everything looks great. Shall I get on those Methods Issues you assigned me? Likely reply: Glad you had fun, looking forward to hearing about it. Im busy on something else right now so, yes, resolving those issues will be great, thanks. And the project keeps moving in an efficient way. By having code shared publically, it is easy to answer questions, such as this one I received: Rather than go searching on my laptop for the code that I hadnt looked at for six months, I could click on the link the questioner sent and answer very quickly, with a simple link to the file I am referring to (there is no ambiguity): You can even ask who last edited a particular line of code/text (GitHub amusingly calls it Blame): You can properly keep track of (and discuss) Issues to be thought about or fixed, rather than having things in emails that get forgotten: Important: You still have all your work locally on your computer. So if your internet access goes down or GitHub is unavailable (which of course will only happen when you have a deadline) you can still carry on with your work. Why this course? Delving into the Git and GitHub world online it can feel like you need a computer science degree to get started. This may not be surprising as Git was writting by the guy who wrote the operating system Linux, to help people collaborate on writing the operating system Linux. But it means that, for example, the second paragraph of the Wikipedia Git page says: As with most other distributed version control systems, and unlike most clientserver systems, every Git directory on every computer is a full-fledged repository with complete history and full version-tracking abilities, independent of network access or a central server. Say what??? That is fairly incomprehensible to those without strong computer science backgrounds. The aim of this module is to introduce biologists to the world of Git and GitHub, while avoiding a lot of the technical details. However, once you have mastered the basics then it should be easier to delve deeper. Our target audience is: graduate level biology students biology faculty government scientists scientists in non-governmental organisations in fact anyone wanting to learn these tools This work is extended from lectures and exercises developed by Chris Grandin and myself as part of a Fisheries and Oceans Canada workshop. (Luckily Chris does have a computer science degree, and so was able to get some of us going with Git and GitHub several years ago). These tools are now widely used within our organisation. Computer language For sharing code, it doesnt matter what language your code is in (R, Matlab, Python, C, ), as we will just be sharing text files. There is a learning curve, but once you get going you only really need a few main commands. Unfortunately the hardest bit is actually getting everything set up. 2.2 Getting set up for the first time Before you start using Git you need to set up your computer to use it, and install a few other programs that are useful. This is a one-time setup and once it is done, you will be able to easily create new projects or join others in collaboration. We have tested the installations as much as feasible. If you have an issue then search the internet, as it may be due to some configuration on your particular computer. This module is for any operating system: Windows, MacOS, Linux or Unix. 2.2.1 What you will end up having installed These are programs/things you will need (instructions are on the next slides). Obviously skip any that you already have working. A GitHub account A text editor that isnt Notepad Git on your computer Diffmerge or something similar for comparing changes to files (not completely necessary) Markdown Pad 2 or Chrome extension or something similar for viewing Markdown files (not completely necessary) 2.2.2 Get a GitHub account Sign up for GitHub: http://github.com If possible, choose a user name that will make sense to colleagues, e.g. andrew-edwards or cgrandin, not pink-unicorn. Desirable: attach a photo (headshot) to your profile. This makes it easy for collaborators to identify you. 2.2.3 Text Editor You must have a text editor that is aware of outside changes in a file. This is necessary because if you have a file open in the editor and you download an updated version of the file, you want the editor to ask you if you want to use the updated version. We know that Emacs, Xemacs and maybe Vim are okay, as is RStudiofor.R (and other) files. Notepad is not okay. But you can download and install Notepad++ which is fine: https://notepad-plus-plus.org/download/v7.3.3.html 2.2.4 Install the Git application on your machine See https://git-scm.com/downloads for downloading instructions for Windows, MAC and Linux/Unix It seems best to accept the default options, except NOT Notepad or Vim (unless you use Vim) as the default editor. 2.2.5 Git shell For this course we will use a simple git shell to type commands (rather than a point-and-click Graphical User Interface). This is for several reasons: Commands are the same across operating systems. It is easier to demonstrate (and remember) a few simple commands, rather than follow a cursor moving across a screen. Learning the text commands will give you a good understanding of how Git and GitHub work. It is easier to Google for help when you get stuck or want to learn about more advanced options. Commands are quick, and you can usually the up arrow (or ctrl-up-arrow) to retrieve recent commands, or auto-complete commands using . 2.2.6 Git shell, RStudio There are many Graphical User Interfaces that are available, as described at https://git-scm.com/downloads/guis. Many (but not all) biologists use R in RStudio for their analyses. There is Git functionality built in to RStudio that we (TODO: SOMEONE?) will demonstrate later. I use magit which works in the text editor emacs (which for years I have used for pretty much everything, such as editing files, running R, Matlab, etc.). But I would not have been able to learn magit without first knowing the Git commands from using the shell. For now we will stick with the Git shell for the aforementioned reasons. It will also give you a better understanding of Git and GitHub, and emphasise that you can use Git for any files, not just R code. 2.2.7 Powershell and posh-git Download a Powershell (a shell window in which you can type commands, presumably the power part means its more powerful than a basic version) and then posh-git following the instructions at https://github.com/dahlbyk/posh-git Do the Installation and Using posh-git sections. If you dont understand some options (I dont!) just pick the simplest, usually the first. The next slides are from our course about three years ago (and were for Windows). So they may be out of date (though first one is recent tips from a colleague). [Maybe we should see https://upg-dh.newtfire.org/explainGitShell.html] 2.2.8 One-time authentication The first time you get set up or start using Git, there will be some one-timeauthentication to connect to your GitHub account. Follow any instructions. 2.2.9 Configure the Git application Windows Create a github directory, such as C:. It is fine to put it in a differentpath, but make sure there are no spaces or special charactersanywherein the fullpath. This is where you want to be saving your work that you are tracking with Git. TODO: Andy has to reinstall anyway and will write something here. Think its just following instructions. MAC Create the directory ~/github Enjoy a beverage TODO: Check with Luwen if it is that simple 2.2.10 Install the difftool The difftool will be used to examine differences between different versions of files and also to simplify merging of branches and collaborators code. There are many programs that can be used but for consistency we will use Diffmerge. It is nice to have but not essential if you have trouble installing it. Install Diffmerge: https://sourcegear.com/diffmerge/downloads.php The configuration for directing git to use Diffmerge will be done below. 2.2.11 Cloning the git-course repository On the GitHub webpage, sign into your account and navigate to: https://github.com/quantitative-biology/module-1-git Windows: Open the Git shell and run the following command to clone the repository (clone means copy all files in the repository to your computer): git clone https://github.com/quantitative-biology/module-1-git MAC: Open terminal and change to the GitHub directory: cd ~/github then run the clone command: git clone https://github.com/quantitative-biology/module-1-git You now have the files for the GitHub course on your computer 2.2.12 Copy the .gitignore file Git uses a configuration file for your account info, name to use when committing, aliases for commands, and other things. Open up the misc sub-directory in the git-module-1 directory and copy the file .gitconfig. For Windows, copy this file (overwrite the existing file) to: C:\\Users\\YOUR-COMPUTER-USER-NAME\\.gitconfig, where YOUR-COMPUTER-USER-NAME is your username on your computer, not your GitHub account name. For MAC, copy this file (overwrite the existing file) to: ~/.gitconfig 2.2.13 Edit the .gitconfig file Use your favourite editor to edit the new file (not the one ingit-course/misc). Change the [user] settings to reflect your information. Change the [difftool] and [diffmerge] directories so they point to the location where you have DiffMerge (if it installed okay).. For Windows the location should be: C:\\Program Files\\SourceGear\\Common\\DiffMerge\\sgdm.exe For MAC the location should be: /usr/local/bin/diffmerge If you could not install [difftool] or [diffmerge] then delete those lines in your .gitconfig file. 2.2.14 MAC only: make your output pretty On the MAC, change the ~/github directory and run the following command: git config global color.ui.auto This will make your git output colored in a similar way to the Windows powershell version. 2.2.15 Markdown Pad Each project has an associated README.md file that appears on its GitHub homepage.The extension .md stands for Markdown and is just an ASCii text file that contains simple formatting (such as bold or italics). There are two options we have used to readmarkdown files, choose one: The Markdown Pad 2 editor/viewer which is easy to use: http://markdownpad.com. Just get the free version. The Chrome extension for markdown viewing: https://chrome.google.com/webstore/detail/markdown-viewer/ckkdlimhmcjmikdlpkmbgfkaikojcbjk?hl=en. 2.3 Using Git and GitHub This section also has a recorded lecture to demonstrate the main concepts and ideas. The video is available here TODO and the slides from the talk are here TODO, though the notes below mostly replicate the slides. 2.3.1 Definitions Lets start with some definitions: Repository  essentially a directory containing all your files for a project (plus some files that Git uses). Git  a program that allows you to efficiently save ongoing versions of your files (`version control). GitHub  a website that hosts your repositories so that you can easily share code and collaborate with colleagues. Basically, the idea is that you work on your files in a repository on your computer, use Git on your computer when you are happy to keep your changes, and use GitHub to easily share the files. Here you will learn the important steps: Creating  create a new repository on GitHub Cloning  copying it to your local computer Committing  the crux of working with Git Collaborating  efficiently work with colleagues Conflicts  fixing conflict changes (happens rarely) 2.3.2 Creating a new repository Sign into your GitHub account, click on the Repositories tab, and press the New button. Give your repository a name. Lets call it test. Check Initialize this repository with a README. Leave Add .gitignore and Add a license set to None Click Create repository. You now have a new repository on the GitHub website. Next we will clone it onto your computer. 2.3.3 Cloning your new repository Copy the full URL (web address) of your test repository. Open the Git shell and navigate to your C://github directory (or whatever you called it when you created it in the setup instructions  its the place you are going to save all your Git repositories). Run the following command to clone your repository: git clone URL where URL is the url of your newly created repository (paste should work). You should now have a subdirectory called github/test on your computer. In Git shell, change to that directory (with cd test). So clone is Git speak for copying something from GitHub onto your local computer. This example has just one file (README.md). But the process is the same for a repository with multiple files and multiple directories, and the complate file sturcture is fully preserved. Windows only: Storing your credentials When you are using the Git shell for the very first time on Windows, issue the following command: git config --global credential.helper wincred This means that you dont have to repeatedly enter you GitHub password (just do it when you are first prompted). 2.3.4 Committing Create a new file, newFile.txt, in the github/test directory. Add a line of text at the start of the file and save it. Check the status of your (test) repository: git status It should say that you have an Untracked file called newFile.txt. You want to tell Git to start tracking it, by using: git add. gitignore Type git status again. You should see that the file is listed as a new file under Changes to be commited. Lets now commit it: git commit -a -m \"Add newFile.txt.\" The commit message (in the quotes) should be a useful message saying what the commit encapsulates (more on that later). Push the commit to GitHub: git push Check (refresh) the GitHub webpage and see your commit and the uploaded file. What just happened? We just used three of the main Git commands: git add &lt;filename&gt;  tell Git to start keeping track of changes to this file. You only need to tell Git this once. git commit -a -m \"Message.\"  committing your changes, which means tell Git you are happy with your edits and want to save them. git push  this sends your commit to the GitHub website. You always have your files stored locally on your computer (as usual), even if you dont add them or commit changes. When you push to GitHub then your colleagues can easily fetch (retrieve) them. Keyboard aliases (shortcuts) Now, git commit -a -m \"Message.\" is a bit much to type, so we have an alias for it: git com \"Message.\" This is defined in the .gitconfig file you installed in the git-setup instructions into C:\\Users\\YOUR-USER-NAME\\.gitconfig (for Windows). YOu can also add your own commands to that file. The -a means commit all changes of files that Git is tracking, and -m is to include a message. Since we usually want to do both of these, git com \"Message.\" is a useful shortcut. But it is important to realise it is an alias if searching online for help. Similarly: git s  for git status git p  for git push git d  for git diff git f  for git fetch From now on we will mostly use the aliases. Use the full commands if the .gitconfig file didnt work for you. Edit Readme.md Edit the Readme.md file. Add some simple comments describing the project such as: A test repository for learning Git. Look over the changes, commit them, and push them to your GitHub repository: git s git d (or git diff)  this gives a simple look at the differences between the last committed version and your current version (of all files; only one in this case) git com Initial edit of Readme.md git p (or git push) Refresh your GitHub web page and you should see your text (the Readme.md file is what is shown on the main page of your repo). If you got Diffmerge installed okay, instead of git diff you can do git difftool. This opens up, in turn, each file that changed since your last commit and shows you the differences. This is useful for changes that are more complex than can be easily see in the quick git d. 2.3.5 Exercise 1: create, edit and commit simpleText.txt Create a text file simpleText.txt in your local test repository. Add a line of text at the start and save it. Predict what git s will tell you, then type it in the Git shell to check. Add the file to the repository using the git commands: git add simpleText.txt git s  not necessary but useful to check you understand what is changing before you commit git com \"Adding simpleText.txt\" git p Add some more test to simpleText.txt then git com \"Message.\" and git p. Repeat this a few times to get the hang of it. git com frequently and git p occasionally (you do not have to push every commit), while intermittently doing git s and git d to understand whats changing. Keep and eye on your commits by refreshing the GitHub page. In reality when writing code/text you wont be committing quite so frequently, as your focus will be on the writing. Adding multiple files at once Often you add multiple files in a new directory. When you run git s, you will see a large list of Untracked files. They can be all added at once by simply adding the whole directory. 2.3.6 Exercise 2: multiple files Do the following, to get the idea of creating multiple files in a folder and committing that folder. Create a new directory in your test repository, using your normal method. Call it new-stuff. Add a few new test files to that directory called test1.txt, test2.txt, etc. Put some example text in one or more of them if you want. On the command line, check the status: git s You will see a listing showing the new-stuff/ directory in Untracked files. To add all the new files in preparation for a commit, issue the command: git add new-stuff/ Check the status of the repository again with git s It will now show all files in Changes to be committed Commit the changes: git com \"Added new-stuff directory.\" Push the changes to GitHub: git p Check your GitHub webpage and see your commit and that the files have been uploaded. That works no matter how many files are in your new-stuff directory. There could be a hundred and its the same command. Wildcard symbol * This is useful to know (no need to do it as part of the exercise): To add multiple files with similar names you can use the wildcard * symbol. You just added (told Git to keep track of) the new files in your new-stuff/ directory. If you add more new files to that directory, you will have to tell Git to track those also. This is because they are new  you havent told Git about them yet. Say you have 10 new files called idea1.txt, idea2.txt, , idea 10.txt. Instead of typing git add new-stuff/idea1.txt git add new-stuff/idea2.txt etc. you can just use the wildcard *: git add new-stuff/idea*.txt or even just git add new-stuff/*.txt or git add new-stuff/*.*. The .gitignore file But what if you dont want to add all the files that you create? Each repository can have a .gitignore file, in the root directory of the repository. Such a file has names of files (such as my-secret-notes.txt) or wildcard names (such as _*.pdf_ or _*.doc_ ) that will be completely ignored by Git. For an example, see https://github.com/pacific-hake/hake-assessment/blob/master/.gitignore, noting that the # can be used for comments. When sharing a repository with others, you want to share your code (for example, R, Python or Matlab code) and maybe data, but generally not share the output (such as figures that the code generates; more on this later). For reproducible research your colleague (or anyone) should be able to run your code to generate the results. Some programs you run may make temporary files that dont need to be tracked by Git, the names of which should also be included in your .gitignore. When sharing code or collaborating you want to keep your repository as clean as possible and not clutter it up with files that other people dont need. So when you run git s and see untracked files that you dont want to be tracked, add them (or a suitable wildcard expression) to your .gitignore file so that they are not added inadvertently. This will also simplify your workflow (you dont need to keep being reminded that you have untracked files). If you are on MacOS and you find that folders have a .DS_Store file in them, then create (and add and commit) a .gitignore file with .DS_Store as a line. Git Workflow You have now learnt the basics of using Git. By creating a public repository on GitHub you can now release your code to the world! You can also choose the private repository option when creating a repository, so that you can control who can see it. Go into Settings--Manage Access to add collaborators. 2.3.7 Collaborating Now we will show how to collaborate with colleagues, which is where the usefulness of Git will become more apparent. There are a few different ways to collaborate using Git and GitHub. We will focus on the following one since it is the simplest, and is what you need to collaborate with colleagues. Concept: there is a project where people contribute to a main repository that is considered the master copy. Everyone clones directly from the creators repository. All collaborators push their commits to the main repository (the creator has to add them as collaborators once on GitHub). Since the creator has to grant permission, you wont have just anyone contributing to (and maybe messing up your work), just your trusted collaborators. But you have to trust your team to not mess things up (more on that later!). Okay, so in the video we demonstrated the following: Kim creates new repo called collaborate (and clones it to her computer). Andy clones it also. On GitHub, Kim gives Andy push access to her collaborate repo. Both do some edits (create some new simple text files). For Andy to get Kims updates (and vice versa), it was just: git fetch (or just git f)  fetches the latest version of the repository from GitHub onto your computer. Your local files have not yet changed (check them), but Git has the changes stored on your computer (?!?). git rebase  updates your local repository (the committed files on your computer) with the changes you have just fetched, merging both peoples work together. git p  pushes the merged changes back up to GitHub so that the other person can get them. That is the basic workflow. We also showed an example of git p not being allowed for Person A because there are recent commits on GitHub (by Person B) that Person A has not yet merged into their local version of the repository. Here is an example of the error message you get: While a bit lengthy, the error message is useful. It forces you to get the other persons work before you push yours. You do this by: git f git rebase. So to be allowed to push, just fetch to get the new commits onto your computer, and then rebase to combine the commits into your local version. Then you can git push. Here is a full screenshot (g is just a shortcut for git). The green up arrow number 8 tells me I have 8 commits to push to GitHub. The yellow arrows I think of as just implying I need to do a rebase (before doing that I might browse through the other persons commits on GitHub): After the rebase I was allowed to push and then everything is up to date. A bit more about git rebase Andy commits local changes, tries to git push but is told to first git fetch (to get Kims changes from GitHub). Andy does git fetch and then git rebase. What git rebase does is basically rewind to the last common commit that both people had, and then add one persons commits and the others. Andy then does git push to push his commits to GitHub (from where Kim will fetch them when shes ready). Providing there are no conflicts, this will work fine. Another option is to do a git merge, which basically creates a new commit that merges both peoples work together. Our groups used to use git merge and now use git rebase; some people dont like git merge because it adds extra commits. For a more in-depth understanding see https://reflectoring.io/git-rebase-merge/ for one of the clearer explanations out there concerning rebase v merge. Note that the error in the above screenshot (when I could not git push) told me that I might want to do git pull. This is basically git fetch git merge in one command, but it seems preferable to do git fetch git rebase. Fixing a conflict A conflict happens when two people have edited the same line(s) of the same file. Conflicts happen relatively rarely and can be generally avoided by co-ordinating with collaborators so that you are working on different files. But, they will happen and you need to know how to resolve them. Git forces you to explicitly decide whose changes to keep  this is a good thing, since you want a human to make such a decision. In the video we demonstrated a conflict. Fixing a conflict The best approach I have found to fixing a conflict is the following: Trying git rebase will tell you there is a conflict. git rebase --abort  do this to abort the rebase attempt. git merge  this will tell you there is a conflict. Open the file(s) with the conflict and edit the text (see below). git add &lt;filename(s)&gt;  you have to then add the files that had the conflict (I am not sure why this is necessary, I just do it). git com \"&lt;message&gt;\"  in your commit message you can explain how you fixed the conflict. This is useful so that your collaborators know you have resolved a conflict (they can look at the commit to see if they are happy with it). The merge message will tell you which files are conflicting. Open those files one by one, and you will see the conflicted section bracketed like the following: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Line(s) of text/code which are currently in your file. ======= Line(s) of text/code which are trying to merge in, but conflict. &gt;&gt;&gt;&gt;&gt;&gt; origin/main where origin/main refers to the version you have fetched from GitHub. All you do is remove the line(s) of text that you do not want to keep (or edit the line(s) to be something else entirely), and remove the bracketing lines &lt;&lt;&lt;... and &gt;&gt;&gt;..., and the ====== line. Save each conflicted file and then (as mentioned previously): git add &lt;filename(s)&gt; git com &quot;Kept Kim&#39;s edits as more consistent with remaining text.&quot; git p 2.3.8 Exercise 3: collaborating on a single repository If you have a colleague available, try what we just did: Person 1 creates a new repository on GitHub and clone to their computer. Give the Person 2 push access to the repository (on the repo page on GitHub: Settings  Manage access  Invite a collaborator) Person 2 clones to their computer Both create a simple text file (use different filenames), add some text and, as usual, add, commit, and push. git fetch and git rebase to get the other persons file. Continue editing either file, committing, and pushing. If you get the push error (shown earlier), refresh GitHub repository site to see recent commits (click on the XX commits link). You can easily spot the other persons recent commits. Click on one (the bold message) to see details. Purposefully create a conflict (both edit the same line of the same file). Resolve it as described earlier. In practice you wont commit so frequently when working, but this is good to get the hang of it. Congratulations Congratulations, you now know the few basic commands and functionality needed to collaborate with Git and GitHub. It takes a bit of practice, but it is very powerful. 95% of the time, this is all you are doing: Change some code. git s git d git com &quot;My commit message&quot;` git p (the git s and git d are useful to check you have changed only what you think you have changed). If GitHub does not allow you to push: git fetch git rebase If conflicts, then git rebase --abort git merge fix the conflicts manually and then git add &lt;conflicted file(s)&gt; git com &quot;Message to explain what you did&quot; git p Change some code and repeat! The next section gives slightly more advanced background that should further improve your understanding (including why Git is useful even when not collaborating or sharing your code), plus tips for improving your workflow. 2.4 Beyond the basics Here are some Git and GitHub concepts and tips that go beyond the basics that we just covered. 2.4.1 Workflow tips Realise that you still edit and save your files in the usual way on your computer. If you dont do Git commits you will still have the latest versions of your files on your computer (as you would if you werent using Git). So if you do get stuck with Git you can carry on working as normal (though you probably do want to try and fix it at some point). When collaborating: If working closely with others, when you start each day (or after a break) then make sure you are up to date and have all their commits. Refresh the GitHub page for you repository, and git fetch (or just git f)and git clone if needed. (To be safe you can git f and git s to check). We find it helpful to co-ordinate our work (Slack is useful for this, or use GitHub Issues for complex discussion  see below), so that if multiple people are working at the same time, you are at least not working on exactly the same parts, just to reduce conflicts. Commit fairly frequently and write helpful commit messages (so your colleagues get an idea of what youve done in each commit). Push less frequently, and dont push code that doesnt work  that will annoy your colleagues. And then they (and probably you) may both spend time fixing it. To see who last edited a particular piece of code, when viewing the file on GitHub click Blame (you can even click on the square icon to go to the previous commit that changed that line): GitHub Issues GitHub Issues are very useful for discussing issues with your repo. For our annual Pacific Hake assessment we have used them extensively over the years: The Issues tab lists our current Open issues  we have 20, of which five (the most recently posted) are shown here. We are currently in-between assessments (and not working on it), so we have created Issues that we want to think about or deal with for next year. This avoids forgetting about ideas or losing them in old emails. Issues are intuitive to use. There is a bright green New Issues button to create new ones, you give a title and then write some details, people can reply, you can assign people to look at them, and you can close them. In the above screenshot you can see that we have closed 815 issues (this was over several years). Useful tip: when doing a commit that refers to an Issue, if you refer to the Issue number (with #&lt;number&gt;) in your commit message, then after pushing that commit the Issue on GitHub will automatically mention and link to the commit: git com \"Add more options to fancy_function(), #21.\" will mention the commit when you look at the issue. You can even automatically close the issue by saying closes #21 in your commit message: git com \"Add more options to fancy_function(), closes #21.\" Issues are particularly useful to avoid cluttering up code with commented notes or ideas that you may easily not come back to, or avoiding endless emails that end up getting overlooked. You dont have to fix an Issue to close it, you can decide not to pursue, but at least you have made a decision. (We also use Slack a lot to communicate, but moreso for quick questions or bouncing ideas around  Issues are better for stuff that you want to come back to at some point). You may receive emails regarding Issues, but if you use GitHub a lot you will see Notifications (the blue dot on the bell in the top-right corner when signed in on GitHub) and that will show you new Issues of repositories you are involved with, or if anyone has updated an Issue. GitHub organizations If you will frequently collaborate with colleagues, you can create an Organization on GitHub and invite collaborators to it (click on your GitHub photo in the top-right corner, Settings, Organizations). Then they will automatically have access to all repositories created under the Organization. You can choose the security settings. 2.4.2 So Ive made some changes but dont really want to keep them  git stash If youve changed some code but have not committed it, and then maybe got in a mess and just want to go back to your last commit, you can stash your changes git stash and to include a message (for your future self): git stash save &quot;Message&quot; This stashes them away such that they can be retrieved later This is handy. You may think you dont want to keep those changes, but sometimes you may later wish you had kept them somehwere. Note this only for files that Git is tracking (i.e. files that have been added at some point). You can have multiple stashes, seen by doing: git stash list To retrieve the last stash: git stash pop TODO: check rules for other stashed things. TODO: Often youre working on something but its not finished, but you want to push it. Stash, then make branch then copy files in. Tricky to do all with git, so do manually. TODO: see my README for details, and test what to do 2.4.3 Pull requests TODO 2.4.4 The power to go back With Git you can revert back to any previous state of your repository. This is very powerful, though slightly scary at first. Do this with your test repository, that should have some files in it from the earlier excercise: git s to make sure you are all up-to-date (commit and/or push if necessary). In File Explorer (or whatever you use) look at your repository, you should see all your files, including the new-stuff\\ directory. Look at the commit tab on GitHub for your test repo and click on the clipboard icon to copy the HASH number thingy to the clipboard . In Git shell: git checkout HASH (where HASH is the pasted HASH, or git co HASH using our Alias) Look at File Explorer again  your new-stuff directory should have  disappeared!! (If it hasnt disappeared then open it  the test files, i.e. test1.r, test2.r, etc. should be gone, but your text editor may have saved backup versions; manually delete them plus the new-stuff/ directory.) You are now back to the very first version of your repo! Powerful and scary. Now, to get your files back to the most recent version you had committed: git checkout main (it used to be git checkout master, the names have recently changed). Thats it! Check that your files are back. All this means that you can revert to any previous commit in your repository. This is very reassuring. For example you have some complex code that you realise is now a complete mess and you want to go back to yesterdays version of everything. In practice you rarely actually do this, but its very comforting to know that you can. Consequently, your workflow is less cluttered and more tractable than having to save multiple versions of the same files with dates in the filename, such as this nightmare: Retrieving older work in practice I think there are fancy ways that Git can replace a current file with a version from an earlier commit. But, in practice (especially since you rarely want to do this) it is a bit safer to do the following: Say you are up-to-date (git s says all is good), but your program my_code.R just isnt working and you want to go back to the version you had yesterday at commit number abc123. git co abc123 (or git checkout abc123) to checkout the earlier commit, which includes the old version of my_code.R that you want get. Copy my_code.R to a new file my_code_old.R. In the shell you can just do this with cp my_code.R my_code_old.R. Do NOT edit my_code.R or make any changes, as you may end up with a scary DETACHED HEAD warning. git co main to checkout the latest version again. Since you have NOT done git add my_code_old.R, Git is not tracking my_code_old.R and so it is just sitting in your folder as normal. Now you can manually copy what you want from my_code_old.R into my_code.R to fix your problem. It could be the full file, or just some part of it. Then commit as normal. At some point you can delete my_code_old.R so it is not hanging around, but you dont have to. (Though maybe make a note in it as to which commit it was from, in case you do need it again). 2.4.5 So how does Git do all this? By now youre probably wondering how Git keeps track of everything. Git does not keep versions of code, it keeps commits. The commits are kept track of using a HASH key which is a generated 40-digit key in hexadecimal (base 16). The hashes are what you see on GitHub and in various places when you use Git shell. By stitching all the commits back together again, Git can recreate all your code. There is a hidden .git/ directory in each repository. Look at the .git/objects/ subdirectory. Each subdirectory name is the first two digits of a HASH. The rest of the digits of the HASH are the filenames in the subdirectory. You can basically think of the hashes as representing commits (apparently they can also be blobs and trees, whatever they might be). I think of the files in the subdirectories containing the differences between each commit. Because of these structures, Git can go back and rebuild any or all files at any commit, and even have different directory structures at each commit. Since Git is keeping track of differences between files, this all works best for plain ASCii (text) files, such as .R, .txt, .Rmd, etc. Git does work for binary files, such as .xls, .docx, .RData, but since changes to the files are not easily saved (Git essentially has to resave the whole file at each commit), this is not very efficient and may make your repository large. Such files will be fully resaved every time they are changed. Think of a binary file as something that you cannot open in a text editor and read (it does not contain simple ASCii letters and numbers). Exceptions: often you may have an image or photo or other type file that you need to share for a document, but it isnt going to keep changing. So thats fine to commit. An example of why you should not commit binary files: A collaborator was running some R code (and correctly committed the .R files so that I could run it), but also committed the results, which included .pdf, .png and .RData files, which can get quite large. But, these latter files got updated every time the code was run. So changing one line of the .R code (which Git deals with very efficiently), and running that code and committing, resulted in the new .pdf etc. files being fully saved (since Git cannot just save the difference from the last commit because they are binary files). Even if, say, one point changes on a figure in a graph in a .pdf file, Git has to save the whole new version. This ended up with .git/objects/pack (whatever that might be!) being 2.8Gb. I needed space quickly on my computer so just deleted four files in .git/objects/pack, which freed up 1.6Gb. Note that I still had the actual final versions of files (as you would if not using Git), but just not the full repository history. However, when I tried to later do some work and then commit I got lots of fatal errors with scary messages like bad object HEAD and the awesomely titled You are on a branch yet to be born: I just had to start again from scratch (reclone I think). Take-home message: Dont mess with the .git directory!! 2.4.6 Git terminology At some point you will likely need to search online for some help (often questions are posted and answered on the excellent stackoverflow website). A bit more understanding of terminology will help you. Remember that Git keeps commits. Several of these commits have pointers to them that have special names: HEAD points to the commit you are currently on in the Git shell. main or master is the default branch when you set up a repository on GitHub (there are two names because of recent changes on GitHub). 2.4.7 Branching So far we have only worked on the main branch. Sometimes you want to create a new branch that branches off from the main branch. Its bit like a tree branching, except that at some point you want your new branch to be merged back into main. For example, you may want to try adding some new code to your project, but dont want to break what is already there. You may do this even if working alone, but its especially useful if you are collaborating, or if, say, you have an R packages hosted on GitHub that anyone may be downloading  you dont want to annoy them by pushing experimental code that doesnt work. So you would create a new branch, work on that new branch (i.e. commit changes to the new branch), and when you are happy with your new changes you can easily merge it all back into main. Working on a new branch When creating a new branch, your starting point is identical to the branch you were when you created the new one. In the Git shell navigate into your test repository: cd test Depending on your set up, you should see main indicated somewhere (if not do git s and it should say On branch main. Make sure you are up-to-date and have committed all changes (git s, and commit if necessary). Create a new branch called temp, this will be based off the latest commit of the main branch you are currently on: git checkout -b temp (We have an alias for that: git cb temp`). You will be automatically placed in the new branch called temp, and commits you make will now occur in that branch only. Make and commit some changes (e.g. add a new file)  these will now be on your temp branch. You can push to GitHub. The first time you try git p, the Git shell will tell you that you need to type the following so that future pushes go to the new branch: git --set-upstream origin BRANCH-NAME Check the GitHub webpage to see that your branch was pushed. You repository page (that will still be looking at your main branch) may tell you that there is a temp branch with more recent commits than main. If not then if you click on the main drop-down menu: it should give you the option to look at your new temp branch. (The 1 branch in the above image should also say 2 branches). You can now view your new file in your new temp branch on GitHub. A graphical way to see and understand branching is to click on InsightsNetwork to see the Network graph. The Network Graph is a useful visualization tool, where each commit is shown as a point on the graph (the numbers along the top are the dates). You can hover your mouse over a commit to see who committed it and the commit message. You can click to see full details of the commit. The Network Graph is particularly useful if you or others are working on multiple branches, or to check details about merges. Okay, back in your Git shell you can easily switch back to your original main branch: git checkout main (or the alias git co main). You will see that the file you just added is gone, because it only exists in the temp branch at this moment. Imagine that in your temp branch you did several commits to create a new function in your code, or have added some new text to a report. Now you are happy with what youve done you want to merge it back into the main branch. To view all local branches: git branch There is an asterisk next to the branch you are currently in. To switch to another branch (main in our case): git checkout main To combine the changes from the temp branch: git rebase temp or git merge temp Now the file you created in the temp branch now appears in the main branch. All commits done in the temp branch will now be in the main branch as well. If there was a merge conflict, you must fix it at this point (see earleir). Once youve merged your temp branch into main, you dont really need temp any more and so it is good protocol to delete to keep things tidy: git branch -d temp If you have unmerged changes in a branch, you will not be allowed to delete it, but Git shell will tell you the command to forcibly delete it: git branch -D temp Warning  you wont be able to get any of those changes back once you do this. To remove a branch entirely from GitHub: git p origin --delete BRANCH-NAME 2.4.8 Undoing stuff If you make a commit followed by other commits, then realize you want to undo the earlier commit, you use revert: git revert HASH where HASH is the hash for the commit you want to undo. Remember that Git shell is smart enough that you only need the first five digits: git revert 1ef1d This actually creates a new commit with the automatic message Revert \"&lt;previous commit message&gt;\". Obviously, you have to be careful with this if youre changing something that was a few commits back, as you might mess up your code. Undoing changes not yet committed If youve made a mess in your working directory and you want to change everything back to the way it was on the last commit: git reset --hard HEAD If youve messed up a single file and just want that one file to go back to the way it was on the last commit: git checkout HEAD &lt;filename_to_restore&gt; Warning  running these commands will delete the changes you have made. Since you have not committed any changes, they will be lost. Make sure you are certain you dont need the changes before running these commands. If you arent sure if you need the changes again in the future, use git stash instead. Changing the commit message in the last commit If you make a commit then realize you want to change it (add more information, fix something that will confuse your colleagues, fix something that will confuse you tomorrow), you can change the commit message: git commit --amend -m \"Correct message.\" This only works on the last commit. If you already pushed the commit before realizing that the message needs modification, do this: git p --force after making the amendment to the commit message. "],["introduction-to-r-markdown.html", "3 Introduction to R Markdown 3.1 Motivation 3.2 Basic idea 3.3 Simple example 3.4 Output format 3.5 Further reading", " 3 Introduction to R Markdown 3.1 Motivation Say you get some tree data from a colleague, and spend a month writing lots of R code to analyse the data. Your code also produces beautiful figures and tables, that you then manually copy into a Word document. You have also written lots of text, and include specific calculated numbers in the text, such as the simple The average tree height was 10.1 m. Then your colleague sheepishly tells you that someone found an error in some of the data, and so you need to redo everything. Your heart sinks with the prospect of re-running all your code and making sure you manually copy the correct new figures into your document. Oh, and you need to redo the tables and check all the numbers in your text. The alternative modern approach is to use R Markdown. The idea is that you can generate a dynamic report. You write code that contains a mixture of your R code and your write up. You can use this for short analyses, scientific manuscripts, or even a complete thesis. This introduction, aimed at biologists, will get you started with the basics. You will then be in a good position to learn more details from the RStudio introduction and the online Definitive Guide to R Markdown. A key concept is that everything is written as code. You do not have to manually point and click anything, or copy and paste figures between directories. So once you understand how something works or have figured out some formatting that you like, you can just copy that code and use it elsewhere. Example application A recent 328-page document we wrote in R Markdown is A reproducible data synopsis for over 100 species of British Columbia groundfish. For each of 113 species, we produced two pages of figures: For each species, the layout of the figures is identical (even to show no data when none are available). Producing each figure and manually inserting them into a Word document would be extremely tedious. Instead, the production of the document is automated using R Markdown. Furthermore, the work is transparent and traceable. Because the code produces the figures (they are not pasted in from somewhere), we can trace back from the R Markdown code to see the R code that: pulled the data from databases fit models generated plots. In particular, we intend to periodically update the document as new data become available. While still a lot of work, it is less daunting knowing that the code is already available. On a practical level, the report has allowed anyone to see the data available, and has consequently increased data transparency between Fisheries and Oceans Canada, the fishing industry, non-governmental organizations, and the public. This is admittedly a very advanced example with a ton of code (several new R packages) and work behind it, but the idea is to show you what is possible. 3.2 Basic idea In the above tree example, if using Word, for example, you would have a sentence that says The average tree height was 10.1 m The 10.1 is hard-wired into your text, and you typed it on from the value 10.1 that your R code calculated (in a variable you calculated, lets say you called it avge_height). In your R Markdown document you have your R code and your text write up. You would equivalently have: The average tree height was `r avge_height` m Instead of 10.1 you refer directly to the variable avge_height that you have already calculated. When you render your R Markdown document (convert it from code into .pdf, .html or other formats), it automatically fills in the avge_height value as 10.1. The `r means that the next bit of code (until the next backtick) should be evaluated using R, and the result inserted. This is the basic idea. Then, when your colleague mentions the error (or, say, provides you with extra data) you can just re-run your code and the 10.1 will be automatically updated in your document. This concept extends to your tables and figures  they can all be automatically updated. 3.3 Simple example Here we will generate some data, show some of it in a table, plot it, and show the results of fitting a simple linear regression. Read through this and then you will download and run the code in the exercise. Generate data First well need some libraries: library(kableExtra) library(dplyr) library(xtable) Now generate some data: set.seed(42) n &lt;- 50 # sample size x &lt;- 1:n y &lt;- 10*x + rnorm(n, 0, 10) n [1] 50 So we are showing our R code here (we can choose to hide it if we like), and it has been executed, yielding the printed output of the value of n (because of the final line of the code). We can also embed results from R within sentences, for example: We have a sample size of 50. This is done (as mentioned above) by the code: We have a sample size of `r n` Note that you need the space straight after the r. We can also automatically say that the maximum value of the data is 506.5564788, or round it to a whole number: the maximum value of the data is 507. These were done by: the maximum value of the data is `r max(y)` the maximum value of the data is `r round(max(y))` Show some of the data Lets combine the data in a tibble (think of it as a data frame if you dont know what that is): data &lt;- tibble(x, y) data # A tibble: 50 x 2 x y &lt;int&gt; &lt;dbl&gt; 1 1 23.7 2 2 14.4 3 3 33.6 4 4 46.3 5 5 54.0 6 6 58.9 7 7 85.1 8 8 79.1 9 9 110. 10 10 99.4 # ... with 40 more rows (only the first 10 rows get printed here thanks to dplyr). To have a proper table, we can do kable(data[1:10,], caption = &quot;The first rows of my data.&quot;) Table 3.1: The first rows of my data. x y 1 23.70958 2 14.35302 3 33.63128 4 46.32863 5 54.04268 6 58.93875 7 85.11522 8 79.05341 9 110.18424 10 99.37286 (If youre running the code separately the exact style may look different because of settings we have, but pretty much everything is tweakable with the kable and kableExtra packages). Plot then fit a regression data Now to plot the data: plot(x, y) To fit and then print the summary regression output from R: fit &lt;- lm(y ~ x) print(summary(fit)) Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -27.403 -4.366 -1.193 8.319 21.072 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.7071 3.2719 1.133 0.263 x 9.8406 0.1117 88.124 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 11.39 on 48 degrees of freedom Multiple R-squared: 0.9939, Adjusted R-squared: 0.9937 F-statistic: 7766 on 1 and 48 DF, p-value: &lt; 2.2e-16 And for a report we can produce a simple table (including a caption) of output and the regression fit: kable(coefficients(summary(fit)), caption = &quot;Linear regression fit.&quot;) Table 3.2: Linear regression fit. Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.707115 3.2719100 1.133012 0.2628368 x 9.840634 0.1116686 88.123530 0.0000000 And create a plot: plot(x, y) abline(fit, col=&quot;red&quot;) Now, lets go back and change the data The big feature of dynamically generating reports is when you go back and change or update the input data. For example, changing the data in the above example and then re-running it to redo the report. The best way to demonstrate this is for you to do it in the following Exercise. 3.3.1 Exercise In R do library(rmarkdown) to make sure you have the rmarkdown package. If not then install it. Download this file onto your computer and put it where you want to work on this exercise. The file is an R Markdown (.Rmd) file that you can run by either clicking the knitr button in RStudio or doing rmarkdown::render(\"rmark-exercise.Rmd\") in R. Check that this has produced (rendered) an .html file document that looks similar to what you see above (dont worry if the styling is not identical, but the important content should be). Note that the .Rmd file is not repeating all the explanations that we gave above. Carefully read through the .Rmd file and compare it with what you see in the resulting .html. There are some comments in there, denoted by &lt;!-- comment --&gt; to help you, but you should be able to get the idea of how commands in the .Rmd file translate into output in the .html file. Copy the resulting rmark-exercise.html to rmark-exercise-orig.html, change n to 30 in rmark-exercise.Rmd, and re-render it. Compare the two .html files. You have done the same analyses but on different data. You changed one value, n, and consequently all the resulting calculations changed, and these are all updated in your new .html document! You have performed exactly the same analysis on your new data, including creating tables and figures. No copy-and-pasteing of tables, or manually keeping track of which figure corresponds to which analysis. That last bit is the crux of R Markdown. Once you understand that then you should incorporate it into your workflow. You can easily run identical analyses on two data sets, or do 10 runs of a model with different parameters, and easily compare the output. You can even get a bit clever with your writing by including an R ifelse statement to somewhat automate the text. Here is the results of some code (that will be included in the file): So the maximum value of \\(y\\) is 507, which is greater than the special value of 400. The greater than or less than part is given by `r ifelse(max(y)&gt;400, paste(&quot;greater than&quot;), paste(&quot;less than&quot;))` But you have to be careful and think about all possibilities  what if \\(y=399.9\\)? 3.4 Output format To keep it simple, in the Exercise we have set the output to be .html. For scientific documents, .pdf are preferable. For this you will need LaTeX installed. For decades, LaTeX has been the standard typesetting system for writing mathematical papers. In 2002, Friedrich Leisch created Sweave, to weave together R code for calculations and LaTeX code for writing up results into a .pdf file. (It was called Sweave because S was the precursor language to R, plus I expect Sweave sounded better than Rweave). Sweave motivated knitr (to knit together R code with text write ups) by Yihui Xie, which also allows html and other output formats. R Markdown was then created to allow simpler use of knitr, without learning lots of LaTeX commands. To create .pdf output, and use LaTeX for writing equations and customising your output, if you dont have LaTeX installed it then see the simple instructions another book by Yhiui Xie et al., the R Markdown cookbook. 3.5 Further reading As we said, the idea of this module was to give you a simple introduction to using R Markdown. The Definitive Guide to R Markdown is highly recommended (and often where you end up when Googling for how to do something), but you do not need to understand all of it (I know that Pandoc is doing stuff behind the scenes, but have managed to not need much more than that). Chapter 2 goes through the basics, including many things that we glossed over above, such as the code between the --- at the top of rmark-exercise.Rmd. For me to explain it properly, I would have to refer to the guidebook anyway. In practice, once you have something working in the style you like, you can just copy those settings for each new project. It can be handy to keep a readme file of containing commands/options/tips that you use a lot. "],["introduction-to-multivariate-analysis.html", "4 Introduction to multivariate analysis 4.1 Multivariate resemblance 4.2 Cluster Analysis 4.3 Ordination", " 4 Introduction to multivariate analysis In this module well be disccusing multivariate quantitative methods. Analyses such as linear regression, where we relate a response, y, to a predictor variable, x, are univariate techniques. If we have multiple responses, \\(y_1...y_n\\), and multiple predictors, \\(x_1...x_n\\), we need multivariate approaches. For example, we may wish to understand how both precipitation and soil type are related to plant community composition. In this question, we may be tracking the abundance of over a dozen different species and many different sites with different types of soil, precipitation and other environmental factors. You can easily see that this is not a situation that ordinary univariate approaches are designed to handle! There are many types of multivariate analysis, and in this module and the next, we will only describe some of the most common ones. We can think of these different types of analysis as laying at different ends of a spectrum of treating the data as discrete vs continuous, and relying on identifying a reponse variable a priori versus letting the data tell us about explanatory features, i.e., latent variables (Fig. 4.1. Figure 4.1: Types of multivariate analysis 4.1 Multivariate resemblance The starting point for a lot of the classic multivariate methods is to find metrics that describe how similar two individuals, samples, sites or species might be. A natural way to quantify similarity is to list those characters that are shared. For example, what genetic or morphological features are the same or different between two species? A resemblance measure quantifies similarity by adding up in some way the similarities and differences between two things. We can express the shared characters of objects as either: similarity (S), which quantifies the degree of resemblance or dissimilarity (D) which quantifies the degree of difference. 4.1.1 Binary Similarity metrics The simplest similarity metric just tallys the number of shared features. This is called a binary similarity metric, since we are just indicating a yes or no for each characteristic of the two things we wish to compare (Table 4.1). Table 4.1: List of shared attributes for two things Attribute Object 1 Object 2 Similar Attribute 1 1 0 no Attribute 2 0 1 no Attribute 3 0 0 yes Attribute 4 1 1 yes Attribute 5 1 1 yes Attribute 6 0 0 yes Attribute 7 0 1 no Attribute 8 0 0 yes Attribute 9 1 1 yes Attribute 10 1 0 no We could also use a shared lack of features as an indicator of similarity. The simple matching coefficient uses both shared features, and shared absent features, to quantify similarity as \\(S_m=\\frac{a+d}{a+b+c+d}\\), where a refers to the number of shared characteristics of object 1 and object 2, b is the number characterisitics that object 1 possesses but object 2 does not and so on (see Table 4.2). Table 4.2: Summary of shared and absent attributes Object 1 Object 2 Present Absent Object 1 Present a b Object 2 Absent c d We can further categorize similarity metrics as symmetric, where we regard both shared presence and shared absence as evidence of similarity, the simple matching coefficient, \\(S_m\\) would be an example of this, or asymmetric, where we regard only shared presence as evidence of similarity (that is, we ignore shared absences). Asymmetric measures are most useful in analyzing ecological community data, since it is unlikely to be informative that two temperature zone communities lack tropical data, or that aquatic environments lack terrestrial species. The Jaccard index is an asymmetric binary similarity coefficient calculated as \\(S_J=\\frac{a}{a+b+c}\\), while the quite similar Sørenson index is given as \\(S_S=\\frac{2a}{2a+b+c}\\), and so gives greater weight to shared similarities. Both metrics range from 0 to 1, where a value of 1 indicates complete similarity. Notice that both metrics exclude cell \\(d\\) - the shared absences. Lets try an example. In the 70s, Watson (1974) compared the zooplankton species present in Lake Erie and Lake Ontario. We can use this information to compare how similar the communities in the two lakes were at this time. We can see that they shared a lot of species (Table 4.3)! Table 4.3: Species presence and absence in lake Erie and lake Ontario (data from from Watson 1974) species erie ontario 1 1 1 2 1 1 3 1 1 4 1 1 5 1 1 6 1 1 7 1 1 8 1 1 9 1 1 10 1 1 11 1 1 12 1 1 13 1 1 14 1 1 15 1 1 16 1 1 17 1 1 18 1 1 19 1 0 20 0 1 21 0 0 22 0 0 23 0 0 24 0 0 We can calculate the similarity metrics quite easily using the table() function, where 1 indicates presence and 0 indicates absence. I have stored the information from Table 4.3 in the the dataframe lksp. Im just going grab the presences and absences, since I dont need the species names for my calculation. tlake = table(lksp[, c(&quot;erie&quot;, &quot;ontario&quot;)]) tlake ontario erie 1 0 1 18 1 0 1 4 a = tlake[1, 1] b = tlake[1, 2] c = tlake[2, 1] d = tlake[2, 2] S_j = a/(a + b + c) S_j [1] 0.9 S_s = 2 * a/(2 * a + b + c) S_s [1] 0.9473684 A final note: when a dissimilarity or similarity metric has a finite range, we can simply convert from one to the other. For example, for similarities that range from 1 (identical) to 0 (completely different), dissimilarity would simply be 1-similarity. 4.1.2 Quantitative similarity &amp; dissimilarity metrics While binary similarity metrics are easy to understand, there are a few problems. These metrics work best when we have a small number of characteristics and we have sampled very well (e.g., the zooplankton in Lake Erie and Ontario). However, these metrics are biased against maximum similarity values when we have lots of charactersitics (or species) and poor sampling. In addition, we sometimes have more information than just a yes or no which we could use to further characterize similarity. Quantiative similarity and dissimilarity metrics make use of this information. Some examples of quantitative similarity metrics are: Percentage similarity (Renkonen index), Morisitas index of similarity (not dispersion) and Horns index. However, quantitative dissimilarity metrics are perhaps more commonly used. In this case, we often talk about the distance between two things. Distances are of two types, either dissimilarity, converted from analogous similarity indices, or specific distance measures, such as Euclidean distance, which doesnt have a counterpart in any similarity index. There are many, many such metrics, and obviously, you should choose the most accurate and meaningful distance measure for a given application. Legendre &amp; Legendre (2012) offer a key on how to select an appropriate measure for given data and problem (check their Tables 7.4-7.6). If you are uncertain, then choose several distance measures and compare the results. Euclidean Distance Perhaps the mostly commonly used, and easiest to understand, dissimilarity, or distance, measure is Euclidian distance. This metric is zero for identical sampling units and has no fixed upper bound. Euclidean distance in multivariate space is derived from our understanding of distance in a Cartesian plane. If we had two species abundances measured in two different samples, we could then plot the abundance of species 1 and species 2 for each sample on a 2D plane, and draw a line between them. This would be our Euclidean distance: the shortest path between the two points (Fig. 4.2). Figure 4.2: Example of a Euclidean distance calculation in a two dimensional space of species abundance We know that to calculate this distance we would just use the Pythagorean theorem as \\(c=\\sqrt{a^2+b^2}\\). To generalize to \\(n\\) species we can say \\(D^E_{jk}=\\sqrt{\\sum^n_{i=1}(X_{ij}-X_{ik})^2}\\), where Euclidean distance between samples j and k, \\(D^E_{jk}\\), is calculated by summing over the distance in abundance of each of n species in the two samples. Lets try an example. Given the species abundances in Table 4.4, we can calculate the squared difference in abundance for each species, and sum that quantity. Table 4.4: Species abundance and distance calculations for two samples sample j sample k \\((X_j-X_k)^2\\) Species 1 19 35 256 Species 2 35 10 625 Species 3 0 0 0 Species 4 35 5 900 Species 5 10 50 1600 Species 6 0 0 0 Species 7 0 3 9 Species 8 0 0 0 Species 9 30 10 400 Species 10 2 0 4 TOTAL 131 113 3794 Then all we need to do is to take the square root of the sum to obtain the Euclidean distance. Did you get the correct answer of 61.6? Of course, R makes this much easier, I can calculate Euclidean distance using the dist() function, after creating a matrix of the two columns of species abundance data from my original eu dataframe. dist(rbind(j, k), method = &quot;euclidean&quot;) j k 61.59545 There are many other quantitative dissimilarity metrics. For example, Bray Curtis dissimilarity is frequently used by ecologists to quantify differences between samples based on abundance or count data. This measure is usually applied to raw abundance data, but can be applied to relative abundances. It is calculated as: \\(BC_{ij}=1-\\frac{C_{ij}}{S_{i}+S_{j}}\\), where \\(C_{ij}\\) is the sum over the smallest values for only those species in common between both sites, \\(S_{i}\\) and \\(S_{j}\\) are the sum of abundances at the two sites. This metric is directly related to the Sørenson binary similarity metric, and ranges from 0 to 1, with 0 indicating complete similarity. This is not at distance metric, and so, is not appropriate for some types of analysis. 4.1.3 Comparing more than two communities/samples/sites/genes/species What about the situation where we want to compare more than two communities, species, samples or genes? We can simply generate a dissimilarity or similarity matrix, where each pairwise comparison is given. In the species composition matrix below (Table 4.5), sample A and B do not share any species, while sample A and C share all species but differ in abundances (e.g. species 3 = 1 in sample A and 8 in sample C). The calculation of Euclidean distance using the dist() function produces a lower triangular matrix with the pairwise comparisons (Ive included the distance with the sample itself on the diagonal). You might notice that the Euclidean distance values suggest that A and B are the most similar! Euclidean distance puts more weight on differences in species abundances than on difference in species presences. As a result, two samples not sharing any species could appear more similar (with lower Euclidean distance) than two samples which share species that largely differ in their abundances. Table 4.5: Species abundance versus species presence and Euclidean distance sample A sample B sample C species 1 0 1 0 species 2 1 0 4 species 3 1 0 8 dist(t(spmatrix), method = &quot;euclidean&quot;, diag = TRUE) A B C A 0.000000 B 1.732051 0.000000 C 7.615773 9.000000 0.000000 There are other disadvantages as well, and in general, there is simply no perfect metric. For example, you may dislike the fact that Euclidean distance also has no upper bound, and so it becomes difficult to understand how similar two things are (i.e., the metric can only be understood in a relative way when comparing many things, Sample A is more similar to sample B than sample C, for example). You could use a Bray-Curtis dissimilarity metric, which is quite easy to interpret, but this metric will also confound differences in species presences and differences in species counts (Greenacre 2017). The best policy is to be aware of the advantages and disadvantages of the metrics you choose, and interpret your analysis in light of this information. 4.1.4 R functions There are a number of functions in R that can be used to calculate similarity and dissimilarity metrics. Since we are usually not just comparing two objects, sites or samples, these functions can help make your calculations much quicker when you are comparing many units. dist() offers a number of quantitative distance measures (e.g. euclidean,canberra and manhattan). The result is the distance matrix which gives the dissimilarity of each pair of objects, sites or samples. the matrix is an object of the class dist in R. vegdist() (library vegan). The default distance used in this function is Bray-Curtis distance, which is considered more suitable for ecological data. dsvdis() (library labdsv) Offers some other indices than vegdist (e.g. ruzicka (or Rika), a quantitative analogue of Jaccard, and Roberts. For full comparison of dist, vegdist and dsvdis,see http://ecology.msu.montana.edu/labdsv/R/labs/lab8/lab8.html. dist.ldc() (library adespatial) Includes 21 dissimilarity indices described in Legendre &amp; De Cáceres (2013), twelve of which are not readily available in other packages. Note that Bray-Curtis dissimilarity is called percentage difference (method = percentdiff). designdist() (library vegan) Allows one to design virtually any distance measure using the formula for their calculation. daisy() (library cluster) Offers euclidean, manhattan and gower distance. distance() (library ecodist) Contains seven distance measures, but the function is more for demonstration (for larger matrices, the calculation takes rather long). 4.2 Cluster Analysis When we have a large number of things to compare, an examination of a matrix of similarlity or dissimilatiry metrics can be tedious or even impossible to do. One way to visualize the similarity among units is to use some form of cluster analysis. Clustering is the grouping of data objects into discrete similarity categories according to a defined similarity or dissimilarity measure. We can contrast clustering, which assumes that units (e.g., sites, communities, species or genes) can be grouped into discrete categories based on similarity, with ordination, which treats the similarity between units as a continuous gradient (well discuss ordination in section 4.3). We can use clustering to do things like discern whether there are one or two or three different communities in three or four or five sampling units. It is used in many fields, such as machine learning, data mining, pattern recognition, image analysis, genomics, systems biology, etc. Machine learning typically regards data clustering as a form of unsupervised learning, or from our figure above (Fig 4.1), as a technique that uses latent variables because we are not guided by a priori ideas of which variables or samples belong in which clusters. 4.2.1 Hierarchical clustering: groups are nested within other groups. Perhaps the most familiar type of clustering is hierarchical. There are two kinds: divisive and agglomerative. In the divisive method, the entire set of units is divided into smaller and smaller groups. The agglomerative method starts with small groups of few units, and groups them into larger and larger clusters, until the entire data set is sampled (Pielou, 1984). Of course, once you have more than two units, you need some way to assess similarity between the clusters. There are a couple of different methods here. Single linkage assigns the similarity between clusters to the most similar units in each cluster. Complete linkage uses the similarity between the most dissimilar units in each cluster, while average linkage averages over all the units in each cluster (Fig. 4.3). Figure 4.3: Different methods of determining similarity between clusters Single Linkage Cluster Analysis Single linkage cluster analysis is one of the easiest to explain. It is hierarchical, agglomerative technique. We start by creating a matrix of similarity (or dissimilarity) indices between the units we want to compare. Then we find the most similar pair of samples, and that will form the 1st cluster. Next, we find either: (a) the second most similar pair of samples or (b) highest similarity between a cluster and a sample, or (c) most similar pair of clusters, whichever is greatest. We then continue this process until until there is one big cluster. Remember that in single linkage, similarity between two clusters = similarity between the two nearest members of the clusters. Or if we are comparing a sample to a cluster, the similarity is defined as the similarity between sample and the nearest member of the cluster. Lets try this with simulated data where we have 5 data units (e.g., sites, species, genes), that each have 5 different quantitative characters (e.g., number of individuals of a given species, morphological features, functions). cls = data.frame(a = c(5, 6, 34, 1, 12), b = c(10, 5, 2, 3, 4), c = c(10, 59, 32, 3, 40), d = c(2, 63, 10, 29, 45), e = c(44, 35, 40, 12, 20)) clsd = dist(t(cls), method = &quot;euclidean&quot;) round(clsd, 0) a b c d b 33 c 60 71 d 76 76 36 e 51 62 48 66 We can see that we construct the cluster diagram by first grouping a and b, followed by c &amp; d, and so on (Fig. 4.4). Figure 4.4: Example of using a dissimilarity matrix to construct a single-linkage cluster diagram 4.2.2 How many clusters? These hierarchical methods just keep going until all objects are included (agglomerative methods), or are each in their own group (divisive methods). However, neither endpoint is very useful. How do we select the number of groups? There are metrics and techniques to make this decision more objective (see the NbClust package. In this brief introduction, well just mention that for hierarchical methods, you can determine the number of groups a given degree of similarity, or set the number of groups and find the degree of similarity that results in that number of groups. Lets try. Well use the cutree() function that works on cluster diagrams produced by the hclust() function (Fig. 4.5). If we set our dissimilarity threshold at 40, we find that there are three groups: a&amp;b, c&amp;d, and e in its own group. Figure 4.5: Cluster diagram produced by the function hclust() with cut-off line at euclidean distance=40 for group membership a b c d e 1 1 2 2 3 4.2.3 Partitional clustering and Fuzzy clustering There are other means of clustering data of course. Partitional clustering is the division of data objects into non-overlapping subsets, such that each data object is in exactly one subset. In one version of this, k-means clustering, each cluster is associated with a centroid (center point), and each data object is assigned to the cluster with the closest centroid. In this method, the number of clusters, K, must be specified in advance. Our method is: Choose the number of K clusters Select K points as the initial centroids Calculate the distance of all items to the K centroids Assign items to closest centroid Recompute the centroid of each cluster Repeat from (3) until clusters assignments are stable K-means has problems when clusters are of differing sizes and densities, or are non-globular shapes. It is also very sensitive to outliers. In contrast to strict (or hard) clustering approaches, fuzzy (or soft) clustering methods allow multiple cluster memberships of the clustered items. Fuzzy clustering is commonly achieved by assigning to each item a weight of belonging to each cluster. Thus, items at the edge of a cluster may be in a cluster to a lesser degree than items at the center of a cluster. Typically, each item has as many coefficients (weights) as there are clusters that sum up for each item to one. 4.2.4 R functions for clustering hclust() calculates hierarchical, agglomerative clusters and has its own plot function. agnes() (library cluster) Contains six agglomerative algorithms, some not included in hclust. diana() divisive hierarchical clustering kmeans() - kmeans clustering fanny()(cluster package) - fuzzy clustering 4.2.5 Exercise: Cluster analysis of isotope data Lets try some of these methods on some ecological data. Try to work through the exercise semi-independently. Our first step is to download and import the dataset Dataset_S1.csv from Perkins et al. 2014 (see url below). This data contains 15N and 13C signatures for species from different food webs. Unfortunately, this data is saved in an .xlsx file. To read data into R one of the easiest options is to use the read.csv() function with the argument on a .csv file. These Comma Separated Files are one of your best options for reproducible research. They are human readable and easily handled by almost every type of software. In contrast Microsoft Excel uses a propriatory file format, is not fully backwards compatible, and although widely used, is not human readable. As a result, we need special tools to access this file outside of Microsoft software products Well download the data set using download.file(), and read it using the R library openxlsx (see example below).Once you have successfully read your data file into R, take a look at it! Type iso (or whatever you named your data object) to see if the data file was read in properly. Some datasets will be too large for this approach to be useful (the data will scroll right off the page). In that case, there are a number of commands to look at a portion of the dataset. For example, you could use a command like names(iso) or str(iso). One of the best things to do is plot the imported data. Of course, this is not always possible with very large datasets, but this set should work. library(openxlsx) urlj = &quot;https://doi.org/10.1371/journal.pone.0093281.s001&quot; download.file(urlj, &quot;p.xlsx&quot;, mode = &quot;wb&quot;) iso = read.xlsx(&quot;p.xlsx&quot;) plot(iso$N ~ iso$C, col = as.numeric(as.factor(iso$Food.Chain)), xlim = c(-35, 0), pch = as.numeric(as.factor(iso$Species)), xlab = &quot;d13C&quot;, ylab = &quot;d15N&quot;) legend(&quot;topright&quot;, legend = unique(as.factor(iso$Food.Chain)), pch = 1, col = as.numeric(unique(as.factor(iso$Food.Chain))), bty = &quot;n&quot;, title = &quot;Food chain&quot;) legend(&quot;bottomright&quot;, legend = as.character(unique(as.factor(iso$Species))), pch = as.numeric(unique(as.factor(iso$Species))), bty = &quot;n&quot;) Figure 4.6: Isotope data from Perkins et al (2014) We are going to use this data set to see if a cluster analysis on 15N and 13C can identify the foodweb. That is we are going to see if the latent variables identified by our clustering method match up to what we think we know about the data. Our first step is to create a dissimilarity matrix, but even before this, we must select that part of the data that we wish to use, just the 15N and 13C data, not the other components of the dataframe for the downloaded data. In addition, our analysis will be affected by the missing data. So lets get remove those rows with missing data right now using the complete.cases() function. The function returns a value of TRUE for every row in a dataframe that no missing values in any column. So niso=iso[complete.cases(mydata),], will be a new data frame with only complete row entries. The function dist() will generate a matrix of the pairwise Euclidean distances between pairs of observations. Now that you have a dissimilarity matrix, you can complete a cluster analysis. The function hclust() will produce a data frame that can be sent to the plot() function to visualize the recommended clustering. The method used to complete the analysis is indicated below the graph. Please adjust the arguments of the function to complete a single linkage analysis (look at ?hclust to determine the method to do this). str(iso) &#39;data.frame&#39;: 165 obs. of 7 variables: $ Replicate : num 1 2 3 4 5 6 7 8 9 10 ... $ Food.Chain : chr &quot;Wheat&quot; &quot;Wheat&quot; &quot;Wheat&quot; &quot;Wheat&quot; ... $ Species : chr &quot;Plant&quot; &quot;Plant&quot; &quot;Plant&quot; &quot;Plant&quot; ... $ Tissue : chr &quot;Leaf&quot; &quot;Leaf&quot; &quot;Leaf&quot; &quot;Leaf&quot; ... $ Lipid.Extracted: chr &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... $ C : num -30.1 -31.7 -30.1 -30.9 -31 ... $ N : num -3.47 -2.68 3.42 1.27 6.2 ... diso &lt;- dist((iso[, c(&quot;C&quot;, &quot;N&quot;)]), method = &quot;euclidean&quot;) p = hclust(diso, method = &quot;single&quot;) plot(p, cex = 0.5, main = &quot;&quot;) Figure 4.7: Cluster figure of isotope data from Perkins et al. 2014) When you graph your cluster using plot(), you notice that there are many individual measurements, but there are only a few large groups. Does it look like there is an outlier? If so, you may want to remove this point from the data set, and then rerun the analysis. The row numbers are used as labels by default, so this is easy to do (niso=niso[-5,]). When you examine the data set, you noted that there are 4 Food.chain designations. We will use the cutree() function to cut our cluster tree to get the desired number of groups (4), and then save the group numbers to a new column in our original dataframe. For example, iso$clust&lt;- cutree(p,4).We can then plot the data using colours and symbols to see how well our niso = iso[complete.cases(iso), ] niso = niso[-5, ] diso &lt;- dist((niso[, c(&quot;C&quot;, &quot;N&quot;)]), method = &quot;euclidean&quot;) p = hclust(diso, method = &quot;single&quot;) niso$clust &lt;- cutree(p, k = 4) # plotting the data with 4 groups identified by the # single-linkage cluster analysis superimposed plot(niso$N ~ niso$C, col = as.numeric(as.factor(niso$clust)), xlim = c(-35, 0), pch = as.numeric(as.factor(niso$Species)), xlab = &quot;d13C&quot;, ylab = &quot;d15N&quot;) legend(&quot;topright&quot;, legend = unique(as.factor(niso$clust)), pch = 1, col = as.numeric(unique(as.factor(niso$clust))), bty = &quot;n&quot;, title = &quot;cluster&quot;) legend(&quot;bottomright&quot;, legend = as.character(unique(as.factor(niso$Species))), pch = as.numeric(unique(as.factor(niso$Species))), bty = &quot;n&quot;, title = &quot;Species&quot;) Figure 4.8: Data from Perkins et al (2014) with grouping from single linkage clustering superimposed It doesnt look like our cluster algorithm is matching up with our Food.chain data categories very well. Wheat- and Nettle-based food chains cannot be distinguished, which makes sense when you consider that both of these plants are terrestrial and use a C3 photosynthesis system. If you are not happy with the success of this clustering algorithm you could try other variants (e.g., complete linkage) and a different number of groups. Lets try a non-hierarchical cluster analysis on the same data to see if it works better. The kmeans() function requires that we select the required number of clusters ahead of time (we want 4, so kclust=kmeans(niso[,c(C, N)], 4)), we can then save the assigned clusters to our dataframe and plot in a similar way Figure 4.9: K means clustering on Perkins et al (2014) data It looks like kmeans has the same problem with distinguishing C3 plant-based foodwebs. But we still get three groups that roughly map onto our information about the data. Were you able to produce the same graph? 4.3 Ordination While cluster analysis lets us visualize multivariate data by grouping objects into discrete categories, ordination uses continuous axes to help us accomplish the same task. Physicists grumble if space exceeds four dimensions, while biologists typically grapple with dozens of dimensions. We can order this multivariate data in order to produce a low dimensional picture (i.e., a graph in 1-3 dimensions). Just like cluster analysis, we will use similarity metrics to accomplish this. Also like cluster anlaysis, simple ordination is not a statistical test: it is a method of visualizing data. Essentially, we find axes in the data that explain a lot of variation, and rotate so we can use the axes as our dimensions of visual representation (Fig. 4.10). Figure 4.10: Synthetic axis rotation in ordination. We find the axis in 2D that explains most variation, and rotate to give a 1D representation Another way to think about it is that we are going to summarize the raw data, which has many variables, p, by a smaller set of synthetic variables, k (Fig. 4.11). If the ordination is informative, it reduces a large number of original correlated variables to a small number of new uncorrelated variables. But it really is a bit of a balancing act between clarity of representation, ease of understanding, and oversimplification. We will lose information in this data reduction, and if that information is important, then we can make the multivariate data harder to understand! Also note that if the original variables are not correlated, then we wont gain anything with ordination. Figure 4.11: Ordination as data reduction. We summarize data with many variables (p) by a smaller set of derived or synthetic variables (k) There are lots of different ways to perform an ordination, but most methods are based on extracting the eigenvalues of a similarity matrix. The four most commonly used methods are: Principle Component Analysis (PCA), which is the main eigenvector-based method, Correspondence Analysis (CA) which is used used on frequency data, Principle Coordinate Analysis (PCoA) which works on dissimilarity matrices, and Non Metric Multidimensional Scaling (nMDS) which is not an eigenvector method, instead it represents objects along a predetermined number of axes. Legendre &amp; Legendre (2012) provide a nice summary of when you should use each method (Table 4.6). Table 4.6: Domains of application of ordination methods (adapated from Legendre &amp; Legendre 2012) Method Distance Variables Principal component analysis (PCA) Euclidean Quantitative data, but not species community data Correspondence analysis (CA) X^2 Non-negative, quantitiative or binary data (e.g., species frequencies or presence/absence data) Principal coordinate analysis (PCoA), metric (multidimensional) scaling, classical scaling Any Quantitative, semiquantitative, qualitative, or mixed data Nonmetric multidimensional scaling (nMDS) Any Quantitative, semiquantitative, qualitative, or mixed data 4.3.1 Principal Components Analysis (PCA) Principal Components Analysis is probably the most widely-used and well-known of the standard multivariate methods. It was invented by Pearson (1901) and Hotelling (1933), and first applied in ecology by Goodall (1954) under the name factor analysis (NB principal factor analysis is also a synonym of PCA). Like most ordination methods, PCA takes a data matrix of n objects by p variables, which may be correlated, and summarizes it by uncorrelated axes (principal components or principal axes) that are linear combinations of the original p variables. The first k components display as much as possible of the variation among objects. PCA uses Euclidean distance calculated from the p variables as the measure of dissimilarity among the n objects, and derives the best possible k-dimensional representation of the Euclidean distances among objects, where \\(k &lt; p\\) . We can think about this spatially. Objects are represented as a cloud of n points in a multidimensional space with an axis for each of the p variables. So the centroid of the points is defined by the mean of each variable, and the variance of each variable is the average squared deviation of its n values around the mean of that variable (i.e., \\(V_i= \\frac{1}{n-1}\\sum_{m=1}^{n}{(X_{im}-\\bar{X_i)}^2}\\)). The degree to which the variables are linearly correlated is given by their covariances \\(C_{ij}=\\frac{1}{n-1}\\sum_{m=1}^n{(X_{im}-\\bar{X_i})(X_{jm}-\\bar{X_j})}\\). The objective of PCA is to rigidly rotate the axes of the p-dimenional space to new positions (principal axes) that have the following properties: they are ordered such that principal axis 1 (or the principal component has the highest variance, axis 2 has the next highest variance etc, and the covariance among each pair of principal axes is zero (the principal axes are uncorrelated) (Fig. 4.12). Figure 4.12: Selecting the synthetic axes in ordination So our steps are to compute the variance-covariance matrix of the data, calculate the eigenvalues of this matrix and then calculate the associated eigenvectors. Then, the jth eigenvalue is the variance of the jth principle component and the sum of all the eigenvalues is the total variance explained. The proportion of variance explained by each component, or synthetic axis, is the eigenvalue for the component divided by the total variance explained, while the rotations are the eigenvectors. Dimensionality reduction is the same as first rotating the data with the eigenvalues to be aligned with the principle components, then display using only the components with the greatest eigenvalues. 4.3.2 Exercise: PCA on the iris data Were going to use a sample dataset in R and the base R version of PCA to start exploring this data analysis technique. Get the iris dataset into memory by typing data(iris). Take a look at this dataset using the head(), str() or summary() functions. For a multivariate data set, you would like to take a look at the pairwise correlations. Remember that PCA cant help us if the variables are not correlated. Lets use the pairs() function to do this data(&quot;iris&quot;) str(iris) &#39;data.frame&#39;: 150 obs. of 5 variables: $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(iris[1:4]) Sepal.Length Sepal.Width Petal.Length Petal.Width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 pairs(iris[1:4], main = &quot;Iris Data&quot;, pch = 19, col = as.numeric(iris$Species) + 1) Figure 4.13: Correlation matrix for the iris data The colours let us see the data for each species, the graph is all the pairwise plots of each pair of the 4 variables (Fig. 4.13). Do you see any correlations? If there seem to be some correlations we might use PCA to reduce the 4 dimensional variable space to 2 or 3 dimensions. Lets rush right in and use the prcomp() function to run a PCA on the numerical data in the iris dataframe. Save the output from the function to a new variable name so you can look at it when you type that name. The str() function will show you what the output object includes. If you use the summary() function, R will tell you what proportion of the total variance is explained by each axis. pca &lt;- prcomp(iris[, 1:4]) summary(pca) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 2.0563 0.49262 0.2797 0.15439 Proportion of Variance 0.9246 0.05307 0.0171 0.00521 Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 4.3.2.1 Standardize your data There is a problem though, lets examine the variance in the raw data. Use the apply() function to quickly calculate the variance in each of the numeric columns of the data as apply(iris[,1:3], 1, var). What do you see? Are the variances of each the columns comparable? apply(iris[, 1:4], 2, var) Sepal.Length Sepal.Width Petal.Length Petal.Width 0.6856935 0.1899794 3.1162779 0.5810063 Using covariances among variables only makes sense if they are measured in the same units, and even then, variables with high variances will dominate the principal components. These problems are generally avoided by standardizing each variable to unit variance and zero mean as \\(X_{im}^{&#39;}=\\frac{x_{im}-\\bar{X_i}}{sd_i}\\) where sd is the standard deviation of variable i. After standardizaton, the variance of each variable is 1 and the covariances of the standardized variables are correlations. If you look at the help menu, the notes for the use of prcomp() STRONGLY recommend standardizing the data. To do this there is a built in option. We just need to set scale=TRUE. Lets try again with data standardization. Save your new PCA output to a different name. Take a look at the summary. p &lt;- prcomp(iris[, 1:4], scale = TRUE) summary(p) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 1.7084 0.9560 0.38309 0.14393 Proportion of Variance 0.7296 0.2285 0.03669 0.00518 Cumulative Proportion 0.7296 0.9581 0.99482 1.00000 We now have less variance explained by axis 1. This makes sense, because, as we will see in a moment, axis 1 is strongly influenced by petal length, but in the unstandardized data, petal length had larger variance than anything else. 4.3.2.2 Choose your axes Now we need to determine how many axes to use to interpret our analysis. For 4 variables it is easy enough to just look that the amount of variance, as we just did. For larger numbers of variables a plot can be useful. The screeplot() function will output the variance explained by each of the principle component axes, and you can make a decision based on that (e.g., screeplot(pca2, type=lines)). An ideal curve should be steep, then bend at an elbow  this is your cutting-off point  and after that flattens out. To deal with a not-so-ideal scree plot curve you can apply the Kaiser rule: pick PCs with eigenvalues of at least 1. Or you can select using the proportion of variance where the PCs should be able to describe at least 80% of the variance. It looks like synthetic axes 1 &amp; 2 explain most of the variation. This is, of course, always true, but in this case only a very small proportion of the variance is explanaed by axes 3 &amp; 4, so we dont need to consider them any further. So lets plot those out. 4.3.2.3 Plot your ordination A PCA plot displays our samples in terms of their position (or scores) on the new axes. We can add information about how much variation each axis explains, and colour our points to match species identity. In this 2D representation of 4 dimensional space, it looks like species I. versicolor and I. viriginica are the most similar (Fig. 4.14). pvar = round(summary(p)$importance[2, 1:2], 2) plot(p$x[, 1:2], col = as.numeric(iris$Species) + 1, ylim = c(-3, 3), cex = 0.7, pch = 16, xlab = paste0(&quot;PC1 (&quot;, pvar[1] * 100, &quot;%)&quot;), ylab = paste0(&quot;PC2 (&quot;, pvar[2] * 100, &quot;%)&quot;)) legend(&quot;topright&quot;, legend = unique(iris$Species), pch = 16, col = c(2, 3, 4), bty = &quot;n&quot;) Figure 4.14: PCA plot for the iris data We can also plot information about influence the various characteristics are having on each of the axes. The eigenvectors used for the rotation give us this information. So lets just print that out. Table 4.7: Eigenvectors for each variable and synthetic axis PC1 PC2 PC3 PC4 Sepal.Length 0.52 -0.38 0.72 0.26 Sepal.Width -0.27 -0.92 -0.24 -0.12 Petal.Length 0.58 -0.02 -0.14 -0.80 Petal.Width 0.56 -0.07 -0.63 0.52 We can see that a lot of information is coming from the petal variables for PC1, but less from the sepal variables (Table 4.7). We can plot this out to show how strongly each variable affects each principle component (or synthetic axis). We can see that petal width and length are aligned along the PC1 axis, while PC2 explains more variation in sepal width (Fig. ??). That is, petal length and petal width variables are the most important contributors to the first PC. Sepal width variable is the most important contributor to the second PC. To interpret the variable plot remember that positively correlated variables are grouped close together (e.g., petal length and width). Variables with about a 90 angle are probably not correlated (sepal width is not correlated with the other variables), while negatively correlated variables are positioned on opposite sides of the plot origin (~180 angle; opposed quadrants). However, the direction of the axes is arbitrary! The distance between variables and the origin measures the contribution of the variables to the ordination. A shorter arrow indicates its less importance for the ordination. Variables that are away from the origin are well represented. Avoid the mistake of interpreting the relationships among variables based on the proximities of the apices (tips) of the vector arrows instead of their angles in biplots. Another way to portray this imformation is to create a biplot which, in addition to the coordinates of our samples on the synthetic axes PC1 and PC2, also provides information about how the variables align along the synthetic axes. (Fig. 4.15). According to the plot, I. versicolor and I. virginica have similar petal length and width, but not that the axis direction is arbitrary and coud not be interpreted as suggesting that these two species have longer petal widths than I.setosa. I should note that I have used an arbitrary scaling to display the variable loadings on each axis. Some of the R packages will use a specific scaling that will emphasize particular parts of the plot, either preserving the Euclidean distances between samples or the correlations/covariances between variables (e.g., the vegan package can do this for you, see section 4.3.11). Figure 4.15: PCA biplot for the iris data There are some final points to note regarding interpretation. Principal components analysis assumes the relationships among variables are linear, so that the cloud of points in p-dimensional space has linear dimensions that can be effectively summarized by the principal axes. If the structure in the data is nonlinear (i.e., the cloud of points twists and curves its way through p-dimensional space), the principal axes will not be an efficient and informative summary of the data. For example, in community ecology, we might use PCA to summarize variables whose relationships are approximately linear or at least monotonic (e..g, soil properties might be used to extract a few components that summarize main dimensions of soil variation). However, in general PCA is generally not useful for ordinating community data because relationships among species are highly nonlinear. This nonlinearity can lead to characteristic artifacts, where, for example, community trends along environmental gradients appear as horseshoes in PCA ordinations because of low species density at opposite extremes of an environmental gradiant appear relatively close together in ordination space (i.e., arch or horseshoe effect). 4.3.2.4 R functions for PCA prcomp() - base R PCA() (FactoMineR library) dudi.pca() (ade4) acp() (amap) 4.3.3 Principle Coordinates Analysis (PCoA) The PCoA method may be used with all types of distance descriptors, and so might be able to avoid some problems of PCA. Although, a PCoA computed on a Euclidean distance matrix gives the same results as a PCA conducted on the original data 4.3.3.1 R functions for PCoA cmdscale() -base R, no package needed smacofSym() (library smacof) pco()(ecodist) pco()(labdsv) pcoa()(ape) dudi.pco()(ade4) 4.3.4 Nonmetric Multidimensional Scaling (nMDS) Like PCoA, the method of nonmetric multidimensional scaling (nMDS), produces ordinations of objects from any resemblance matrix. However, nMDS compresses the distances in a non-linear way and its algorithm is computer-intensive, requiring more computing time than PCoA. PCoA is faster for large distance matrices. This ordination method does not to preserve the exact dissimilarities among objects in an ordination plot, instead it represent as well as possible the ordering relationships among objects in a small and specified number of axes. Like PCoA, nMDS can produce ordinations of objects from any dissimilarity matrix.The method can also cope with missing values, as long as there are enough measures left to position each object with respect to a few others. nMDS is not an eigenvalue technique, and it does not maximize the variability associated with individual axes of the ordination. In this computational method the steps are: Specify the desired number m of axes (dimensions) of the ordination. Construct an initial configuration of the objects in the m dimensions, to be used as a starting point of an iterative adjustment process. (tricky: the end result may depend on this. A PCoA ordination may be a good start. Otherwise, try many independent runs with random initial configurations. The package vegan has a function that does this for you) Try to position the objects in the requested number of dimensions in such a way as to minimize how far the dissimilarities in the reduced-space configuration are from being monotonic to the original dissimilarities in the association matrix The adjustment goes on until the difference between the observed and modelled dissimilarity matrices (called stress), can cannot be lowered any further, or until it reaches a predetermined low value (tolerated lack-of-fit). Most nMDS programs rotate the final solution using PCA, for easier interpretation. We can use a Shephard plot to get information about the distortion of representation. A Shepard diagram compares how far apart your data points are before and after you transform them (ie: goodness-of-fit) as a scatter plot. On the x-axis, we plot the original distances. On the y-axis, we plot the distances output by a dimension reduction algorithm. A really accurate dimension reduction will produce a straight line. However since information is almost always lost during data reduction, at least on real, high-dimension data, so Shepard diagrams rarely look this straight. Lets try this for the iris data. We can evaluate the quality of the nMDS solution by checking the Shephard plot as : stressplot(nMDS, main = Shepard plot). In addition to the original dissimilarity and ordination distance, the plot displays two correlation-like statistics on the goodness of fit. The nonmetric fit is given by \\(R^2\\), while the linear fit is the squared correlation between fitted values and ordination distances (Fig. ?? ). There is some deformation here, but in general the representation is really quite good. nMDS often achieves a less deformed representation of the dissimilarity relationships among objects than a PCoA in the same number of dimensions. But nMDS is a computer-intensive iterative technique exposed to the risk of suboptimum solutions. In comparison, PCoA finds the optimal solution by eigenvalue decomposition. 4.3.4.1 R functions for nMDS metaMDS() (vegan package) isoMDS( ) (MASS) 4.3.5 Exercise: nMDS and PCoA We are going to use the vegan package, and some built-in data with it to run the nMDS and PcOA. Varespec is a data frame of observations of 44 species of lichen at 24 sites. Well calculate both an nMDS and a PCoA using the (cmdscale() function) on the Bray-Curtis distance matrix of these data. In each case, we will specify that we want 2 dimensions as our output. The vegan wrapper for nMDS ordination (metaMDS() standardizes our community data by default, using a Wisconsin transform. This is a method of double standardization that avoids negative values in the transformed data, and is completed by first standardizing species data using the maxima, and then the site by totals. We will have to apply this standardization manually for the PCoA analysis, and then calculate the dissimilarity matrix. library(vegan) data(varespec) nMDS &lt;- metaMDS(varespec, trymax = 100, distance = &quot;bray&quot;, k = 2, trace = FALSE) svarespec = wisconsin(varespec) disimvar = vegdist(svarespec, method = &quot;bray&quot;) PCoA &lt;- cmdscale(disimvar, k = 2, eig = T, add = T) str(PCoA) List of 5 $ points: num [1:24, 1:2] -0.118 -0.103 0.182 0.486 0.106 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:24] &quot;18&quot; &quot;15&quot; &quot;24&quot; &quot;27&quot; ... .. ..$ : NULL $ eig : num [1:24] 1.208 0.832 0.743 0.491 0.461 ... $ x : NULL $ ac : num 0.178 $ GOF : num [1:2] 0.298 0.298 Well plot the PCoA and the nMDS side by side to see if they differ, using the par(mfrow()) functions. In this case, our species are the variables and our sites are the objects of our attention. For the nMDS it does not make sense to plot the species as vectors, as that implies directionality or increasing abundance, and there is no reason to assume that the abundance will increase linearly in a given direction across the nMDS plot. For this package, the species score is calculated as the weighted average of the site scores, where the weights are the abundance of that species at each site. If we look at the object PCoA we see the new 2D coordinates for each site (points). These are raw scores, but we can weight them using the in the same way as the nMDS using the wascores() function. We can plot the results as plot(PCoA$points). But, since this will be a crowded plot, lets use a vegan package function ordipointlabel() for both instead, which will use an optimization routine to produce the best plot par(mfrow = c(1, 2)) ordipointlabel(nMDS, pch = c(NA, NA), cex = c(1.2, 0.6), xlim = c(-0.6, 1.2)) abline(h = 0, col = &quot;grey&quot;) abline(v = 0, col = &quot;grey&quot;) PCoA$species &lt;- wascores(PCoA$points, varespec, expand = TRUE) ordipointlabel(PCoA, pch = c(NA, NA), cex = c(1.2, 0.6), xlab = &quot;PCoA1&quot;, ylab = &quot;PCoA2&quot;, xlim = c(-0.6, 1), ylim = c(-0.5, 0.6)) abline(h = 0, col = &quot;grey&quot;) abline(v = 0, col = &quot;grey&quot;) Figure 4.16: Biplots of the lichen data for nMDS and PCoA ordinations Interpretation of the plots is straightforward: sites ordinated closer to one another are more similar than those ordinated further away. We can interpret the nMDS, remembering that the first ordination axis corresponds to the most variance in our data and so on. Looking at the plot, it seems that sites 28, 27 and possibly 21 are pretty similar to each other, and different from other sites, possibly due to the species aligned in that direction on the x-axis such as Betupube. Sites 2 and 5 might be rather different from other sites. The might be due species like Cladcerv for site 2. Some sites (e.g., 6, 13, 20) are not well distinguished by the ordination. Others like 9, 10, 11, and 12 might group together. PCoA gives us broadly the same information and also suggests that sites 21, 28, 27 are similar to each other and different from other sites. This visualization also agrees that sites 2 and 5 might be different from the others, and that 9, 10, 11, and 12 might be similar. 4.3.6 Constrained Ordination The patterns we see in the previous exercise are created by differences among thie sites in the relative abundances in species aligned with the major direction of difference (e.g. Betupube). However, biologists often go further than this, and attempt to explain the differences in characteristics (in this case, species abundances) that drives data object differences (in this case, sampling sites) by superimposing relationships of environmental variation associated with the sites in a regression type exercise. This is constrained or canonical ordination Simple (or unconstrained) ordination is done on one data set, and we try to explain/understand the data by examining a graph constructed with a reduced set of orthogonal axes. The ordination is not influenced by external variables; these may only be considered after we construct the ordination. There is no hypotheses or hypothesis testing: this is an exploratory technique only. In unconstrained ordination axes correspond to the directions of the greatest variability within the data set. In contrast, constrained ordination associates two or more data sets in the ordination and explicitly explores the relationships between two matrices: a response matrix and an explanatory matrix (Fig 4.17). Both matrices are used in the production of the ordination. In this method, we can formally test statistical hypotheses about the significance of these relationships. The constrained ordination axes correspond to the directions of the greatest variability of the data set that can be explained by the environmental variables Figure 4.17: Diagram showing the differences between uncontrained ordination, multiple regression and constrained ordination (adapted from Legendre &amp; Legendre 2021??) There are two major methods of constrained commonly used by ecologists. Both combine multiple regression with a standard ordination: Redundancy analysis (RDA) and Canonical correspondence analysis (CCA). RDA preserves the Euclidean distances among objects in matrix, which contains values of Y fitted by regression to the explanatory variables X. CCA preserves the \\(\\chi^2\\) distance (as in correspondence analysis), instead of the Euclidean distance. The calculations are a bit more complex since the matrix contains fitted values obtained by weighted linear regression of matrix of correspondence analysis on the explanatory variables X 4.3.7 Redundancy Analysis Redundancy analysis was created by Rao (1964) and also independently by Wollenberg (1977).The method seeks, in successive order, linear combinations of the explanatory variables that best explain the variation of the response data. The axes are defined in the space of the explanatory variables that are orthogonal to one another. RDA is therefore a constrained ordination procedure. The difference with unconstrained ordination is important: the matrix of explanatory variables conditions the weights (eigenvalues), and the directions of the ordination axes. In RDA, one can truly say that the axes explain or model (in the statistical sense) the variation of the dependent matrix. Furthermore, a global hypothesis (H0) of absence of linear relationship between Y and X can be tested in RDA; this is not the case in PCA. As in PCA, the variables in Y should be standardized if they are not dimensionally homogeneous (e.g., if they are a mixture of temperatures, concentrations, and pH values), or transformations applicable to community composition data applied if data is species abundance or presence/absence (Legendre &amp; Legendre 2012). As in multiple regression analysis, matrix X can contain explanatory variables of different mathematical types: quantitative, multistate qualitative (e.g. factors), or binary variables. If present, collinearity among the X variables should be reduced. In cases where several correlated explanatory variables are present, without clear a priori reasons to eliminate one or the other, one can examine the variance inflation factors (VIF) which measure how much the variance of the regression or canonical coefficients is inflated by the presence of correlations among explanatory variables. As a rule of thumb, ter Braak recommends that variables that have a VIF larger than 20 be removed from the analysis. (Beware: always remove the variables one at a time and recompute the analysis, since the VIF of every variable depends on all the others!) So our steps for an RDA are a combination of the things we would do for a multiple linear regression, and the things we would do for an ordination: Multivariate linear regression of Y on X: equivalent to regressing each Y response variable on X to calculate vectors of fitted values followed by stacking these column vectors side by side into a new matrix Test regression for significance using a permutation test If significant, compute a PCA on matrix of fitted values to get the canonical eigenvalues and eigenvectors We may also compute the residual values of the multiple regressions and do a PCA on these values 4.3.7.1 R functions for RDA BEWARE: many things are called rda in R that have nothing to do with ordination!! rda (vegan package)- this function calculates RDA if a matrix of environmental variables is supplied (if not, it calculates PCA). Two types of syntax are available: matrix syntax - rda (Y, X, W), where Y is the response matrix (species composition),X s the explanatory matrix (environmental factors) and W is the matrix of covariables, or formula syntax (e.g., RDA = rda (Y ~ var1 + factorA + var2*var3 + Condition (var4), data = XW, where var1 is quantitative, factorA is categorical, there is an interaction term between var2 and var3, while var4 is used as covariable and hence partialled out). We should mention that there are several closely related forms of RDA analysis: tb-RDA (transformation-based RDA, Legendre &amp; Gallagher 2001): transform the species data with vegans decostand(), then use the transformed data matrix as input matrix Y in RDA. db-RDA (distance-based RDA, Legendre &amp; Anderson 1999): compute PCoA from a pre-computed dissimilarity matrix D, then use the principal coordinates as input matrix Y in RDA. db-RDA can also be computed directly by function dbrda() in vegan. For a Euclidean matrix, the result is the same as if PCoA had been computed, followed by regular RDA. Function dbrda() can directly handle non-Euclidean D matrices, but beware of the results if the matrix is strongly non-Euclidean and make certain this is what you want. Vegan has three methods of constrained ordination: constrained or canonical correspondence analysis (function cca), redundancy analysis (function rda) and distance-based redundancy analysis (function capscale). All these functions can have a conditioning term that is partialled out. All functions accept similar commands and can be used in the same way. The preferred way is to use formula interface, where the left hand side gives the community data frame and the right hand side lists the constraining variables: 4.3.8 Exercise: Constrained ordination The rda approach is best for continuous data. Lets use the lichen ta in the vegan package again, with the related environmental variables regarding soil chemistry. library(vegan) data(&quot;varespec&quot;) data(&quot;varechem&quot;) Remember that the environmental data can be inspected using commands str which shows the structure of any object in a compact form, and by asking for a summary of a data frame (e.g., str(varechem) or summary(varechem)). We can also plot the data to get a sense of how correlated the various predictors might be (Fig. ??) plot(varechem, gap = 0, panel = panel.smooth, cex.lab = 1.2, lwd = 2, pch = 16, cex = 0.75, col = rgb(0.5, 0.5, 0.7, 0.8)) We will use a formula interface to constrain the ordination. The shortcut ~. indicates that we want to use all the variables in the environmental dataset. We will of course remember to standardize and scale our data. The output shows the eigenvalues for both the constrained and unconstrained ordination axes. In particular, see how the inertia and rank (number of axes) are decomposed in constrained ordination. stvarespec = wisconsin(varespec) scvarechem = as.data.frame(scale(varechem)) constr &lt;- rda(stvarespec ~ ., data = scvarechem) constr Call: rda(formula = stvarespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Baresoil + Humdepth + pH, data = scvarechem) Inertia Proportion Rank Total 0.05084 1.00000 Constrained 0.03612 0.71057 14 Unconstrained 0.01471 0.28943 9 Inertia is variance Eigenvalues for constrained axes: RDA1 RDA2 RDA3 RDA4 RDA5 RDA6 RDA7 RDA8 RDA9 RDA10 0.008120 0.005642 0.004808 0.003525 0.002956 0.002592 0.001953 0.001665 0.001265 0.001129 RDA11 RDA12 RDA13 RDA14 0.000964 0.000627 0.000507 0.000373 Eigenvalues for unconstrained axes: PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 0.004051 0.002184 0.002003 0.001853 0.001247 0.001101 0.001022 0.000722 0.000532 However, it is not recommended to perform a constrained ordination with all the environmental variables you happen to have: adding a large number of constraints means slacker constraint, and eventually you end up with solution similar to unconstrained. Moreover, collinearity in the explanatory variables will cause instability in the estimation of regression coefficients (see Fig ??). Instead, lets use the formula interface to select particular constraining variables ord3 = rda(stvarespec ~ N + K + Al, data = scvarechem) ord3 Call: rda(formula = stvarespec ~ N + K + Al, data = scvarechem) Inertia Proportion Rank Total 0.05084 1.00000 Constrained 0.01171 0.23041 3 Unconstrained 0.03913 0.76959 20 Inertia is variance Eigenvalues for constrained axes: RDA1 RDA2 RDA3 0.006722 0.003232 0.001761 Eigenvalues for unconstrained axes: PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 0.007246 0.004097 0.003737 0.003515 0.003305 0.002474 0.002369 0.001785 (Showing 8 of 20 unconstrained eigenvalues) Examine the output. How did we do? Unsurprisingly, we have not explained as much variance as the full model. In addition, the amount of variance (inertia) explained by the constrained axes is less than the unconstrained axes. Maybe we shouldnt be using this analysis at all. Of course, some of the variables in the full model may or may not be important. We need a significance test to try and sort this out. 4.3.9 Signficance tests for constrained ordination The vegan package provides permutation tests for the significance of constraints. The test mimics standard ANOVA function, and the default test analyses all constraints simultaneously. anova(ord3) Permutation test for rda under reduced model Permutation: free Number of permutations: 999 Model: rda(formula = stvarespec ~ N + K + Al, data = scvarechem) Df Variance F Pr(&gt;F) Model 3 0.011714 1.996 0.001 *** Residual 20 0.039125 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So the results suggest that our predictors expain a signficant amount of variation in the data. We can also perform significance tests for each variable: anova(ord3, by = &quot;term&quot;, permutations = 199) Permutation test for rda under reduced model Terms added sequentially (first to last) Permutation: free Number of permutations: 199 Model: rda(formula = stvarespec ~ N + K + Al, data = scvarechem) Df Variance F Pr(&gt;F) N 1 0.002922 1.4937 0.095 . K 1 0.003120 1.5948 0.045 * Al 1 0.005672 2.8994 0.005 ** Residual 20 0.039125 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The analysis suggests that maybe we havent chosen the best set of predictors. This test is sequential: the terms are analyzed in the order they happen to be in the model, so you may get different results depending on how the model is specified. See if you can detect this, by rearranging your model terms in the call to ordination. NB It is also possible to analyze the significance of marginal effects (Type III effects) using anova(ord, by=mar, permutations=199). Moreover, it is possible to analyse significance of each axis: anova(ord, by=axis, permutations=499) 4.3.10 Forward Selection of explanatory variables You may wonder if you have selected the correct variables! We can use the ordiR2step() function to perform forward selection (and backwards selection or both directions) on a null model. In automatic model building we usually need two extreme models: the smallest and the largest model considered. First we specify a full model with all the environmental variables, and then a minimal model with intercept only, where both are defined using a formula so that terms can be added or removed from the model. Then we allow an automated model selection function to move between these extremes, trying to minimze a metric of model fit, such as \\(R^2\\). We will then use ordiR2step() to find an optimal model based on both permutation and adjusted R2 values. If you switch the trace=FALSE option to trace=TRUE you can watch the function doing its work. mfull &lt;- rda(stvarespec ~ ., data = scvarechem) m0 &lt;- rda(stvarespec ~ 1, data = scvarechem) optm &lt;- ordiR2step(m0, scope = formula(mfull), trace = FALSE) optm$anova R2.adj Df AIC F Pr(&gt;F) + Fe 0.063299 1 -71.156 2.5543 0.004 ** + P 0.103626 1 -71.329 1.9898 0.008 ** + Mn 0.136721 1 -71.402 1.8051 0.022 * &lt;All variables&gt; 0.260356 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Examine the significance test. Does this automatically selected model differ from the full model and your manually selected version? Next check the variance (i.e., inertia). If the variance explained by the constrained model is not much higher than your unconstrained variance, then it may be that the rda is not really needed. Call: rda(formula = stvarespec ~ Fe + P + Mn, data = scvarechem) Inertia Proportion Rank Total 0.05084 1.00000 Constrained 0.01268 0.24932 3 Unconstrained 0.03816 0.75068 20 Inertia is variance Eigenvalues for constrained axes: RDA1 RDA2 RDA3 0.006070 0.003607 0.002998 Eigenvalues for unconstrained axes: PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 0.006435 0.004626 0.003763 0.003301 0.003204 0.002592 0.002003 0.001852 (Showing 8 of 20 unconstrained eigenvalues) Contrary to what one sometimes reads, variables with high VIFs should generally not be manually removed before the application of a procedure of selection of variables. Indeed, two highly correlated variables that are both strong predictors of one or several of the response variables Y may both contribute significantly, in complementary manners, to the linear model of these response variables. A variable selection procedure is the appropriate way of determining if that is the case. However, lets see what happened with collinearity. VIFs can be computed in vegan after RDA or CCA. The algorithm in the vif.cca() function allows users to include factors in the RDA; the function will compute VIF after breaking down each factor into dummy variables. If X contains quantitative variables only, the vif.cca() function produces the same result as the normal calculation for quantitative variables. vif.cca(mfull) N P K Ca Mg S Al Fe Mn 1.884049 6.184742 11.746873 9.825650 9.595203 18.460717 20.331905 8.849137 5.168278 Zn Mo Baresoil Humdepth pH 7.755110 4.575108 2.213443 5.638807 6.910118 vif.cca(optm) Fe P Mn 1.259334 1.431022 1.738208 Some VIF values are above 10 or even 20 in the full model, so that a reduction of the number of explanatory variables is justified. The VIF values in the reduced model are quite low. Finally, lets examine the adjusted \\(R^2\\) of the model round(RsquareAdj(mfull)$adj.r.squared, 2) [1] 0.26 round(RsquareAdj(optm)$adj.r.squared, 2) [1] 0.14 The adjusted \\(R^2\\) for the reduced model is pretty bad, but so is the full model! It will definitely be easier to interpret the reduced model Now! We are all set to produce some ordination plots 4.3.11 Triplots: Graphing a constrained ordination If the constrained ordination is significant, we go on to display the results graphically. All ordination results of vegan can be displayed with a plot command. For a constrained ordination we will want a triplot, which has three different things plotted: the data objects, the response variables and the explanatory variables. For the varespec and varechem data these will be sites, species, and environmental predictors respectively. Plotting an biplot or triplot is not as simple as it sounds, as I mentioned above, one can choose various scaling conventions that will dramatically change the appearance and interpretation of the plot. One of the editors of the vegan package, Gavin Simpson, has more to say about this here. Essentially, there can be no scaling (none), or either the data object (site) or characteristic (species) scores are scaled by eigenvalues, and the other set of scores is left unscaled, or both scores are scaled symmetrically by square root of eigenvalues (symmetric). The general advice is to sse scaling for sites where you want a biplot focussed on the sites/samples and the (dis)similarity between them in terms of the species (or variables), and use scaling for species where you want to best represent the correlations between species (or variables). Ive plotted the site and species types below (Fig. 4.18). Were you able to produce the same ordinations? par(mfrow = c(1, 2)) # Triplots of the parsimonious RDA (scaling=1) Scaling 1 plot(optm, scaling = &quot;sites&quot;, main = &quot;Sites scaling (scaling=1)&quot;, correlation = TRUE) # Triplots of the parsimonious RDA (scaling=2) Scaling 2 plot(optm, scaling = &quot;species&quot;, main = &quot;Species scaling (scaling=2)&quot;) Figure 4.18: Triplots of an RDA ordination on the lichen data with either site or species scaling using a basic plot command Its pretty hard to see what is going on in these basic plots, so I am going to take the time to produce more curated versions. I am going to select which species data I display, and the change the appearance and axis limits somewhat. par(mfrow = c(1, 1)) sp.scores = scores(optm, display = &quot;species&quot;, scaling = 1) sp.scores = sp.scores[sp.scores[, 1] &gt; abs(0.1) | sp.scores[, 2] &gt; abs(0.1), ] plot(optm, scaling = 1, type = &quot;n&quot;, main = &quot;Sites scaling (scaling=1)&quot;, ylim = c(-0.2, 0.2), xlim = c(-0.3, 0.5)) arrows(x0 = 0, y0 = 0, sp.scores[, 1], sp.scores[, 2], length = 0.05) text(sp.scores, row.names(sp.scores), col = 2, cex = 0.6, pos = 3) text(optm, display = &quot;bp&quot;, scaling = 1, cex = 0.8, lwd = 1.5, row.names(scores(optm, display = &quot;bp&quot;)), col = &quot;blue&quot;) text(optm, display = c(&quot;sites&quot;), scaling = 1, cex = 1) Figure 4.19: Triplot of an RDA ordination on the lichen data with site scaling plot(optm, scaling = 2, type = &quot;n&quot;, main = &quot;Species scaling (scaling=2)&quot;, ylim = c(-0.4, 0.4)) arrows(x0 = 0, y0 = 0, sp.scores[, 1], sp.scores[, 2], length = 0.05) text(sp.scores, row.names(sp.scores), col = 2, cex = 0.6, pos = 3) text(optm, display = &quot;bp&quot;, scaling = 2, cex = 0.7, lwd = 1.5, row.names(scores(optm, display = &quot;bp&quot;)), col = &quot;blue&quot;) text(optm, display = c(&quot;sites&quot;), scaling = 2, cex = 1) Figure 4.20: Triplot of an RDA ordination on the lichen data with species scaling Okay, lets figure out how to interpret these plots. In the site scaling option (Fig 4.19), dissimilarity distances between data objects are preserved. So that sites which are closer together on the ordination are more similar. In addition, the projection of a data object (site) onto the line of a response variable (species) at right angle approximates the position of the corresponding object along the corresponding variable. Angles between lines of response variables (species) and lines of explanatory variables are a two-dimensional approximation of correlations. But other angles between lines are meaningless (e.g., angles between response vectors dont mean anything). In the species scaling plot (Fig 4.20), distances are now meaningless, and as a result the length of vectors are not important. However, the cosine of the angle between lines of the response variables or of explanatory variables is approximately equal to the correlation between the corresponding variables. So, looking at our first plot, sites 2, 28 and perhaps 3 seem quite different from others. For site 3, this is perhaps because of the abundance of Cladrang, which might be responding to iron levels (Fe). Site 28 might differ because of higher abundances of Hylosple, which may be related to Mn. Notice how we can see on the species scaling plot the strong correlation between P and species Dicrsp, which is somewhat different on the site scaling plot. 4.3.11.1 What next? In this short introduction we have not demonstrated how to completed an interpret other forms of constrained ordination such as Canonical Correspondence Analysis, partial RDA (which would allow you to control for well-known linear effects), or analysis designed for qualiticative variables, such as Multiple Correspondence Analysis (MCA). There are functions in the vegan and FactoMineR packages to do many of these things. "],["classification.html", "5 Classification 5.1 Logistic regression as a binary classifier 5.2 Tree-based methods for classification", " 5 Classification Classification is the task of assigning data objects, such as sites, species or images to predetermined classes. Determining what class of data object you have is a question that usually turns on multiple predictors. For example, to classify leaf images to different species, predictors such as size, shape and colour may be used. If you have satellite data, you may need to classify the different pixels of the image as agricultural, forest, or urban. So for classifcation tasks our response variable, y, is qualitative or categorical (e.g. gender, species, land classification). There are many methods that can be employed for classification tasks ranging from logistic regression to random forest techniques. While some of these methods are classic multivariate methods, others, like random forest classifiers, are machine learning tasks. Machine learning is an application of artificial intelligence. The computer algorithm finds a solution to the classifcation problem without being explicitly programmed to do so. 5.1 Logistic regression as a binary classifier One of the simplest classification methods, and one that does not involve machine learning is logistic regression. Lets take a common example. A non-native species has been introduced to a region, and we would like to know what percentage of the region would be suitable habitat, in order to get an idea of risks of impact on the native ecosystem. We think that average temperature controls habitat suitability, and we have presence/absence data for the species accross a range of different sites. Could we use simple regression to answer the question of whether a given area is suitable habitat? If we indicate absence as 0, and presence as 1, we can regress species occurance again average annual temperatures. Figure 5.1: Species presence/absence and mean annual temperature with linear regression As we can see in Fig. @ref{fig:log}, the linear regression does not make a lot of sense for a response variable that is restricted to the values of 0 and 1. The regression line \\(\\beta_0+\\beta_1x\\) can take on any value between negative and positive infinity, but we dont know how to interpret values greater than 1 or less than zero. The regression line almost always predicts wrong value for y in classification problems. Instead of trying to predict y, we can try to predict p(y = 1), i.e., the probability that the species will be found in the area. For invasive species, this probability if often interpreted as habitat suitability for the species. We need a function that gives outputs between 0 and 1: logistic regression is one solution. In this model, probability of y for a given value of x, p(x) is given as: \\[\\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}.\\] Rearranging, we have: \\[\\frac{p(x)}{1-p(x)}=e^{\\beta_0+\\beta_1x}.\\] Taking the natural logarithm, we can see that the logistic regression is linear in x: \\[\\log{\\frac{p(x)}{1-p(x)}}=\\beta_0+\\beta_1x,\\] where the lefthand side is called the log-odds or logit. The logistic function will always produce an S-shaped curve, so regardless of the value of x, we will obtain a sensible prediction. Lets apply this model to our non-native species data using the glm() function. We use the glm() function to perform logistic regression by passing in the family=binomial argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function. glm.fit.sp = glm(y ~ x, family = binomial) glm.probs &lt;- predict(glm.fit.sp, type = &quot;response&quot;) Figure 5.2: Species presence/absence and mean annual temperature with logistic regression 5.1.1 Interpreting the logistic regression Lets take a closer look at the output for our logistic regression Fig. @ref{fig:logfit}. First off, we need to know if the intercept, \\(\\beta_0\\), and slope, \\(\\beta_1\\), are significantly different from zero. A z distribution is used for this test, and we find that both the intercept and slope are signficiantly different from zero. @ref(tab=logcoef). Our p-values are very small, and \\(\\beta_1\\) is positive, so we are sure that as temperature increases, the probability of habitat suitability will increase as well. Table 5.1: Logistic regression coefficient estimates and hypothesis tests from species occurance data Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -10.72 1.57 -6.84 0 x 0.75 0.12 6.52 0 The estimated intercept is typically not of interest. Its main purpose is to adjust the average fitted probability to the proportion of ones in the data. You may be confused by the slope value. Interpreting what \\(\\beta_1\\) means is not very easy with logistic regression, simply because we are predicting p(y) and not y. If \\(\\beta_1\\) = 0, this means there is no relationship between p(y) &amp; x. If \\(\\beta_1\\) &gt; 0, this means that when y gets larger so does the probability that y = 1. If \\(\\beta_1\\) &lt; 0, this means that when x gets larger, the probability that y = 1 gets smaller. For example, suppose a region has an average annual temperature of 12°C. The probability that the habitat is suitable p(x) for our invasvie species is then: \\[p(y)=\\frac{e^{paste \\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}.\\] Sustituting in our fitted values from the logisitic regression, we have: \\[p(y)=\\frac{e^{-10.72 + 0.75 (12)}}{1+e^{-10.72 + 0.75(12)}}=0.15.\\] The value changes with temperature, so for an annual average temperature of 16°C that probability is 0.78, and so on. Of course, the data used to fit this relationship is zeros (absence) and ones (present). How well does our model do at predicting species occurence? In order to answer this question we need to decide on a threshold value for the probability prediction. For example, an easy one is 50%. If the model predicts a probability of greater than 0.5, then we score that as presence, while if the predicted value is equal to or less than 0.5, well score as an absence. Now we have model predictions in terms of predicted absences and presences (i.e., zeros and ones), and can compare directly to our data. When binary classifications are made, (after converting the probabilities using threshold), there can be four cases for a certain observation: The response actually negative, the model predicts it to be negative. This is known as true negative (TN). In our case, the invasive species is not present at the location, and the model predicts that it should not be present. The response actually negative, but the model predicts it to be positive (i.e., false positive, FP). The response actually positive, and the model predicts it to be positive (i.e., true positive TP). The response actually positive, but the model predicts it to be negative (i.e., false negative FN). We can summarize this information in a confusion matrix which records the number of times the model correctly predicted the data, and the number of times the model makes incorrect predictions. # determine model occurance predictions based on threshold # value logocc &lt;- ifelse(glm.probs &gt; 0.5, 1, 0) # Calculate a confusion matrix ctab = table(logocc, y) dimnames(ctab) &lt;- list(Actual = c(&quot;absence(0)&quot;, &quot;presence(1)&quot;), Predicted = c(&quot;absence(0)&quot;, &quot;presence(1)&quot;)) ctab Predicted Actual absence(0) presence(1) absence(0) 125 23 presence(1) 20 32 Elements on the diagonal from left to right are correct classifications, while off-diagonal elements are missclassifications (i.e., predictions where the predict code of 0 or 1 does not match the actual code of 0 or 1). So in this case, we have 125 true negatives (TN), 23 false positives (FP), 20 false negatives (FN), and 32 true positives (TP). We can quantify these errors in a number of ways. The misclassification rate, or error rate is the most common metric used to quantify the performance of a binary classifier. This is the probability that the classifier makes a wrong prediction (given as \\(\\frac{FN+FN}{TN+FN+TP+FP}\\). Overall 43 observations have been missclassified, so we have a total error rate of 22%. However, 38% of the presence data has been misclassified, compared to 16% of the absence data. So with respect to the predicting the potential presence of an invasive species, were not doing much better than random chance. The terms sensitivity and specificity characterize the performance of classifier for these specific types of errors. In this case, the sensitivity is the percentage of occupancy locations that are correctly identified (true positives), which is 62% in this case or one minus the misclassification rate of positives (or 1-0.38), or calculated as TP/(TP+FN). Specificity is the percentage of non-occupancy sites that are correctly identified (true negatives). We can calcuate this as one minus the misclassification of true negatives from the values above (1  0.16) = 0.84, or from the formula TN/(TN+FP). We can of course, change the decision threshold to see if we can get a better outcome. Lets try 0.65 instead of 0.5. Predicted Actual 0 1 0 135 40 1 10 15 This higher threshold gives us an error rate of 25%, sensitivity of 60% and specificity of 77%, so not much improvement. We can also examine the performance of the model accross a range of thresholds. We often see this approach in species distribution modelling, where specificity (% of true positives) is plotted again 1-sensitivity (% of false positives) for threshold values from 0 to 1. This graph is called the Receiver Operator Curve (ROC), and the Area Under that Curve (AUC) is calculated. This is a fairly standard evaluation for binary classifiers, and there are a number of R packages that will complete this analysis for you. If the model is not performing better than random chance, the expected ROC curve is simply the y=x line. Where the model can perfectly separate the two classes, the ROC curve consists of a vertical line (x=0) and a horizontal line (y=1). For real and simulated data, usually the ROC stays in between these two extreme scenarios. Lets try with our simulated data. library(ROCit) ROCit_obj &lt;- rocit(score = glm.fit.sp$fitted.values, class = y) plot(ROCit_obj) summary(ROCit_obj) Method used: empirical Number of positive(s): 55 Number of negative(s): 145 Area under curve: 0.8639 Overall, we have an area under the ROC curve (ref?)(fig:R) of 0.86, which is not bad, given the maximum value is one. The optimal threshold value is given by the Youden index as 0.68. The Youden index maximizes the difference between sensitivity and 1-specificity and is defined as sensitivity+specificity-1. Lets try this threshold directly. Predicted Actual 0 1 0 136 42 1 9 13 We get an error rate of 26%, senstivity of 59% and specificity of 76%. Overall, an okay, but not fantastic model, at the best performing threshold. 5.1.2 Cross-validation So far, weve evaluated model performance with the data that we used to train the model, but the point of classification tools is to be able to use them on data where we dont already know the answer. In that sense, we are uninterested in in model performance in training data, what we really want to do is to test the model on data that was not used in model fitting. For example, we dont really care how well our method predicts habitat suitability where the invasive species is already located! What we need to know is how well it predicts the habitat suitability of locations where the species has not yet invaded. The model performance on this testing data will give us a better idea of the errors we might expect when we apply our classifer to novel data. While we might naively expect that model performance on the training data will be the same on the testing data, in practice the errors are usually larger, sometimes much larger. In more complex models, this error rate is often the result of overfitting the training data, so that pattern which is just noise is included in the model fit. Consequently, the model is not well fit to data with different sources of noise. Often course, in biology data is almost always limited! While you might have an extra independent dataset kicking around waiting to be used for model testing, if you dont, you can divide your single dataset into training and testing sets. One easy way to do this is just using random selection. Lets try on our data. Well divide a dataframe with our data into two parts using the sample() function, fit our logistic model on the training data, and evaluate its performance on the testing data. # simulate some temperature and occupancy data using a # random number generator x = rnorm(200, 12, 2.5) y = ifelse(x &gt; 12, 1, 0) e = rnorm(200, 0, 0.1) # adding some randomness to simulated occupancy data y = ifelse(y + e &gt;= 1, 1, 0) # create a dataframe with our temperature and occupancy # data ivsp = data.frame(temp = x, occ = y) # randomly sample 75% of the data (by generating random # numbers based on the number of rows) samp = sample(nrow(ivsp), nrow(ivsp) * 0.75, replace = FALSE) # divide into training and testing sets train &lt;- ivsp[samp, ] test &lt;- ivsp[-samp, ] # fit the logistic model on the training data log.fit.inv = glm(occ ~ temp, family = binomial, data = train) # test the logistic model on the testing data log.predict &lt;- predict(log.fit.inv, newdata = test, type = &quot;response&quot;) # determine predicted occupancy based on threshold value of # 0.5 pred.occ &lt;- ifelse(log.predict &gt; 0.5, 1, 0) # Calculate a confusion matrix ctab = table(pred.occ, test$occ) dimnames(ctab) &lt;- list(Actual = c(0, 1), Predicted = c(0, 1)) ctab Predicted Actual 0 1 0 32 16 1 2 0 # Calculate error rate, sensitivity and specificity err = round((ctab[1, 2] + ctab[2, 1])/sum(ctab), 2) sens = round(ctab[2, 2]/(ctab[2, 1] + ctab[2, 2]), 2) spec = round(1 - ctab[1, 2]/(ctab[1, 1] + ctab[1, 2]), 2) print(paste0(&quot;error rate=&quot;, err, &quot;; sensitivity=&quot;, sens, &quot;; specificity=&quot;, spec)) [1] &quot;error rate=0.36; sensitivity=0; specificity=0.67&quot; You may want to verify for yourself that the sample function randomly selects rows out of a dataframe Youll notice that by dividing the data up this way we have only 50 observations in our testing data. We can of course use different percentages to divide up our one dataset into training and testing sets, but models tend to have poorer performance when trained on fewer observations. On the other hand, the small testing dataset may tend to overestimate the test error rate for the model fit, as compared to error rates obtained on a larger amount of data. And what about the effects of that random sampling? If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the error rate on testng data each time, because different observations will be randomly included in the dataset each time. Sometimes the differences between error rates on different testing datasets can be rather large. For example, by random selection an observation which is a huge outlier could be included in one small testing dataset, but not in another, resulting is very different error rates. To guard against undue influence of single observations in our small test dataset we could do the routine of randomly sampling to obtain testing and training sets several times, and look at the average of our testing data performance. 5.1.2.1 k-Fold Cross-Validation One way of implementing this type of resampling scheme is k-fold cross-validation. With this method, we randomly divide the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a testing set, and the model is fit on the remaining k  1 folds. This procedure is repeated k times, and each time, a different group of observations is treated as the testing set. We then average the error rates from each test fold. We can even repeat the entire procedure several times in repeated k-fold cross-validation. # example code for iterated cross-validation set up reps = 10 nfolds = 5 for (j in 1:reps) { # generate array containing fold-number for each sample # (row) foldsset &lt;- rep_len(1:nfolds, nrow(data)) folds &lt;- sample(foldsset, nrow(data)) # actual cross validation for (k in 1:nfolds) { # split of the data fold &lt;- which(folds == k) data.train &lt;- data[-fold, ] data.test &lt;- data[fold, ] # train and test your model with data.train and # data.test } } 5.1.3 Multiple logistic regression If we have more than one predictor, we can fit multiple logistic just like regular regression, as: \\[p(y)=\\frac{e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}{1+e^{\\beta_0+\\beta_1x_1+...+\\beta_nx_n}}\\] and the \\(x_n\\) predictors can be both qualitiative or quantiative. For example we could add a land classification to our invasive species habitat suitability model so that we have both temperature (\\(x_1\\)) and urban and rural (\\(x_2\\)) land types. glm.fit.sp2 = glm(y ~ x + as.factor(x2), family = binomial) In this case, our model now has two responses, one for land categorized as urban, and one for land categorized as rural (Fig (ref?)(fig:multilog)). Table 5.2: Logistic regression coefficient estimates and hypothesis tests from species occurance data with temperature and land category as predictors Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -7.69 1.26 -6.10 0.00 x 0.49 0.09 5.24 0.00 as.factor(x2)urban 0.89 0.43 2.09 0.04 Figure 5.3: Species occurance vs temperature and land classification as urban or rural both predictors are significantly different from zero (Table (ref?)(tab:logcoefs), and the values tell is that if the land is categorized as urban, we must increase our probability estimate upwards from that of a rural area as: \\[p(y)=\\frac{e^{-7.69 + 0.49 x_1+0.89 (1)}}{1+e^{-7.69 + 0.49x_1+0.89 (1)}}\\] Logistic regression can be extended to multiple classification problems in different ways, but in practice these methods tend not to be used all that often. Instead other technqiues such as discriminant analysis and random forest tend to be used for multiple-class classification problems. 5.1.4 Exercise: Logistic regression as a binary classifier Try to use logistic regression on your own, with a pre-existing dataset in the MASS package. Start by installing the MASS package, load the library and load the data. Well be using the Pima.tr data in the MASS package, which describes risk factors for diabetes. Type help(Pima.tr) or ?Pima.tr to get a description of these data. Youll notice that the type variable is our classifier and determines whether the patient has diabetes or not. library(MASS) library(caret) data(Pima.tr) str(Pima.tr) &#39;data.frame&#39;: 200 obs. of 8 variables: $ npreg: int 5 7 5 0 0 5 3 1 3 2 ... $ glu : int 86 195 77 165 107 97 83 193 142 128 ... $ bp : int 68 70 82 76 60 76 58 50 80 78 ... $ skin : int 28 33 41 43 25 27 31 16 15 37 ... $ bmi : num 30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ... $ ped : num 0.364 0.163 0.156 0.259 0.133 ... $ age : int 24 55 35 26 23 52 25 24 63 31 ... $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ... Next construct a logistic regression, to use as a classifier. Examine your output to determine if the regression is significant. We can use the . in the formula argument to mean that we use all the remaining variables in the dataset as predictors. # run logistic regression Pima.log &lt;- glm(type ~ ., family = binomial, data = Pima.tr) summary(Pima.log) Call: glm(formula = type ~ ., family = binomial, data = Pima.tr) Deviance Residuals: Min 1Q Median 3Q Max -1.9830 -0.6773 -0.3681 0.6439 2.3154 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -9.773062 1.770386 -5.520 3.38e-08 *** npreg 0.103183 0.064694 1.595 0.11073 glu 0.032117 0.006787 4.732 2.22e-06 *** bp -0.004768 0.018541 -0.257 0.79707 skin -0.001917 0.022500 -0.085 0.93211 bmi 0.083624 0.042827 1.953 0.05087 . ped 1.820410 0.665514 2.735 0.00623 ** age 0.041184 0.022091 1.864 0.06228 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 256.41 on 199 degrees of freedom Residual deviance: 178.39 on 192 degrees of freedom AIC: 194.39 Number of Fisher Scoring iterations: 5 Not all the predictors have slopes significantly different from zero, but certainly glucose looks important. If you wanted you could simplify this model, and compare the different models with AIC, just like a standard linear regression. Next, well use some testing data, to test our classifier. The Prima.te has already been created for you in the MASS package. Use the predict function to get the predicted probabilities, and a threshold value to get classifications. Then construct a confusion matrix to determine how well our predictor did. testPima &lt;- predict(Pima.log, newdata = Pima.te, type = &quot;response&quot;) testPima = as.factor(ifelse(testPima &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)) Predicted Actual No Yes No 200 23 Yes 43 66 Notice there there have been some missclassifications at 50%, and that the accuracy is only about 0.8. See if another decision boundary (e.g., 75%) does any better. There is a nice package in R, caret, that is a good wrapper for these kinds of tasks, and which is really great for machine learning. We can use it to generate both our confusion matrix, and other statistical info about our classifier library(caret) confusionMatrix(Pima.te$type, testPima) Confusion Matrix and Statistics Reference Prediction No Yes No 200 23 Yes 43 66 Accuracy : 0.8012 95% CI : (0.7542, 0.8428) No Information Rate : 0.7319 P-Value [Acc &gt; NIR] : 0.002095 Kappa : 0.5271 Mcnemar&#39;s Test P-Value : 0.019349 Sensitivity : 0.8230 Specificity : 0.7416 Pos Pred Value : 0.8969 Neg Pred Value : 0.6055 Prevalence : 0.7319 Detection Rate : 0.6024 Detection Prevalence : 0.6717 Balanced Accuracy : 0.7823 &#39;Positive&#39; Class : No Perhaps the most informative of these stats is the No Information Rate which tests whether our classifier does better than random assignment. We see that our error rate is signficantly greater than this no information rate. Also, the Balanced Accuracy Statistic gives an accuracy value that weights both majority and minority classes evenly, if you have unbalanced class membership in your data. 5.2 Tree-based methods for classification Tree-based methods for classification involve dividing up regions defined by the predictor variables. For example, simple species identfication trees use this approach. You are looking at a tree: does it have needle- or scale-shaped leaves? Or are the leaves wide? The leaf characteristic predictor is used to divide up the classification possibilities into conifers and deciduous trees (not without errors!). So we repeatedly split the response data into two groups that are as homogeneous as possible. The split is determined by the single predictor that best discriminates among the data. The binary splits continue to partition the data into smaller and smaller groups, or nodes, until the groups are no longer homogeneous. This effort produces a single tree where the binary splits form the branches and the final groups compose the terminal nodes, or leaves. If this type of method is applied with a continuous response variable instead it is called a regression tree, if the response is categorical, it is called a classification tree. Often we refer to both technqiues at the same time as classification and regression trees (CART). When used as a regression response predictor, this technqiue differs from standard regression approaches which are global models where the predictive formula is supposed to hold in the entire data space. Instead trees try to partition the data space into small enough parts where we can apply a simple different model on each part. Lets try a simple example on the iris data. Well first split the data into a training and testing set, and then run our classfication tree algorithm in the rpart package. library(rpart) alpha &lt;- 0.7 # percentage of training set inTrain &lt;- sample(1:nrow(iris), alpha * nrow(iris)) train.set &lt;- iris[inTrain, ] test.set &lt;- iris[-inTrain, ] mytree &lt;- rpart(Species ~ ., data = train.set, method = &quot;class&quot;) par(mar = c(5, 3, 3, 3)) plot(mytree) text(mytree) Figure 5.4: A simple plot of a tree classifier for the iris data We can see that petal length is used to distinguish species I.setosa from the other two species, and then petal width classifies into I. versicolor or I. virginia. The model of course includes more information than this regarding the number of observations aggregating to each branch of the tree etc. More detailed information can be obtained from summary(mytree), or just typing mytree. A nicer plot, with more details can also be obtained with rpart.plot library. library(rpart.plot) rpart.plot(mytree) Figure 5.5: A nicer plot of a tree classifier for the iris data made with the rpart.plot package This plot (Fig. (ref?)(fig:fancytree)), in addition to the factor that splits each branch, also tells us the percentage of the data in each class, and the percentage that travels down each branch in each class. Starting at the top, each species makes up roughly a third of the data, after the petal length branch, travelling down the petal length greater than or equal to 2.5, all I.setosa observations all on the other side of the split, and we are left with data divided evenly between the I.versicolor and I. viriginica. The petal length &lt; 4.8 branch separates out these two species, with some error in classification. The algorithm determines which variable to split based on impurity, or how similar points are within a group. If all data points are identical, then impurity is zero. Impurity increases as points become more dissimilar. Impurity is calculated differently for different kinds of trees. For classification trees: the Gini index, which reflects rhe proportion of responses in each level of a categorical variable is often used. The Gini index is calculated as: \\(Gini=1-\\sum p_i\\), where \\(p_i\\) is the proportion of observations in each class. The Gini index is small when many observations fall into a single category, so the split is made at the single variable which minimizes the Gini index. Some classifiers use the Shannon-Weiner index instead, which has similar properties. Using this tree classifer, we can make predictions for our testing data, and get a confusion matrix pred &lt;- predict(mytree, test.set, type = &quot;class&quot;) confusionMatrix(pred, test.set$Species) Confusion Matrix and Statistics Reference Prediction setosa versicolor virginica setosa 14 0 0 versicolor 0 12 1 virginica 0 3 15 Overall Statistics Accuracy : 0.9111 95% CI : (0.7878, 0.9752) No Information Rate : 0.3556 P-Value [Acc &gt; NIR] : 1.048e-14 Kappa : 0.8663 Mcnemar&#39;s Test P-Value : NA Statistics by Class: Class: setosa Class: versicolor Class: virginica Sensitivity 1.0000 0.8000 0.9375 Specificity 1.0000 0.9667 0.8966 Pos Pred Value 1.0000 0.9231 0.8333 Neg Pred Value 1.0000 0.9062 0.9630 Prevalence 0.3111 0.3333 0.3556 Detection Rate 0.3111 0.2667 0.3333 Detection Prevalence 0.3111 0.2889 0.4000 Balanced Accuracy 1.0000 0.8833 0.9170 Our accuracy is pretty good for all species. 5.2.1 Tree pruning So how does the model decide when to stop? Presumably you could continue to build out the tree until every single observation is a node. Another way to phrase this question is: how do you prevent the model from overfitting the data? The answer is: pruning (the best part about this classification method is the metaphorical gardening language!). Pruning is the act of overgrowing the tree and then cutting it back. Ultimately pruning should yield a tree that optimizes the trade-off between complexity and predictive ability. Pruning begins by creating a nested series of trees of increasing number of branches, from 0 (no splits) to however many can be reasonably obtained from the data. For each number of branches, an optimal tree can be recovered, i.e., one that minimizes the overall misclassification rate. To select the tree of optimal size we use cross-validation. For a given tree size, cross-validation divides the data into equal portions, removes one portion from the data, builds a tree using the remaining portion, and then calculates the error between the observed data and the predictions. This procedure is repeated for each of the remaining portions and then the overall error is summed across all subsets of the data. This is done for each of the nested trees. The tree of optimal size is then determined based on the smallest tree that is with in 1 standard error of the minimum error observed across all trees. Even with pruning, a single CART is likely to overfit the data, particularly when there are many, many predictors, and thus is not very good for prediction. One way to get around this is to build a bunch of different, non-nested, trees on subsets of the data, and then average accross them. Because any given tree is constructed with only a portion of the data, the likelihood of overfitting is drastically reduced. Moreover, averaging across many trees reduces the impact of anomalous results from a single tree. This is the idea of ensemble learning, or combining many weak learners (individual trees) to produce one strong learner (the ensemble). 5.2.2 Random Forests One type of ensemble decision tree is a random forest. Random forests use a bootstrapped sample of the data, and only a portion of the predictors to construct each tree. This procedure ensures that each individual tree is independent from the others, making it a much more accurate method than some other ensemble learning techniques (e.g., bagging). As well, since both the data and the predictors are subsampled, these models can be fit to more predictors than there are observations. This seems a little counterintuitive, but can be a real benefit for ecological data which typically suffers from low replication. Lets try an ensemble decision tree on the iris data. We use the randomForest package on the training data. Note that we do not have to split our data into training and testing now, the random Forest package is already doing this sort of thing for us. library(randomForest) RF.model = randomForest(Species ~ ., data = iris) RF.model Call: randomForest(formula = Species ~ ., data = iris) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 2 OOB estimate of error rate: 4% Confusion matrix: setosa versicolor virginica class.error setosa 50 0 0 0.00 versicolor 0 47 3 0.06 virginica 0 3 47 0.06 Our classification is pretty good with a misclassification rate on the Out Of Bag data of only 4% (remember this is the equivalent to the test data from a cross-validation), and errors for invidual species from 0 to 0.06%. We might like to look at what the model is doing but unlike a single CART, random forests do not produce a single visual, since of course the predictions are averaged across many hundreds or thousands of trees. When building random forests, there are three tuning parameters of interest: node size, number of trees, and number of predictors sampled at each split. Careful tuning of these parameters can prevent extended computations with little gain in error reduction. For example, the plot below (ref?)(fig:RFOOB) shows how the overall OOB error rate, and the error rate for each of the three species, changes with the size of the forest (the number of trees). Obviously with few trees the error rate is high, but as more trees are added you can see the error rate decrease and eventually flatten out. in the above plot, we could easily reduce the number of trees down to 300 and experience relatively little loss in predictive ability. This is easy to do: update(RF.model, ntree = 300) Call: randomForest(formula = Species ~ ., data = iris, ntree = 300) Type of random forest: classification Number of trees: 300 No. of variables tried at each split: 2 OOB estimate of error rate: 4.67% Confusion matrix: setosa versicolor virginica class.error setosa 50 0 0 0.00 versicolor 0 46 4 0.08 virginica 0 3 47 0.06 So we see an increase in our error rate, but not much. Despite not yielding a single visualizable tree, one of the major advantages of random forests is that they can provide a measure of relative importance. By ranking predictors based on how much they influence the response, random forests may be a useful tool for selecting predictors before trying another framework, such as CART. Importance can be obtained using the importance function, and plotted using the varImpPlotfunction: varImpPlot(RF.model) Figure (ref?)(fig:VIP) reports the mean decrease in the Gini Index for each predictor. If you recall, the Gini index is a measure of impurity for categorical data. For each tree, each predictor in the OOB sample is randomly permuted (aka, shuffled around) and passed to the tree to obtain the error rate. The error rate from the unpermuted OOB is then subtracted from the error rate on the permuted OOB data, and averaged across all trees. When this value is large, it implies that a variable had a strong relationship with the response. That is, the model got much worse at predicting the data when that variable was permuted. As we already knew, Petal.Length and Petal.Width are the two most important variables. One other useful aspect of random forests is getting a sense of the partial effect of each predictor given the other predictors in the model. This has analogues to partial correlation plots in linear models. We can construct a partial effecs response by holding each value of the predictor of interest constant (while allowing all other predictors to vary at their original values),passing it through the random forest, and predicting the responses. The average of the predicted responses are plotted against each value of the predictor of interest (the ones that were held constant) to see how the effect of that predictor changes based on its value. This exercise can be repeated for all other predictors to gain a sense of their partial effects. The function to calculate partial effects is partialPlot. Lets look at the effect of Petal.Length: partialPlot(RF.model, iris, &quot;Petal.Length&quot;, main = &quot;&quot;, ylab = &quot;log odds&quot;) Figure 5.6: Partial effect of petal length in the random forest model of the iris data The y-axis is a bit tricky to interpret (Fig. (ref?)(fig:PEplot). Since we are dealing with classification trees, y on the logit scale, and is the probability of success. In this case, the partial plot has defaulted to the first class, which is I. setosa. The plot says that there is a high chance of successfully predicting this species from Petal.Length when Petal.Length is less than around 2.5 cm, after which point the chance of successful prediction drops off precipitously. This is actually quite reassuring as this is the first split identified way back in the very first CART (where the split was &lt; 2.45 cm). Missing data Its worth noting that the default behavior of randomForest is to refuse to fit trees with missing predictors. You can, however, specify a few alternative arguments: the first is na.action = na.omit, which removes the rows with missing values outright. Another option is to use na.action = na.roughfix, which replaces missing values with the median (for continuous variables) or the most frequent level (for categorical variables). Missing responses are harder: you can either remove that row, or use the function rfImpute to impute values. The imputed values are the average of the non-missing observations, weighted by their proximity to non-missing observations (based on how often they fall in terminal nodes with those observations). rfImpute tends to give optimistic estimates of the OOB error. What else? Weve just provided a small sampler of classification methods here that will get you started on your way. But other methods such as Linear Discriminent Analysis (LDA(, Artificial Neural Network (ANN), K-nearest neighbors, and Support Vector Machines (SVM) are also useful for building classification models. "],["optimization.html", "6 Optimization 6.1 Introduction 6.2 Fundamentals of Optimization 6.3 Regression 6.4 Iterative Optimization Algorithms 6.5 Calibration of Dynamic Models 6.6 Uncertainty Analysis and Bayesian Calibration 6.7 References", " 6 Optimization 6.1 Introduction To improve is to make better; to optimize is to make best. Optimization is the act of identifying the extreme (cheapest, tallest, fastest) over a collection of possibilities. Optimization over design space (also called decision space) is a critical feature of many engineering tasks, and has a role in most areas of applied science, including biology. Examples include optimal manipulation of biological systems (e.g. optimal harvesting or optimal drug dosing) or optimal design of biological systems (e.g. robust synthetic genetic circuits). A complementary task is optimal experimental design, which aims to identify the best experiment from a collection of possibilities. Model calibration, to be discussed further below, provides another example; here we seek the best fit version of a model from a collection of possible options. Beyond its use in design, optimization is also used to investigate natural systems (i.e. in pure science), in cases where nature presents us with an optimal version of a given phenomenon. For example, the principle of energy minimization justifies a wide variety of phenomena, from the shapes of soap bubbles to the configuration of proteins. Darwinian evolution provides another mechanism for optimization. Assuming evolution has arrived at optimal designs, we can apply optimization to understand a variety of biological phenomena, from metabolic flux distribution, to brain wiring, to foraging strategies. This module introduces optimization methods in R. Applications and illustrations are drawn from a range of biological domains. Although simple optimization tasks, such as those addressed by introductory calculus, can be accomplished with paper-and-pencil calculations, most optimization tasks of interest in biology demand computational techniques. 6.2 Fundamentals of Optimization Figure 1 illustrates some basics terminology associated with optimization. The graph of a function \\(f\\) of a single variable \\(x\\) is shown, defined over a domain \\([a,b]\\). In the context of optimization, we can think of each \\(x\\)-value in the interval \\([a,b]\\) as one possible scenario (e.g. enzyme activity, foraging rate, etc.). The function \\(f\\) maps those scenarios to some objective (e.g. metabolic flux, fitness) to be optimized (either maximized or minimized). The global extrema (i.e. maximum and minimum) represent the goals of optimization. Figure 6.1: Figure 1: Extreme Values More generally, we define local extrema (maxima and minima) as cases that appear to be optimal if we restrict attention only to nearby possibilities (i.e. \\(x\\)-values). There is an extensive theory of optimization methods; unfortunately most of these are dedicated to identifying local optima. These approaches cannot directly identify a global optimum  instead they identify candidate global optima, which then can be compared to identify the best result. (A convenient special case occurs for problems where every local optimum is a global optimum; these are called convex optimization problems. Unfortunately, they typically occur only as special cases when addressing biological phenomena.) 6.2.1 Fermats Theorem To illustrate these fundamentals, consider the following two academic examples that rely on basic calculus, specifically on Fermats Theorem, which states that local extrema occur at points where the tangent line to a functions graph is horizontal, i.e. at points where the derivative (i.e. the slope of the tangent) is zero (Figure 2). Figure 6.2: Figure 2: Fermats Theorem: local extrema occur at points where the tangent line is horizontal. Some local extrema are also global extrema. 6.2.1.1 Example 1 Task: Identify the value of \\(x\\) for which the function \\(f(x) = x^2+3x-2\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 2x+3\\). To find the point(s) where the derivative is zero, we solve: \\[f&#39;(x) = 0 \\ \\ \\Leftrightarrow \\ \\ 2x+3 = 0 \\ \\ \\Leftrightarrow \\ \\ x = -\\frac{3}{2} = -1.5\\] As shown in Figure 3, the single local minimum is the global miminum in this case, so \\(x=-1.5\\) is the desired solution. Figure 6.3: Figure 3: Graph of \\(f(x) = x^2+3x-2\\) (Example 1) 6.2.1.2 Example 2 Task: Identify the value of \\(x\\) for which the function \\(f(x) = 3x^4-4x^3-54x^2+108x\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 12x^3-12x^2-108x+108 = 12(x-3)(x-1)(x+3)\\). To find the point(s) where the derivative is zero, we solve: \\[f&#39;(x) = 0 \\ \\ \\Leftrightarrow \\ \\ x = 3, 1, -3\\] As shown in Figure 4, two of these points are local minima (and one is a local maximum). OF the two local minima, \\(x=-3\\) is where the global minimum occurs. Figure 6.4: Figure 4: Graph of \\(f(x) = 3x^4-4x^3-54x^2+108x\\) (Example 2)$ 6.3 Regression 6.3.1 Linear Regression As mentioned above, finding the best fit from a family of models is a common optimization task in science. The simplest example of this task is linear regression: the task of determining a line of best fit through a given set of points. To illustrate, consider the dataset of \\((x,y)\\) pairs shown in Figure 5 below, which we can label as \\((x_1, y_1)\\), \\((x_2, y_2)\\), \\(\\ldots\\), \\((x_N, y_N)\\). Several lines are displayed. The optimization task is to identify the best line: the one that best captures the trend in the data. To specify this task mathematically, we need to decide on a measure of quality of fit. We start by recognizing that the line will (typically) fail to pass through most of the points in the dataset. Thus, at each point \\(x_i\\) we can define an error, which is the difference between the observed \\(y\\)-value and the \\(y\\)-value on the line, which well call the prediction. If we call the line \\(y=mx+b\\), then the error at \\(x_i\\) can be defined as \\(e_i = y_i-(mx_i+b)\\). We then need to combine these errors into a single quality of fit measure. This is typically done by squaring the errors and adding them together. (Squaring ensures that both under- and over-estimations contribute equally.) We define the sum of squared errors (SSE) as : \\[\\mbox{SSE:} \\ \\ \\ e_1^2+e_2^2+ \\cdots e_N^2 \\ \\ = \\ \\ (y_1-(mx_1+b))^2 + (y_2-(mx_2+b))^2 + \\cdots +(y_N-(mx_N+b))^2. \\] Figure 6.5: Figure 5: finding a line of best fit We can now pose the model-fitting task as an optimization problem. For each line (that is, each assignment of numerical values to \\(m\\) and \\(b\\)), we associate a corresponding SSE. We seek the values of \\(m\\) and \\(b\\) for which the SSE is a global minimum. 6.3.1.1 Example 3 Consider a simplified version of linear regression, in which we know that our model (line) should pass through the origin (0,0). That is, instead of lines \\(y=mx+b\\), we will consider only lines of the form \\(y=mx\\). We thus have a single parameter value to determine: the slope \\(m\\). To keep the algebra simple well take a tiny dataset consisting of just two points: \\((x_1, y_1) = (2,3)\\) and \\((x_2, y_2)= (5,4)\\), as indicated in Figure 6, below. The line passes through points \\((2,2m)\\) and \\((5, 5m)\\). Figure 6.6: Figure 6: Example 3: finding a line of best fit through the origin In this simple case, the sum of squared errors is: \\[\\begin{equation*} \\mbox{SSE} = \\mbox{SSE}(m)= e_1^2+e_2^2 = (3-2m)^2+(4-5m)^2 \\end{equation*}\\] To apply Fermats theorem we take the derivative and identify any values of \\(m\\) for which it is zero: \\[\\begin{eqnarray*} \\frac{d}{dm} \\mbox{SSE}(m) &amp;=&amp; 2(3-2m)(-2)+2(4-5m)(-5) \\\\ &amp;=&amp; -4(3-2m) - 10(4-5m) = -12 +8m+-40+50m = -52+58m. \\end{eqnarray*}\\] The only local extremum (and hence the only candidate for global minimum) is \\(m=52/58 = 26/29\\). We conclude that the best fit line is \\(y=\\frac{26}{29}x\\) The analysis in Example 3 can be extended to determine the general solution of the linear regression task (Fairway, 2002). The solution formula is as follows: Linear regression formula The best fit line \\(y=mx+b\\) to the dataset \\((x_1, y_1)\\), \\((x_2, y_2)\\), \\(\\ldots\\), \\((x_n, y_n)\\) is given by \\[\\begin{eqnarray*} m&amp;=&amp; \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\ b&amp;=&amp; \\bar{y}-m\\bar{x}, \\\\ \\end{eqnarray*}\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the averages \\[\\begin{eqnarray*} \\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\bar{y}=\\frac{1}{n} \\sum_{i=1}^n y_i \\end{eqnarray*}\\] This formula is somewhat unwieldy, but is straightforward to implement. In R, the command lm implements this formula, as the following example illustrates. 6.3.1.2 Example 4 Well work with a dataset called iris that is included in R. Its shown below (via the plot() command). plot(iris$Sepal.Length ~ iris$Petal.Width, xlab = &quot;Petal Width&quot;, ylab = &quot;Sepal Length&quot;) The lm() command applies the formula described above to arrive at the line of best fit (i.e. the minimizer of the sum of squared errors). The command below refers to the labels for the data (as shown above) to generate the parameters (\\(b\\) and \\(m\\)) that specify the line \\(y=mx+b\\). lm(iris$Sepal.Length ~ iris$Petal.Width) Call: lm(formula = iris$Sepal.Length ~ iris$Petal.Width) Coefficients: (Intercept) iris$Petal.Width 4.7776 0.8886 The intercept is \\(b=4.7776\\) and the slope is \\(m=0.8886\\). We can plot the data together with the line of best fit with the abline command: plot(iris$Sepal.Length ~ iris$Petal.Width, xlab = &quot;Petal Width&quot;, ylab = &quot;Sepal Length&quot;) abline(lm(iris$Sepal.Length ~ iris$Petal.Width)) 6.3.2 Nonlinear Regression Nonlinear regression is the task of fitting a nonlinear model (e.g. a curve) to a dataset. The setup for this task is identical to linear regression. We begin by selecting a parameterized family of models (i.e. curves), and aim to identify the model (i.e. curve) that minimizes the sum of squared errors when compared to the data. The family of models is typically chosen based on the mechanism that produced the data. For example, if we are investigating the rate law for a single-substrate enzyme-catalysed reaction, we might choose the family of curves specified by Michaelis-Menten kinetics, which relate reaction velocity \\(V\\) to substraet concentration \\(S\\): \\[\\begin{eqnarray*} V = \\frac{V_{\\mbox{max}} S}{K_M+S} \\end{eqnarray*}\\] Our goal would then be to identify values for the parameters \\(V_{\\mbox{max}}\\) and \\(K_M\\) that minimize the sum of squared errors when comparing with observed data. Regression via linearizing transformation In several important cases, the nonlinear regression task can be transformed into a linear regression task. In the case of Michaelis-Menten kinetics, several linearizing transformations have been proposed, e.g. EadieHofstee and LineweaverBurk (Cho and Lim, 2018). Another common example is fitting exponential curves (e.g. population growth or drug clearance). In those cases, a logarithm modifies the data so that a linear trend is captured: e.g. \\(y = e^{rt}\\), after applying a logarithm becomes \\(Y = \\ln(y) = \\ln (e^{rt})= rt\\). Linear regression on transformed data \\((t_i, Y_i)\\) then provides an estimate of the value of \\(r\\). Unfortunately, linearizing transformations are only available in a handful of special cases. Moreover, they often introduce biases that can make interpretation of the resulting model difficult. In general, the nonlinear regression task must be addressed directly. An example of the general procedure in R follows. 6.3.2.1 Exammple 5 We begin by defining a dataset against which we will fit a Michaelis-Menten curve. S &lt;- c(4.32, 2.16, 1.08, 0.54, 0.27, 0.135, 3.6, 1.8, 0.9, 0.45, 0.225, 0.1125, 2.88, 1.44, 0.72, 0.36, 0.18, 0.9, 0) #substrate concentrations V &lt;- c(0.48, 0.42, 0.36, 0.26, 0.17, 0.11, 0.44, 0.47, 0.39, 0.29, 0.18, 0.12, 0.5, 0.45, 0.37, 0.28, 0.19, 0.31, 0) #reaction velocities MM_data = cbind(S, V) plot(MM_data) The nls command can be used for nonlinear regression in R. Like the lm command, the nls function takes as inputs the dataset and the model. In addition, nls requires that the user provide guesses for the values of the parameters to be estimated. In this case, we can roughly estimate \\(V_{\\mbox{max}} \\approx 0.5\\) as the maximal \\(V\\)-value that can be achieved, and \\(K_M \\approx 0.3\\) as the \\(S\\)-value at which \\(V\\) reaches half of its maximal value. MMmodel.nls &lt;- nls(V ~ Vmax * S/(Km + S), start = list(Km = 0.3, Vmax = 0.5)) #perform the nonlinear regression analysis params &lt;- summary(MMmodel.nls)$coeff[, 1] #extract the parameter estimates params Km Vmax 0.4187090 0.5331688 We thus arrive at parameter estimates that characterize the best-fit model: \\(K_M = 0.4772154\\) and \\(V_{\\mbox max} = 0.0053061\\)). Further details on using the nls command can be found in (Ritz and Streibig, 2008). We can now plot the best fit model along with the dataset: plot(MM_data) curve((params[2] * x)/(params[1] + x), from = 0, to = 4.5, add = TRUE, col = &quot;firebrick&quot;) From the implementation of nls, the reader might have the mistaken impression that nonlinear regression and linear regression are very similar tasks. Although the problem set-up is similar in both cases (chose model type, then minimize SSE), the strategy for optimization is very different. (The first hint of this is the need to provide a guesses to nls.) As we saw above, the solution to the linear regression task can be stated as a general formula. For nonlinear regression, no such formula exists. Worse, as well discuss in the next section there is no procedure (algorithm) that is guaranteed to find the solution! 6.4 Iterative Optimization Algorithms The best techniques for addressing the general nonlinear regression task are iterative global optimization routines. As well discuss below, these algorithms start with a guess for the parameter values and then take steps through parameter space to improve the quality of that estimate. In the exercise above, the nls command executed this kind of algorithm, which is why it requires that the user supply an initial guess. 6.4.1 Gradient Descent A simple iterative optimization algorithm is gradient descent, which can be understood intuitively via a thought experiment. Imagine finding your way to a valley bottom in a think fog. The fog obscures your vision so that you can only observe changes in elevation in your immediate vicinity. To make your way to the valley bottom, it would be reasonable to take each step of your journey in the direction of steepest decline. This strategy is guaranteed to lead to a local minimum, but cannot guarantee arrival at the lowest point: the global minimum. That is, the search may lead to a local minimum at which there is no direction of local descent, and so the algorithm gets `stuck. Mathematically, the local change in elevation is determined by evaluating the optimization objective at points near the current estimate and then comparing to determine the direction of steepest descent. (Technically, this involves a linearization of the function at the current position, or equivalently a calculation of the gradient vector). A step is then taken in this direction, and the process is repeated from this updated estimate. To implement this algorithm, a number of details have to be specified: how long should each step be? How many steps should be taken? Or should there be some other termination condition that will trigger the end of the journey? (Each of these decisions involve a trade-off between precision and computation time. For instance, taking very small steps will guarantee a smooth path down the steepest route, but might take a very long time to complete the journey. Termination conditions are often specified in terms of the local topography: the algorithm stops when the current estimate is at a sufficiently flat position (no downhill direction detected). To illustrate the gradient descent approach, consider the following algorithm, which incorporates two termination conditions: a maximum number of allowed iterations and a threshold for shallowness: library(numDeriv) # contains functions for calculating gradient # define function that implement gradient descent. Inputs # are the objective function f, the initial parameter # estimate x0, the size of each step, the maximum number of # iterations to be applied, and a threshold gradient below # which the landscape is considered flat (and so iterations # are terminated) grad.descent = function(f, x0, step.size = 0.05, max.iter = 200, stopping.deriv = 0.01, ...) { n = length(x0) # record the number of parameters to be estimated (i.e. the dimension of the parameter space) xmat = matrix(0, nrow = n, ncol = max.iter) # initialize a matrix to record the sequence of estimates xmat[, 1] = x0 # the first row of matrix xmat is the initial &#39;guess&#39; for (k in 2:max.iter) { # loop over the allowed number of steps Calculate # the gradient (a vector indicating steepness and # direct of greatest ascent) grad.current = grad(f, xmat[, k - 1], ...) # Check termination condition: has a flat valley # bottom been reached? if (all(abs(grad.current) &lt; stopping.deriv)) { k = k - 1 break } # Step in the opposite direction of the grad xmat[, k] = xmat[, k - 1] - step.size * grad.current } xmat = xmat[, 1:k] # Trim any unused columns from xmat return(list(x = xmat[, k], xmat = xmat, k = k)) } 6.4.1.1 Example 6 Well begin by demonstrating the performance of this algorithm on a simple function with a single local minimum. Paraboloid = function(x) { return((x[1] - 3)^2 + 1/3 * (x[2])^2 + 2) } In this case, we expect that the minimum (valley bottom) will be reached from any initial guess. Starting from four distinct points, the algorithm follows the paths shown below. The table below shows the estimates and the corresponding objective values reached by gradient descent starting from those four points. Each run has arrived at essentially the same point: \\((3, 0)\\), where the function reaches its mimimum value of 2. x1 x2 objective value x0_out 3 -0.01246994 2.000052 x1_out 3 0.01193417 2.000047 x2_out 3 -0.01200889 2.000048 x3_out 3 0.01307634 2.000057 6.4.1.2 Example 7 Next, lets consider a function that has multiple local minima. The second plot below is interactive, allowing you to rotate the surface. complicatedFun = function(x) { return((1/2 * x[1]^2 + 1/4 * x[2]^2 + 3) + cos(2 * x[1] + 1 - exp(x[2]))) } Again, well apply gradient descent from a set of initial guess positions: x0 = c(0.5, 0.5) gd0 = grad.descent(complicatedFun, x0, step.size = 0.01, max.iter = 1000) x1 = c(-0.1, -1.3) gd1 = grad.descent(complicatedFun, x1, step.size = 0.01, max.iter = 1000) x2 = c(-1.5, 1.3) gd2 = grad.descent(complicatedFun, x2, step.size = 0.01, max.iter = 1000) x3 = c(-1.2, -1.4) gd3 = grad.descent(complicatedFun, x3, step.size = 0.01, max.iter = 1000) x4 = c(-0.5, -0.5) gd4 = grad.descent(complicatedFun, x4, step.size = 0.01, max.iter = 1000) From some initial guess values, the algorithm successfully reaches the global minimum. From others, it gets stuck at a local minimum. The plot below shows the paths followed by the algorithm, leading to two separate valley bottoms. The table below shows the final estimate and the corresponding objective value reached by gradient descent starting from those five starting points. x0_out &lt;- c(gd0$x, complicatedFun(gd0$x)) x1_out &lt;- c(gd1$x, complicatedFun(gd1$x)) x2_out &lt;- c(gd2$x, complicatedFun(gd2$x)) x3_out &lt;- c(gd3$x, complicatedFun(gd3$x)) x4_out &lt;- c(gd4$x, complicatedFun(gd4$x)) output &lt;- rbind(x0_out, x1_out, x2_out, x3_out, x4_out) colnames(output) &lt;- c(&quot;x1&quot;, &quot;x2&quot;, &quot;objective value&quot;) output x1 x2 objective value x0_out 1.0697802 -0.5784429 2.810101 x1_out 1.0633307 -0.6030409 2.810100 x2_out -0.3775909 1.1636290 2.426837 x3_out -0.3776268 1.1636047 2.426838 x4_out -0.3776816 1.1635677 2.426839 6.4.1.3 Exercise 1 Consider the following dataset, which corresponds to measurements of drug concentration in the blood over time. The trend in the data suggests it can be well-captured by a biexponential \\(C(t) = a_1 e^{r_1 t} + a_2 e^{r_2 t}\\). Set up a sum of squared errors function and use either the gradient descent function above or the nls command to determine the best-fit estimates of the parameters \\(A\\), \\(r_1\\) and \\(r_2\\). Next, try fitting the data to a single exponential \\(C(t) = a e^{r t}\\). Can this simpler model also fit the data well? 6.4.2 Global Optimization The default optimization algorithm used by nls is the Gauss-Newton method, which is a generalization of Newtons method for solving nonlinear equations (which may be familiar from introductory calculus). This is a refinement of gradient descent in which the local curvature of the function is used to predict the position of the bottom of the local valley assuming that the surface is shaped like a parabola. On parabolic surfaces, this achieves descent to the bottom in one iteration. For non-parabloic objectives (i.e. most objectives), the result is a sequence of iterations; often many fewer than would be required by gradient descent. Both gradient descent and the Gauss-Newton method are designed to reach the bottom of the valley in which the initial guess lies. If thats not the global minimum, then the algorithm will not be successful. So, how does one select a good initial guess? Unfortunately, theres no general answer to this question. In some cases, one can use previous knowledge of the system to begin with a solid initial guess. If no such previous knowledge is available, sometimes a guess is all we have. In those cases, we may have little confidence that the algorithm will arrive at a global minimum. The simplest way to gain some confidence of achieving a global minimum is the multi-start strategy: choose many initial guesses, and run the algorithm from each (as in Example 7 above). This can be computationally expensive, but if the initial guesses are spread widely over the parameters space, one can have hope that the global minimum will be achieved. A number of methods have been developed to complement the multi-start approach. These are known as global optimization routines. They are also known as heuristic methods, because their performance cannot be guaranteed in general: there are no guarantees that theyll find the global minimum, nor are there solid estimates of how many iterations will be required for them to carry out a satisfactory search of the parameter space. We will consider two commonly used heuristic methods: simulated annealing and genetic algorithms. 6.4.2.1 Simulated Annealing Simulated annealing is motivated by the process of annealing: a heat treatment by which the physical properties of metals can be altered. Simulated annealing is an iterative algorithm; the algorithm starts at an initial guess, and then steps through the search space. In contrast with gradient descent, the stapes taken in simulated annealing are partially random. Consequently, the path followed from a particular initial condition wont be repeated if the algorithm is run again from the same point. (Algorithms that incorporate randomness are often referred to as Monte Carlo methods, in reference to the European gambling centre.) At each step, the algorithm begins by identify a candidate next position. (This point could be selected by a variant of gradient descent, or some other method). The value of the objective at this candidate point is then compared with the objective value at the current point. If the objective is lower at the candidate point (i.e. this step takes the algorithm downhill), then the step is taken and a new iteration begins. If the value of the objective is larger at the candidate point (i.e. the step makes things worse), the step can still be taken, but only with a small probability. Both the size of the candidate steps and the probability of accepting wrong (uphill) steps are tied to a cooling schedule: a decreasing temperature profile: at high temperatures, large steps are considered and wrong steps are taken frequently; as the temperature drops, only smaller steps are considered, and fewer wrong steps are allowed. By analogy, imagine a ping-pong ball resting on a curved landscape. One strategy to move the ball to the lowest valley bottom is to shake the table. Following a simulated annealing-inspired approach, we could begin by applying violent shakes (high temperature) which would result in the ball bouncing across much of the landscape. By slowly reducing the severity of the shaking, we could ensure the ball would settle into a local minimum at a valley bottom. The hope is that if the cooling schedule is well-chosen, the ball would have sampled many valleys, and would end up at the bottom of the lowest valley. Simulated annealing can be combined with a multi-start strategy to further ensure broad sampling of the search space. 6.4.2.2 Example 8 To illustrate, well apply simulated annealing to the optimization task in Example 7 above, using the same initial guess points. Well use the GenSA library to implement the algorithm (described in detail here). Calls to GenSA require that we specify the objective function, an initial guess, and upper and lower bounds for the search values for each parameter. (Optional input parameters allow the user to modify internal features of the algorithm such as the cooling schedule and stepping protocol.) library(GenSA) out0 &lt;- GenSA(par = x0, lower = c(-2, -2), upper = c(2, 2), fn = complicatedFun) out0[c(&quot;value&quot;, &quot;par&quot;)] $value [1] 2.426731 $par [1] -0.3629442 1.1734789 The result of this function call is recorded in the variable out0 which indicates the minimal value of the objective achieved (2.426731) and the parameter values at which this minimum occurs \\((x,y)= (-0.3629442, 1.1734789)\\). Next, well call GenSA starting from each of the initial guesses that were provided to the gradient descent algorithm in Example 7. $value [1] 2.426731 $par [1] -0.3629442 1.1734789 $value [1] 2.426731 $par [1] -0.3629442 1.1734789 $value [1] 2.426731 $par [1] -0.3629442 1.1734789 $value [1] 2.426731 $par [1] -0.3629442 1.1734789 $value [1] 2.426731 $par [1] -0.3629442 1.1734789 We see that in each case, simulated annealing has avoided getting stuck in the local minima. It has achieved the same optimized value from every initial guess. The plot below provides some insight into how the simulated annealing run proceed. Iterations (steps) are shown along the horizontal axis. The vertical axis shows values of the objective function. The blue points represents the function value at the current position, while the red shows the minimum achieved so far. The minimum is found rather quickly, but the algorithm continues to jump around in an attempt to widely explore the search space. 6.4.2.3 Example 9 To give the simulated annealing algorithm a more challenging task, consider the following function, which has many local minima. \\[f(x,y) = (y+47)\\sin\\left (\\sqrt{|y+(x/2)+47|)}-x\\sin(\\sqrt{|x-(y+47)|} + \\frac{1}{1000}\\left(x^2+y^2\\right)\\right)\\] We start the algorithm from two different initial guesses. Both runs result in the same solution: egg.out &lt;- GenSA(par = x0, lower = lower, upper = upper, fn = f.egg) egg.out[c(&quot;value&quot;, &quot;par&quot;)] $value [1] -266.8175 $par [1] -161.38281 95.54245 egg.out3 &lt;- GenSA(par = x3, lower = lower, upper = upper, fn = f.egg) egg.out3[c(&quot;value&quot;, &quot;par&quot;)] $value [1] -266.8175 $par [1] -161.38281 95.54245 From the plots below we see that the algorithm occassionaly gets stuck at local minima, but in both cases has visited the lowest point. We next consider a heuristic algorithm in which multiple paths through the search space are followed simultaneously. 6.4.2.4 Genetic Algorithms Genetic algorithms are inspired by Darwinian evolution. The algorithm begins with the specification of a population of initial guesses. At each iteration of the algorithm, this population evolves toward improved estimates of the global minimum. This evolution step involves three substeps: selection, mutation, and cross-over. In the selection step, the population is pruned by removing a fraction that are not sufficiently fit (where fitness corresponds to the value of the objective function being minimized). Then, mutations are introduced into the remaining population by adding small random perturbations to their position in the search space. Finally, a new generation is generated by crossing members of the current population. This can be done in several ways; the simplest is to generate crosses as averages of the numerical values of the two parents. Through several generations, this process leads to a population with high fitness (minimal objective) after a thorough exploration of the search space. Genetic algorithms are a subset of the more general class of evolutionary algorithms all of which involve simultaneous exploration of the search space through multiple paths. To implement a genetic algorithm, well make use of the ga function, described here. As with simulated annealing, the call to ga requires that we specify the objective function, lower and upper bounds to define the search space, and the number of iterations to execute. The ga routine maximizes the objective function, so we enter our objective with a negative sign to achieve minimization. library(GA) ga &lt;- ga(type = &quot;real-valued&quot;, fitness = function(x) -complicated(x[1], x[2]), lower = c(-2, -2), upper = c(2, 2), maxiter = 30) GA | iter = 1 | Mean = -4.041041 | Best = -2.873942 GA | iter = 2 | Mean = -3.552414 | Best = -2.480774 GA | iter = 3 | Mean = -3.385857 | Best = -2.480774 GA | iter = 4 | Mean = -3.316562 | Best = -2.475568 GA | iter = 5 | Mean = -3.233971 | Best = -2.453920 GA | iter = 6 | Mean = -2.942318 | Best = -2.429493 GA | iter = 7 | Mean = -3.057510 | Best = -2.429493 GA | iter = 8 | Mean = -2.929332 | Best = -2.427140 GA | iter = 9 | Mean = -2.862539 | Best = -2.427125 GA | iter = 10 | Mean = -2.611589 | Best = -2.427125 GA | iter = 11 | Mean = -2.665873 | Best = -2.426933 GA | iter = 12 | Mean = -2.649276 | Best = -2.426795 GA | iter = 13 | Mean = -2.574107 | Best = -2.426795 GA | iter = 14 | Mean = -2.667271 | Best = -2.426775 GA | iter = 15 | Mean = -2.516660 | Best = -2.426756 GA | iter = 16 | Mean = -2.705621 | Best = -2.426756 GA | iter = 17 | Mean = -2.598244 | Best = -2.426756 GA | iter = 18 | Mean = -2.580032 | Best = -2.426756 GA | iter = 19 | Mean = -2.584246 | Best = -2.426745 GA | iter = 20 | Mean = -2.654025 | Best = -2.426745 GA | iter = 21 | Mean = -2.551965 | Best = -2.426745 GA | iter = 22 | Mean = -2.612507 | Best = -2.426745 GA | iter = 23 | Mean = -2.675908 | Best = -2.426745 GA | iter = 24 | Mean = -2.628224 | Best = -2.426745 GA | iter = 25 | Mean = -2.664589 | Best = -2.426745 GA | iter = 26 | Mean = -2.631207 | Best = -2.426745 GA | iter = 27 | Mean = -2.598906 | Best = -2.426745 GA | iter = 28 | Mean = -2.553829 | Best = -2.426745 GA | iter = 29 | Mean = -2.591655 | Best = -2.426745 GA | iter = 30 | Mean = -2.482629 | Best = -2.426745 Plotting the results of the genetic algorithm search, we see improvement in the overall behaviour from generation to generation. plot(ga) The summary displayed below shows that the solutions reached by the genetic algorithm agrees with the solution found by simulated annealing. summary(ga) -- Genetic Algorithm ------------------- GA settings: Type = real-valued Population size = 50 Number of generations = 30 Elitism = 2 Crossover probability = 0.8 Mutation probability = 0.1 Search domain = x1 x2 lower -2 -2 upper 2 2 GA results: Iterations = 30 Fitness function value = -2.426745 Solution = x1 x2 [1,] -0.3675713 1.169812 6.4.2.5 Exercise 2 Apply the genetic algorithm with ga to confirm the minimum of the egg-carton function in Example 9. 6.5 Calibration of Dynamic Models The principles of nonlinear regression carry over directly to calibration of more complex models. In many domains of biology, dynamic models are used to describe the time-varying behaviour of systems (from biomolecular networks to physiology to ecology). These models take many forms; a commonly used formulation is based on ordinary differential equations (i.e. rate equations). These models are deterministic (do not incorporate random effects) and assume that the dynamics occur in a spatially homogeneous (well-mixed) environment. Despite these limitations, these models can describe a wide variety of dynamic behaviours, and are useful starting points for investigations across a variety of biological domains. Ordinary differential equation models used in biology often take the form \\[\\begin{equation*} \\frac{d}{dt} {\\bf x}(t) = {\\bf f}({\\bf x}(t), {\\bf p}) \\end{equation*}\\] where components of the time-varying vector \\({\\bf x}(t)\\) are the states of the system (e.g. population sizes, molecular concentrations), the components of vector \\({\\bf p}\\) are the model parameters: numerical values that represent fixed features of the system and its environment (e.g. interaction strengths, temperature, nutrient availability), and the vector-valued function \\({\\bf f}\\) describes the rate of change of the state variables. As a concrete example, consider the Lotka-Volterra equations, a classical model describing interacting predator and prey populations: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; \\alpha x_1(t) - \\beta x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; \\gamma x_1(t) x_2(t) - \\delta x_2(t)\\\\ \\end{eqnarray*}\\] Here \\(x_1\\) is the size of the prey population; \\(x_2\\) is the size of the predator population. The prey are presumed to have access to resources that supports exponential growth in the absence of predation: growth at rate \\(\\alpha x_1(t)\\). Interactions between prey and predator populations, assumed to occur at rate \\(x_1(t) x_2(t)\\), lead to decrease of the prey population and increase of the predator population (characterized by parameters \\(\\beta\\) and \\(\\gamma\\) respectively). Finally, the prey suffer an exponential decline in population size in the absence of prey: decay at rate \\(\\delta x_2(t)\\). A simulation of the model, shown below, demonstrates the boom-bust cycle of oscillations. Simulation of the model requires specification of numerical values for each of the four model parameters and the two initial population sizes. library(deSolve) # define the dynamic model LotVmod &lt;- function(Time, State, Pars) { with(as.list(c(State, Pars)), { dx = x * (alpha - beta * y) dy = -y * (gamma - delta * x) return(list(c(dx, dy))) }) } # specify the model parameters, the initial populations, # and the time course Pars &lt;- c(alpha = 30, beta = 5, gamma = 2, delta = 6) State &lt;- c(x = 0.008792889, y = 1.595545) Time &lt;- seq(0, 6, by = 0.01) # simulate the model out &lt;- as.data.frame(ode(func = LotVmod, y = State, parms = Pars, times = Time)) # plot the output plot(out$y ~ out$time, type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Population Density&quot;, main = &quot;Predatory-Prey Model Simulation&quot;, col = &quot;red&quot;, ylim = c(0, 20)) points(out$x ~ out$time, type = &quot;l&quot;, add = TRUE) legend(3.8, 20, legend = c(&quot;Prey&quot;, &quot;Predator&quot;), col = c(&quot;red&quot;, &quot;black&quot;), pch = 20) The figure below shows a dataset the corresponds to observations of an oscillatory predator-prey system. To calibrate the Lotka-Volterra model to this data we seek values of the model parameters for which simulation of the model provides the best fit. As in the regression tasks described previously, the standard measure for quality of fit is the sum of squared errors. We thus proceed with a minimization task: for each choice of numerical values for the model parameters, we compare model simulation with the data and determine the sum of squared errors. We aim to minimize this measure of fit over the space of model parameters. We will use the observed population values at time zero as initial condition for the simulations. 6.5.0.1 Example 10 To illustrate, well first consider the case that two of the model parameter values have been established independently: \\(\\alpha= 30\\) and \\(\\delta=6\\). Well then calibrate the model by estimating values for parameters \\(\\beta\\) and \\(\\gamma\\). We begin by building a sum-of-squared errors function that takes values of \\(\\beta\\) and \\(\\gamma\\) as arguments: # define SSE function determine_sse2 &lt;- function(p) { # inputs are the two unknown model parameters collected # into a vector p=[beta, gamma] newPars &lt;- c(p[1], p[2]) # initial populations are the observed values at time # zero for x and y, respectively newState &lt;- c(x = 0.008792889, y = 1.595545) # define the time-grid (same as above) Time &lt;- seq(0, # 6, by = .001) # kinetics is the same as before, no need to redefine newLotVmod &lt;- function(Time, State, newPars) { with(as.list(c(State, newPars)), { dx = x * (30 - p[1] * y) dy = -y * (p[2] - 6 * x) return(list(c(dx, dy))) }) } # run the simulation with the user-specified values for # p2 and p3 new_out &lt;- as.data.frame(ode(func = newLotVmod, y = newState, parms = newPars, times = Time)) # generate vector of predictions to align with vector # of observations predictions &lt;- data.frame(t_pred &lt;- t_obs, x_pred &lt;- new_out$x[seq(1, length(new_out$x), 20)], y_pred &lt;- new_out$y[seq(1, length(new_out$y), 20)]) sse &lt;- sum((x_obs_corrupt - x_pred)^2) + sum((y_obs_corrupt - y_pred)^2) return(sse) } We now call an optimization routine to search the space of \\(\\beta\\) and \\(\\gamma\\) values to minimize this SSE function. We apply the simulated annealing function GenSA as described above, with initial guess of \\(\\beta=1\\), \\(\\gamma=1\\): library(GenSA) x1 = c(1, 1) lower &lt;- c(0.1, 0.1) upper &lt;- c(10, 10) out0 &lt;- GenSA(par = x1, lower = lower, upper = upper, fn = determine_sse2, control = list(maxit = 10)) out0[c(&quot;value&quot;, &quot;par&quot;)] $value [1] 45.58991 $par [1] 4.914941 2.018206 The search algorithm has identified values of \\(\\beta=4.914941\\) and \\(\\gamma=2.018206\\) as minimizer, with the minimal sum of squares error value of 45.58991. The best fit model simulation is shown along with the data below: A comprehensive discussion of calibration and uncertainty analysis of dynamic biological models can be found in (Ashyraliyev et al. 2009). 6.5.0.2 Exercise 3 Extend Example 10 to calibrate all four model parameters \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) to the following dataset, with initial states \\(x=0.01\\) and \\(y=1.0\\). Use either simulated annealing t_obs_exercise3 x_obs_exercise3 y_obs_exercise3 0.0 0.0092427 0.6877376 0.2 0.0991726 1.1647015 0.4 0.1890883 5.2346474 0.6 0.8577029 -0.8166061 0.8 3.6158610 -0.1470649 1.0 3.2423363 5.6632291 1.2 0.1285328 8.1547894 1.4 0.0074977 7.5172996 1.6 -0.0005159 5.5716749 1.8 -0.1690455 4.4545184 2.0 -0.0003094 2.9949089 2.2 0.0001233 5.3177857 2.4 0.0001501 2.1671169 2.6 0.0002556 1.8678420 2.8 0.0005672 1.3588565 3.0 0.0015628 1.4045889 3.2 0.0049168 1.1785011 3.4 0.0446260 1.0523866 3.6 0.0845161 2.3427045 3.8 0.3909393 -1.6216964 4.0 1.8200118 -0.2675681 4.2 5.3233189 3.8364321 4.4 0.7226232 8.3619045 4.6 0.0281340 8.3332233 4.8 -0.0006702 6.4087758 5.0 -0.2182455 4.8928742 5.2 0.0001958 4.4952190 5.4 0.0001268 1.8519416 5.6 0.0001293 2.7528138 5.8 0.0001885 2.8815009 6.0 0.0003667 1.5569359 6.2 0.0016078 2.0158893 6.4 0.0733844 1.6517998 6.6 0.0151907 4.2685059 6.8 0.0399855 1.8166511 7.0 0.1789146 2.7772145 7.2 0.8449529 1.3397371 7.4 3.5744301 2.0358086 7.6 3.3128142 6.8458911 7.8 0.1323336 8.7195510 8.0 0.0381401 7.4228031 You can cut-and-paste this data into your own R consloe using either data&lt;- read.table(\"clipboard\", header=T) on Windows or data&lt;- read.table(pipe(\"pbpaste\"), header=T) on a Mac. 6.5.0.3 Exercise 4 The behaviour of the Lotka-Volterra model described above depends on the values of the model parameters. Such models are often used to explore the effect of perturbations on a system. For instance, after successfully calibrating a model to given populations dynamics, one could propose interventions that could alter the value of one of the model parameters, e.g.~by restricting access to resources required for growth of the prey population. We could then simulate the model under this altered value of \\(p_1\\) and thus generate predictions of the effect of this manipulation. Alternatively, we could use a model formulation that incorporates a time-varying perturbation. As an example, consider a version of the Lotka-Volterra equations that includes a term to describe removal (harvesting/culling) of the predator population: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; p_1 x_1(t) - p_2 x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; p_3 x_1(t) x_2(t) - p_4 x_2(t) - u(t) x_2(t) \\end{eqnarray*}\\] Here \\(u(t)\\), called an input signal is a function that represents the effort exerted in removal of predators. With this input signal in place, we can now use the model to explore the consequences of a range of removal schedules. In particular, optimization can be used to identify the removal strategy that best achieves some performance goal. As a concrete example, consider the goal of maximizing harvest over some fixed time period (e.g.~a year). Suppose that the harvesting rate must be fixed throughout the time interval of interest. In this case, the harvesting rate is constant. Lets call it \\(u(t) = u_0\\). We can then account for the total harvest by introducing a new state variable that tracks the progress of the harvest: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; p_1 x_1(t) - p_2 x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; p_3 x_1(t) x_2(t) - p_4 x_2(t) - u_0 x_2(t)\\\\ \\frac{d}{dt} x_3(t) &amp;=&amp; u_0 x_2(t) \\end{eqnarray*}\\] Then the value of \\(x_3(t)\\) is the accumulated harvest from time zero to time \\(t\\). If our goal is to maximize the total harvest over a year, we aim to maximize \\(x_3(1)\\) over the choice of the fixed harvesting effort \\(u_0\\). Take parameter values of \\(p_1 = 30\\), \\(p_2 = 5\\), \\(p_3 = 2\\), \\(p_4 = 6\\), and initial state \\(x_1(0) = 0.0088\\), \\(x_2(0)=1.6\\). Determine the value of \\(u_0\\) that maximizes the total harvest over a period of $t . 6.6 Uncertainty Analysis and Bayesian Calibration So far, weve used regression analysis to provide estimates of parameter values based on data. Regression is usually followed by uncertainty analysis, which provides some measure of confidence in those parameter value estimates. For instance, in the case of linear regression, 95% confidence intervals on the estimates (best fit line slope and intercept) and corresponding confidence intervals on the model predictions are supported by extensive statistical theory, and can be readily generated in R, as follows. library(ggplot2) data(&quot;iris&quot;) iris.lm &lt;- lm(Sepal.Length ~ Petal.Width, data = iris) new.dat &lt;- seq(min(iris$Petal.Width), max(iris$Petal.Width), by = 0.05) pred_interval &lt;- predict(iris.lm, newdata = data.frame(Petal.Width = new.dat), interval = &quot;prediction&quot;, level = 0.95) The plot below shows 95% confidence intervals on the model predictions. The confint() function determines 95% confidence intervals on the estimates of the parameters (the slope and intercept). 2.5 % 97.5 % (Intercept) 4.6335014 4.9217574 Petal.Width 0.7870598 0.9901007 For nonlinear regression (including calibration of dynamic models), the theory is less helpful, but estimates of uncertainty intervals can be achieved: confint(MMmodel.nls) Waiting for profiling to be done... 2.5% 97.5% Km 0.3143329 0.5528199 Vmax 0.4899962 0.5816347 Bayesian methods address the regression task by combining calibration and uncertainty in a single process. The basic idea behind Bayesian analysis (founded on Bayes Theorem, which may be familiar from elementary probability), is to start with some knowledge about the parameter estimates (analogous to the initial guess supplied to nonlinear regression or global optimization) and then use the available data to refine that knowledge. In the Bayesian contect, instead of the initial guess and refined estimate being single numerical values, they are distributions. In Bayesian terminology, we being with a prior distribution, which may be informed by previously established expert knowledge. A commonly used prior is a normal distribution centered at a good initial estimate; in other cases the piro may be a uniform distribution over a wide range of possible values, indicating very little previously established knowledge about the parameetr values. Application of a Bayesian calibration scheme uses the available data to generate an improved distribution of the parameter values, called the posterior distribution. A successful Bayesian calibration could take a `wild guess uniform prior and return a tightly-centered posterior. Uncertainty can then be gleaned directly from the posterior distribution. Here well consider a simple numerical implementation of a Bayesian approach: Approximate Bayesian Computation (ABC). This approach is based on a simple idea: the rejection method, in which we sample repeatedly from the prior distribution and reject all samples that do not satisfy a pre-specified tolerance for quality of fit. (This is reminiscent of the selection step in genetic algorithms: culling unfit members of a population.) The samples that are not rejected form the posterior distribution. To illustrate, well revist the Michaelis-Menten example considered above. We select uniform priors for both parameters: \\(Km\\) uniform on \\([0,1]\\) and \\(V_{max}\\) uniform on \\([0, 0.01]\\). In the call below, we sample from the prior distributions of \\(Km\\) and \\(Vmax\\) 200000 times. We set the acceptance threshold as 0.001. That is, parameter pairs that give rise to SSE values below 0.001 are accepted; others are rejected. The histograms below show the uniform priors along with the posteriors. The algorithm has successfully tightened the distributions about the best-fit parameter estimates established above. [1] 2164 We see that only 2164 of 200 000 samples were accepted, yielding an acceptance rate of about 1%. The acceptance rate can be tuned by choice of the rejection threshold. A low acceptance rate gives rise to a computationally expensive algorithm, while a high acceptance rate can lead to poor estimation. Typically, approximate Bayesian computation is implemented as a sequential method, in which a sequence of rejection steps is applied, with the distribution being refined at each step (generating, in essence, a sequence of posterior distributions, the last of which is considered to be the best description of the desired parameter estimates). Below, we illustrate the use of the EasyABC package to implement sequential ABC for the Michaelis-Menten fitting task. (more details on this package can be found in (Beaumont, 2019) and here) 6.6.0.1 Exercise 5 Consider the task of finding the minimum of the function \\(f(x,y)=x^4 + y^2\\), plotted below. Clearly, the minimum is zero, and numerical optimization routines will have no trouble estimating that solution. However, because the curvature at the minimum is much different along the \\(x\\)- and \\(y\\)-directions, most numerical algorithms will do a better job estimating one parameter compared to the other. This can be well-illustrated by applying the rejection method to this problem and noting the relative spread in the posterior distributions. Implement the rejection algorithm provided above with uniform prior distributions on [-1, 1] for both the \\(x\\) and \\(y\\) values. Choose a rejection threshold that provides an accpetance rate of about 5%. Which estimate is provided with more confidence (i.e. which posterior is more tightly distributed? How can you relate that back to the curvature of the function at the minimum?. 6.6.0.2 Exercise 6 Apply the rejection algorithm to estimate the values of \\(p_2\\) and \\(p_3\\) by fitting the Lotka-Volterra model to the data provided above. Use uniform prior distributions of [0,10] for both parameters, and choose a rejection threshold that provides an acceptance rate of about 5%. Do you find that one parameter is more confidently estimated than the other? 6.7 References Ashyraliyev, M., FomekongNanfack, Y., Kaandorp, J. A., &amp; Blom, J. G. (2009). Systems biology: parameter estimation for biochemical models. The FEBS journal, 276(4), 886-902. Beaumont, Mark A. Approximate bayesian computation. Annual review of statistics and its application 6 (2019): 379-403. Cho, Yong-Soon, and Hyeong-Seok Lim. Comparison of various estimation methods for the parameters of Michaelis-Menten equation based on in vitro elimination kinetic simulation data. Translational and clinical pharmacology 26.1 (2018): 39-47. Fairway, Julien, (2002) Practical Regression and Anova using R. https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf Ritz, C and Streibig, J. C., Nonlinear Regression with R, Springer, 2008. "]]
